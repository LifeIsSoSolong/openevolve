"""
Linus-style minimal prompt optimizer with original logic faithfully reproduced.

Design goals:
- Keep data structures minimal (list[dict]) and keep optimizer decoupled from any Agent.
- Reproduce the optimization flow and meta-prompt semantics from optimizer_sdk:
  - Same meta-prompt template(s)
  - Same per-example formatting
  - Same batch splitting by token budget using tiktoken
  - Same handling of optional ruleset mode (returns ruleset text instead of prompt)

Usage summary:
- Optimize prompts:
    optimizer.optimize(
        baseline_prompt: str,
        dataset: list[dict],           # rows with template variables, output + feedback columns
        output_column: str,
        feedback_columns: list[str],
        evaluators: list[callable] = [],
        annotations: list[str] | None = None,
        ruleset: str | None = None,
        context_size_tokens: int = 128000,
    ) -> str

- Optional helper to fill outputs via an arbitrary Agent:
    fill_outputs_with_agent(agent, baseline_prompt, dataset, task_field, output_column)
  where agent.run(task, prompt) -> str
"""
from __future__ import annotations

import os
import logging
from typing import Callable, Dict, List, Optional, Sequence
from tqdm import trange

try:
    # OpenAI SDK v1
    from openai import OpenAI  # type: ignore
except Exception:  # pragma: no cover
    OpenAI = None  # type: ignore

logger = logging.getLogger(__name__)

"""Embedded templates and constants (no external optimizer_sdk dependency)."""

START_DELIM = "{"
END_DELIM = "}"

SUPPORTED_MODELS = [
    "o1",
    "o3",
    "gpt-4o",
    "gpt-4",
    "gpt-5",
    "gpt-3.5-turbo",
    "gpt-3.5",
]

META_PROMPT_TEMPLATE = """
You are an expert in prompt optimization. Given the original baseline prompt and the following associated metadata (such as model inputs, outputs, evaluation labels and explanations),
generate a revised version of the original prompt that would likely improve results with respect to the evaluation labels.
Your goal is to align the prompt with the feedback and evaluation criteria.

BELOW IS THE ORIGINAL BASELINE PROMPT
************* start prompt *************


{baseline_prompt}
************* end prompt *************

BELOW ARE THE EXAMPLES USING THE ABOVE PROMPT
************* start example data *************


{examples}
************* end example data *************

HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:
{annotations}

FINAL INSTRUCTIONS
Iterate on the original prompt (above) with a new prompt that will improve the results, based on the examples and feedback above.

A common best practice in prompt optimization is to add guidelines and the most helpful bullets of experiences from successful and failure problems.

Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.
{var}). If you fail to include these variables, the LLM will not be able to access the required data.
Do not add any single or double brackets around anything other than the variables from the original prompt. The only curly brackets that should be used are the ones that wrap the variables from the original prompt.
Make sure to copy paste the exact return instructions from the original prompt. Do not add any brackets here. 

YOUR NEW PROMPT:
"""

CODING_AGENT_META_PROMPT_TEMPLATE = """
You are an expert in coding agent prompt optimization.  
Your goal is to improve the dynamic ruleset that guides the coding agent.  

Process:
1. Carefully review the baseline prompt, the current dynamic ruleset, examples, and annotations.  
2. Identify high-level issues in the baseline prompt and dynamic ruleset — focus on missing guidance, vague constraints, or areas where rules could be made more robust.  
3. Revise the dynamic ruleset so it is stronger, more reliable, and generalizes well beyond the provided examples.  

BELOW IS THE ORIGINAL BASELINE PROMPT WITH STATIC RULESET
************* start prompt *************  

{baseline_prompt}  
************* end prompt *************  

BELOW IS THE CURRENT DYNAMIC RULESET (CHANGE THESE OR ADD NEW RULES)
************* start ruleset *************  

{ruleset}  
************* end ruleset *************  

Now you will be given data examples that use the above prompt and ruleset. Each example consists of:
- problem_statement: the problem statement
- coding agent patch: a patch generated by the coding agent, which is supposed to fix the problem.
- ground truth patch: a ground truth solution/patch to the problem
- test patch: a test patch that the coding agent's output should pass, which directly addresses the issue in the problem statement
- pass_or_fail: either "pass" or "fail" indicating whether the coding agent's code changes passed the unit tests (indicates whether the coding agent's output is correct or incorrect)
- explanation: explanation of your reasoning: why/why not the coding agent's output is correct, why the coding agent may have taken that approach, and general improvement suggestions for the coding agent to improve its output.

BELOW ARE THE EXAMPLES USING THE ABOVE PROMPT AND RULESET
************* start example data *************  

{examples}  
************* end example data *************  

FINAL INSTRUCTIONS  
Iterate on the **dynamic ruleset only**. You may:  
- Add new rules  
- Edit or strengthen existing rules  

Important constraints:
- Do **not** modify the static rules in the baseline prompt.  
- Do **not** add rules that request user input, confirmations, or follow-up questions (e.g., `ask_followup_question`). The coding agent should always act autonomously.  
- Keep the ruleset concise and relevant — avoid unnecessary rules that don't match the general types of problems that the coding agent is likely to encounter or overly specific rules that only patch the given examples.  
- Remember that you are writing GENERAL rules. They should not be specific to the repositories or problems that you are given. They should be general rules that would improve the overall ability of the coding agent. 
Output format:
- Return only the final, revised dynamic ruleset as a bullet-point list.  
- Do not include any extra commentary, explanations, or text outside the ruleset.  

New ruleset:
"""


def _build_meta_prompt(
    baseline_prompt: str,
    batch_rows: List[Dict[str, object]],
    template_variables: List[str],
    feedback_columns: List[str],
    output_column: str,
    annotations: Optional[List[str]] = None,
    ruleset: Optional[str] = None,
    meta_template: str = META_PROMPT_TEMPLATE,
    coding_agent_template: str = CODING_AGENT_META_PROMPT_TEMPLATE,
) -> str:
    # Construct examples section exactly like optimizer_sdk.meta_prompt.MetaPrompt
    ex_parts: List[str] = []
    for idx, row in enumerate(batch_rows):
        row_dict = row
        # Prepare output value with delimiter sanitation
        output_value = row_dict.get(output_column)
        output_str = "None"
        if output_value is not None:
            output_str = str(output_value)
            output_str = output_str.replace(
                START_DELIM, " ").replace(END_DELIM, " ")

        if ruleset is None:
            current = [
                "",
                f"                    Example {idx}",
                "",
                f"                    Data for baseline prompt: {[row_dict.get(v, '') for v in template_variables]}",
                "",
                f"                    LLM Output using baseline prompt: {output_str}",
                "",
                "                    Output level feedback:",
            ]
        else:
            current = [
                "",
                f"                    Example {idx}",
                "",
                f"                    coding agent patch: {output_str}",
            ]

        # Feedback lines (present in both regular and ruleset modes in original logic)
        for col in feedback_columns:
            val = row_dict.get(col)
            if val is None:
                fv = "None"
            else:
                fv = str(val).replace(START_DELIM, " ").replace(END_DELIM, " ")
            current.append(f"                    {col}: {fv}")

        ex_parts.append("\n".join(current))

    examples_block = "".join(ex_parts)

    content = (coding_agent_template if ruleset is not None else meta_template)
    content = content.replace("{baseline_prompt}", baseline_prompt)
    content = content.replace("{examples}", examples_block)
    content = content.replace("{annotations}", "\n".join(annotations or []))
    if ruleset is not None:
        content = content.replace("{ruleset}", ruleset)
    return content


class LinusPromptOptimizer:
    def __init__(
        self,
        model: str = "gpt-4o",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        meta_prompt_template: Optional[str] = None,
        coding_agent_meta_prompt_template: Optional[str] = None,
    ) -> None:
        if OpenAI is None:
            raise RuntimeError(
                "openai package is required. Install `openai`. ")
        self.model = model
        self.api_key = api_key or os.getenv("OPENAI_API_KEY") or ""
        self.base_url = base_url or os.getenv("OPENAI_API_BASE") or ""

        if not self.api_key:
            raise ValueError(
                "OPENAI_API_KEY not set and api_key not provided.")
        # Allow user to override meta-prompt templates
        self.meta_prompt_template = meta_prompt_template or META_PROMPT_TEMPLATE
        self.coding_agent_meta_prompt_template = (
            coding_agent_meta_prompt_template or CODING_AGENT_META_PROMPT_TEMPLATE
        )
        self._client = OpenAI(api_key=self.api_key, base_url=self.base_url)

    @staticmethod
    def detect_template_variables(prompt_content: str) -> List[str]:
        import re

        _TEMPLATE_RE = re.compile(r"\{([a-zA-Z_][a-zA-Z0-9_]*)\}")
        return list({m.group(1) for m in _TEMPLATE_RE.finditer(prompt_content)})

    @staticmethod
    def _token_batches(
        rows: List[Dict[str, object]],
        columns: Sequence[str],
        context_size_tokens: int,
        model_for_tiktoken: str,
    ) -> List[List[Dict[str, object]]]:
        import tiktoken

        # Map model names to something tiktoken supports (aligning with repo behavior)
        model = model_for_tiktoken
        if model.startswith("o3"):
            model = "o3"
        if model.startswith("gpt-5"):
            model = "gpt-4o"
        if model.startswith("gpt-4.1"):
            model = "gpt-4"
        if model not in SUPPORTED_MODELS:
            raise ValueError(
                f"Model {model} not supported. Supported: {SUPPORTED_MODELS}")

        enc = tiktoken.encoding_for_model(model)

        def count_row_tokens(r: Dict[str, object]) -> int:
            total = 0
            for c in columns:
                v = r.get(c, "")
                s = "" if v is None else str(v)
                total += len(enc.encode(s))
            return total

        batches: List[List[Dict[str, object]]] = []
        start = 0
        current: List[Dict[str, object]] = []
        current_tokens = 0
        for row in rows:
            t = count_row_tokens(row)
            if current and current_tokens + t > context_size_tokens:
                batches.append(current)
                current = [row]
                current_tokens = t
            else:
                current.append(row)
                current_tokens += t
        if current:
            batches.append(current)
        return batches

    def optimize(
        self,
        baseline_prompt: str,
        dataset: List[Dict[str, object]],
        output_column: str,
        evaluators: List[Callable] | None = None,
        feedback_columns: Optional[List[str]] = None,
        annotations: Optional[List[str]] = None,
        ruleset: Optional[str] = None,
        context_size_tokens: int = 128000,
    ) -> str:
        """
        Faithful re-implementation of optimizer_sdk.PromptLearningOptimizer.optimize.

        - dataset: list of dicts; keys should include the template variables referenced in baseline_prompt,
          the output_column, and provided feedback_columns.
        - If evaluators are provided, they should accept (dataset: list[dict]) and return
          (feedback_rows: list[dict], column_names: list[str]). We'll append those columns to each row.
        - ruleset: if provided, returns an updated ruleset (string) instead of a prompt.
        """

        data = [dict(x) for x in dataset]
        feedback_columns = feedback_columns or []
        evaluators = evaluators or []

        # Run evaluators, append feedback columns
        for evaluator in evaluators:
            try:
                fb_rows, col_names = evaluator(data)
                if len(fb_rows) != len(data):
                    raise ValueError(
                        "evaluator must return one feedback row per input row")
                for row, fb_row in zip(data, fb_rows):
                    for col in col_names:
                        row[col] = fb_row.get(col)
                feedback_columns.extend(col_names)
            except Exception as e:
                print(f"⚠ evaluator failed: {e}")

        # Extract template variables
        template_variables = self.detect_template_variables(baseline_prompt)

        # Build columns list to count tokens (mirror original: use all present keys)
        all_columns: List[str] = sorted({k for r in data for k in r.keys()})

        # Build batches
        batches = self._token_batches(
            rows=data,
            columns=all_columns,
            context_size_tokens=context_size_tokens,
            model_for_tiktoken=self.model,
        )

        optimized_prompt_content = baseline_prompt
        current_ruleset = ruleset

        for i, batch in enumerate(batches):
            try:
                content = _build_meta_prompt(
                    baseline_prompt=optimized_prompt_content,
                    batch_rows=batch,
                    template_variables=template_variables,
                    feedback_columns=feedback_columns,
                    output_column=output_column,
                    annotations=annotations,
                    ruleset=current_ruleset,
                    meta_template=self.meta_prompt_template,
                    coding_agent_template=self.coding_agent_meta_prompt_template,
                )
                resp = self._client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": content}],
                )
                response_text = (resp.choices[0].message.content or "").strip()
                if current_ruleset is not None:
                    current_ruleset = response_text
                else:
                    optimized_prompt_content = response_text
            except Exception as e:
                print(f"❌ Batch {i+1}/{len(batches)} failed: {e}")
                continue

        return current_ruleset if current_ruleset is not None else optimized_prompt_content

    # Convenience end-to-end wrapper: multi-iteration optimization with Agent + feedback
    def optimize_with_agent(
        self,
        baseline_prompt: str,
        agent: object,
        dataset: List[Dict[str, object]],
        test_dataset: List[Dict[str, object]],
        task_field: str,
        output_column: str,
        feedback_fn: Callable[..., str],
        *,
        loops: int = 1,
        ruleset: Optional[str] = None,
        annotations: Optional[List[str]] = None,
        context_size_tokens: int = 128000,
        feedback_column_name: str = "feedback",
        max_workers: Optional[int] = None,
        verbose=False,
        run_logger=None
    ) -> str:
        """
        End-to-end multi-iteration optimization loop.

        For each outer loop iteration:
          1) Use the current prompt (or baseline) to generate outputs via agent.run(task, prompt).
          2) Compute natural-language feedback via feedback_fn.
          3) Call `optimize` (single pass, batched) to update prompt or ruleset.

        Returns the final optimized prompt (or ruleset if provided).
        """

        if not hasattr(agent, "run") or not callable(getattr(agent, "run")):
            raise TypeError(
                "agent must expose a callable 'run(task, prompt)' method")

        current_prompt = baseline_prompt
        current_ruleset = ruleset

        def _gen_output(row: Dict[str, object]) -> str:
            task = str(row.get(task_field, ""))
            try:
                return str(agent.run(task=task, prompt=current_prompt))
            except Exception as e:
                return f"<agent-error: {e}>"

        def _gen_feedback(row: Dict[str, object]) -> str:
            try:
                # Try signature (task, response, row)
                return str(feedback_fn(row.get(task_field, ""), row.get(output_column, ""), row))
            except TypeError:
                # Fallback to (task, response)
                return str(feedback_fn(row.get(task_field, ""), row.get(output_column, "")))
            except Exception as e:  # pragma: no cover
                return f"<evaluator-error: {e}>"

        def _parallel_map(func, rows: List[Dict[str, object]]) -> List[str]:
            if not max_workers or max_workers <= 1:
                return [func(r) for r in rows]
            from concurrent.futures import ThreadPoolExecutor
            results: List[str] = []
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                for val in ex.map(func, rows):
                    results.append(val)
            return results

        data = [dict(x) for x in dataset]

        for cur_loop in trange(max(1, loops), disable=verbose):
            # 1) Generate outputs
            logger.info("Generating outputs with current prompt/ruleset...")
            outputs = _parallel_map(_gen_output, data)
            for row, out in zip(data, outputs):
                row[output_column] = out

            # 2) Generate feedback
            logger.info("Generating feedback with feedback function...")
            feedbacks = _parallel_map(_gen_feedback, data)
            for row, fb in zip(data, feedbacks):
                row[feedback_column_name] = fb

            # 3) Optimize single pass
            logger.info("Optimizing prompt/ruleset...")
            optimized = self.optimize(
                baseline_prompt=current_prompt,
                dataset=data,
                output_column=output_column,
                evaluators=None,
                feedback_columns=[feedback_column_name],
                annotations=annotations,
                ruleset=current_ruleset,
                context_size_tokens=context_size_tokens,
            )

            if current_ruleset is not None:
                current_ruleset = optimized
            else:
                current_prompt = optimized

            train_acc = compute_acc(data, feedback_fn)
            run_logger.log_event(
                {
                    "step": cur_loop + 1,
                    "type": "train",
                    "accuracy": train_acc
                }
            )

            test_acc, _ = self.fill_outputs_with_agent(
                agent=agent,
                baseline_prompt=current_prompt,
                dataset=test_dataset,
                task_field="task",
                output_column="output",
                max_workers=max_workers,
                metric_func=feedback_fn
            )
            run_logger.log_event(
                {
                    "step": cur_loop + 1,
                    "type": "eval",
                    "accuracy": test_acc,
                    "prompt": current_prompt
                }
            )
            run_logger.update_status(
                state="running",
                current_step=cur_loop+1,
                total_steps=loops,
                error=None,
            )

        return current_ruleset if current_ruleset is not None else current_prompt, test_acc

    @staticmethod
    def fill_outputs_with_agent(
        agent: object,
        baseline_prompt: str,
        dataset: List[Dict[str, object]],
        task_field: str,
        output_column: str,
        max_workers: Optional[int] = None,
        metric_func=None
    ) -> None:
        """Populate dataset[output_column] by running agent.run(task, prompt) per row.

        Supports optional concurrency via threads. Concurrency is best-effort and
        may be limited by the underlying agent's rate limits.
        """
        copied = [dict(r) for r in dataset]

        if not hasattr(agent, "run") or not callable(getattr(agent, "run")):
            raise TypeError(
                "agent must expose a callable 'run(task, prompt)' method")

        def _one(row: Dict[str, object]) -> str:
            task = str(row.get(task_field, ""))
            try:
                return str(agent.run(task=task, prompt=baseline_prompt))
            except Exception as e:  # pragma: no cover
                return f"<agent-error: {e}>"

        if not max_workers or max_workers <= 1:
            for row in copied:
                row[output_column] = _one(row)
            return

        from concurrent.futures import ThreadPoolExecutor

        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            results = list(ex.map(_one, copied))
        for row, out in zip(copied, results):
            row[output_column] = out

        if metric_func is not None:
            acc = compute_acc(copied, metric_func)
            return acc, copied
        else:
            return None, copied


def compute_acc(copied, metric_func):
    correct = 0
    for row in copied:
        fb_txt = metric_func(row["task"], row.get(
            "output", ""), row.get("ground_truth", ""))
        row["feedback"] = fb_txt
        if fb_txt.strip().lower() == "correct":
            correct += 1
    acc = correct / max(1, len(copied))
    return acc


__all__ = [
    "LinusPromptOptimizer",
    "_build_meta_prompt",
]

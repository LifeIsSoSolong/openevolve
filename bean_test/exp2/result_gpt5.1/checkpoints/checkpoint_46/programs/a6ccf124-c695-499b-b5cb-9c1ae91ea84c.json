{"id": "a6ccf124-c695-499b-b5cb-9c1ae91ea84c", "code": "\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(\"int8\")\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_agg_features(train, test):\n    \"\"\"Add a simple state\u2013month prior built from train targets.\"\"\"\n    sm = (\n        train.groupby([\"state_enc\", \"month\"])[\"yield\"]\n        .mean()\n        .rename(\"state_month_mean_yield\")\n        .reset_index()\n    )\n    train = train.merge(sm, on=[\"state_enc\", \"month\"], how=\"left\")\n    test = test.merge(sm, on=[\"state_enc\", \"month\"], how=\"left\")\n    # fallback for unseen (state, month) combinations in test\n    test[\"state_month_mean_yield\"].fillna(train[\"yield\"].mean(), inplace=True)\n    return train, test\n\n\ndef train_and_predict() -> Path:\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_agg_features(train, test)\n\n    # numeric feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n\n    # shared LightGBM parameters\n    params = dict(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        n_jobs=-1,\n    )\n\n    # two-seed ensemble for robustness\n    models = [lgb.LGBMRegressor(random_state=s, **params) for s in (42, 2024)]\n    for m in models:\n        m.fit(train[features], train[\"yield\"])\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "language": "python", "parent_id": "e1a68ef2-4857-4f91-afb7-df34a413a063", "generation": 5, "timestamp": 1764743550.6699, "iteration_found": 41, "metrics": {"combined_score": 0.8941268121063681, "mape": 4.825358538577976e-05, "rmse": 0.26854956345752984}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 57 lines with 73 lines", "parent_metrics": {"combined_score": 0.8070259695522676, "mape": 7.221882911296327e-05, "rmse": 0.628335240640556}, "island": 0}, "prompts": {"diff_user": {"system": "You are optimizing bean_test exp2 for Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set used by evaluator.py.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read ./input/train.csv & ./input/test.csv; write ./output/submission.csv with columns [year, month, state, yield].\n- Preserve feature pipeline basics (state encoding, months_since_crop_start); do not drop all features.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.8070\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.8070\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 59 lines\n- Metrics: combined_score: 0.8070, mape: 0.0001, rmse: 0.6283\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 61 lines with 76 lines\n- Metrics: combined_score: 0.8941, mape: 0.0000, rmse: 0.2685\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 57 lines with 67 lines\n- Metrics: combined_score: 0.8941, mape: 0.0000, rmse: 0.2685\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.8941)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(\"int8\")\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_agg_features(train, test):\n    sm = (\n        train.groupby([\"state_enc\", \"month\"])[\"yield\"]\n        .mean()\n        .rename(\"state_month_mean_yield\")\n        .reset_index()\n    )\n    train = train.merge(sm, on=[\"state_enc\", \"month\"], how=\"left\")\n    test = test.merge(sm, on=[\"state_enc\", \"month\"], how=\"left\")\n    test[\"state_month_mean_yield\"].fillna(train[\"yield\"].mean(), inplace=True)\n    return train, test\n\n\ndef train_and_predict() -> Path:\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    train, mapping = encode_state(train)\n    test, _ = encode_state(test, mapping)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_agg_features(train, test)\n\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n\n    params = dict(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        n_jobs=-1,\n    )\n\n    models = []\n    for seed in (42, 2024):\n        m = lgb.LGBMRegressor(random_state=seed, **params)\n        m.fit(train[features], train[\"yield\"])\n        models.append(m)\n\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.8941), Performs well on mape (0.0000), Performs well on rmse (0.2685)\n\n### Program 2 (Score: 0.8941)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df, mapping: Dict[str, int] | None = None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(\"int8\")\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_agg_features(train: pd.DataFrame, test: pd.DataFrame):\n    \"\"\"Use train targets to build a simple state\u2013month prior feature.\"\"\"\n    sm_mean = (\n        train.groupby([\"state_enc\", \"month\"])[\"yield\"]\n        .mean()\n        .reset_index()\n        .rename(columns={\"yield\": \"state_month_mean_yield\"})\n    )\n    train = train.merge(sm_mean, on=[\"state_enc\", \"month\"], how=\"left\")\n    test = test.merge(sm_mean, on=[\"state_enc\", \"month\"], how=\"left\")\n    global_mean = train[\"yield\"].mean()\n    test[\"state_month_mean_yield\"].fillna(global_mean, inplace=True)\n    return train, test\n\n\ndef train_and_predict() -> Path:\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_agg_features(train, test)\n\n    # feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n\n    # train (two-seed ensemble for robustness)\n    models = []\n    for seed in (42, 2024):\n        m = lgb.LGBMRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            num_leaves=63,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        m.fit(train[features], train[\"yield\"])\n        models.append(m)\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.8941), Performs well on mape (0.0000), Performs well on rmse (0.2685)\n\n### Program 3 (Score: 0.8070)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef train_and_predict() -> Path:\n    # read\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n    target = \"yield\"\n\n    # train (two-seed ensemble for robustness)\n    models = []\n    for seed in (42, 2024):\n        m = lgb.LGBMRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            num_leaves=63,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        m.fit(train[features], train[target])\n        models.append(m)\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.8070), Performs well on mape (0.0001), Performs well on rmse (0.6283)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.8070)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef train_and_predict() -> Path:\n    # read\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n    target = \"yield\"\n\n    # train (two-seed ensemble for robustness)\n    models = []\n    for seed in (42, 2024):\n        m = lgb.LGBMRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            num_leaves=63,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        m.fit(train[features], train[target])\n        models.append(m)\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.8070)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(\"int8\")\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef train_and_predict() -> Path:\n    # read\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n    target = \"yield\"\n\n    # shared LightGBM parameters\n    params = dict(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        n_jobs=-1,\n    )\n\n    # two-seed ensemble\n    models = [lgb.LGBMRegressor(random_state=s, **params) for s in (42, 2024)]\n    for m in models:\n        m.fit(train[features], train[target])\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Predictions saved to {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.8070, Type: High-Performer)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef train_and_predict() -> Path:\n    # read\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n    target = \"yield\"\n\n    # train (two-seed ensemble for robustness)\n    models = []\n    for seed in (42, 2024):\n        m = lgb.LGBMRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            num_leaves=63,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        m.fit(train[features], train[target])\n        models.append(m)\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 73 lines with 62 lines, Alternative mape approach, Comprehensive implementation\n\n### Inspiration 2 (Score: 0.8070, Type: High-Performer)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef train_and_predict() -> Path:\n    # read\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n    target = \"yield\"\n\n    # train (two-seed ensemble for robustness)\n    models = []\n    for seed in (42, 2024):\n        m = lgb.LGBMRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            num_leaves=63,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        m.fit(train[features], train[target])\n        models.append(m)\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 59 lines, Alternative mape approach, Comprehensive implementation\n\n### Inspiration 3 (Score: 0.8070, Type: High-Performer)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(\"int8\")\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef train_and_predict() -> Path:\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n\n    # shared LightGBM parameters\n    params = dict(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        n_jobs=-1,\n    )\n\n    # two-seed ensemble for robustness\n    models = [lgb.LGBMRegressor(random_state=s, **params) for s in (42, 2024)]\n    for m in models:\n        m.fit(train[features], train[\"yield\"])\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Predictions saved to {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 62 lines with 59 lines, Alternative mape approach, Comprehensive implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef train_and_predict() -> Path:\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n\n    # train (two-seed ensemble for robustness)\n    models = []\n    for seed in (42, 2024):\n        m = lgb.LGBMRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            num_leaves=63,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        m.fit(train[features], train[\"yield\"])\n        models.append(m)\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef train_and_predict() -> Path:\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n\n    # train (two-seed ensemble for robustness)\n    models = []\n    for seed in (42, 2024):\n        m = lgb.LGBMRegressor(\n            n_estimators=1000,\n            learning_rate=0.05,\n            num_leaves=63,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        m.fit(train[features], train[\"yield\"])\n        models.append(m)\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        mapping = {s: i for i, s in enumerate(sorted(df[\"state\"].unique()))}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(\"int8\")\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_agg_features(train, test):\n    \"\"\"Add a simple state\u2013month prior built from train targets.\"\"\"\n    sm = (\n        train.groupby([\"state_enc\", \"month\"])[\"yield\"]\n        .mean()\n        .rename(\"state_month_mean_yield\")\n        .reset_index()\n    )\n    train = train.merge(sm, on=[\"state_enc\", \"month\"], how=\"left\")\n    test = test.merge(sm, on=[\"state_enc\", \"month\"], how=\"left\")\n    # fallback for unseen (state, month) combinations in test\n    test[\"state_month_mean_yield\"].fillna(train[\"yield\"].mean(), inplace=True)\n    return train, test\n\n\ndef train_and_predict() -> Path:\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_agg_features(train, test)\n\n    # numeric feature selection\n    num_kinds = \"biufc\"\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in num_kinds]\n\n    # shared LightGBM parameters\n    params = dict(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        n_jobs=-1,\n    )\n\n    # two-seed ensemble for robustness\n    models = [lgb.LGBMRegressor(random_state=s, **params) for s in (42, 2024)]\n    for m in models:\n        m.fit(train[features], train[\"yield\"])\n\n    # predict\n    test_pred = sum(m.predict(test[features]) for m in models) / len(models)\n\n    # output\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}
{"id": "ffb32e21-b008-4136-809f-030eb215f235", "code": "\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_target_stats(train, test):\n    train = train.copy()\n    test = test.copy()\n\n    global_mean = train[\"yield\"].mean()\n    state_mean = train.groupby(\"state\")[\"yield\"].mean()\n    month_mean = train.groupby(\"month\")[\"yield\"].mean()\n    year_mean = train.groupby(\"year\")[\"yield\"].mean()\n    state_month_mean = train.groupby([\"state\", \"month\"])[\"yield\"].mean().to_dict()\n    state_year_mean = train.groupby([\"state\", \"year\"])[\"yield\"].mean().to_dict()\n\n    cols = [\n        \"state_yield_mean\",\n        \"month_yield_mean\",\n        \"year_yield_mean\",\n        \"state_month_yield_mean\",\n        \"state_year_yield_mean\",\n    ]\n\n    for df in (train, test):\n        df[\"state_yield_mean\"] = df[\"state\"].map(state_mean)\n        df[\"month_yield_mean\"] = df[\"month\"].map(month_mean)\n        df[\"year_yield_mean\"] = df[\"year\"].map(year_mean)\n\n        key_sm = list(zip(df[\"state\"], df[\"month\"]))\n        key_sy = list(zip(df[\"state\"], df[\"year\"]))\n        df[\"state_month_yield_mean\"] = pd.Series(key_sm, index=df.index).map(state_month_mean)\n        df[\"state_year_yield_mean\"] = pd.Series(key_sy, index=df.index).map(state_year_mean)\n\n        df[cols] = df[cols].fillna(global_mean)\n\n    return train, test\n\n\ndef train_and_predict():\n    # read\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_target_stats(train, test)\n\n    # feature selection\n    numeric_kinds = \"biufc\"\n    features = [col for col in train.columns if col != \"yield\" and train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # model\n    model = lgb.LGBMRegressor(\n        n_estimators=3000,\n        learning_rate=0.025,\n        num_leaves=96,\n        min_child_samples=5,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[target], categorical_feature=[\"state_enc\", \"month\", \"year\"])\n\n    # stacked linear calibration\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n    y_true = train[target].values\n\n    # linear regression stack: y \u2248 w0 + w1 * model_pred + w2 * baseline\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    try:\n        coef, _, _, _ = np.linalg.lstsq(X, y_true, rcond=None)\n        train_pred_cal = X @ coef\n        X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n        test_pred = X_test @ coef\n    except np.linalg.LinAlgError:\n        # fallback to 1D variance-based calibration\n        var = np.var(train_pred)\n        if var > 0:\n            cov = np.cov(train_pred, y_true, bias=True)[0, 1]\n            b = cov / var\n            a = y_true.mean() - b * train_pred.mean()\n            train_pred_cal = a + b * train_pred\n            test_pred = a + b * test_pred\n        else:\n            # degenerate case: keep uncalibrated predictions\n            train_pred_cal = train_pred\n\n    # bias correction by (state, year) using training residuals\n    residual = y_true - train_pred_cal\n    tmp = train[[\"state\", \"year\"]].copy()\n    tmp[\"residual\"] = residual\n    sy_residual = tmp.groupby([\"state\", \"year\"])[\"residual\"].mean().to_dict()\n    key_sy_test = list(zip(test[\"state\"], test[\"year\"]))\n    correction = pd.Series(key_sy_test, index=test.index).map(sy_residual).fillna(0.0).to_numpy()\n    test_pred = test_pred + correction\n\n    # clip to training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n\n    # output\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model training finished, predictions saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "language": "python", "parent_id": "f11a6931-2738-4191-9664-e6e059636c26", "generation": 14, "timestamp": 1764750103.9188528, "iteration_found": 97, "metrics": {"combined_score": 0.9984714660237699, "mape": 3.498101288994774e-07, "rmse": 0.0030660903163500083}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 24 lines with 40 lines", "parent_metrics": {"combined_score": 0.9984609740002699, "mape": 3.4830012402145883e-07, "rmse": 0.003087205202329681}, "island": 0}, "prompts": {"diff_user": {"system": "You are optimizing bean_test exp2 for Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set used by evaluator.py.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read ./input/train.csv & ./input/test.csv; write ./output/submission.csv with columns [year, month, state, yield].\n- Preserve feature pipeline basics (state encoding, months_since_crop_start); do not drop all features.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.9985\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.9985\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 8 lines with 7 lines\nChange 2: Replace 5 lines with 4 lines\nChange 3: Replace 41 lines with 32 lines\n- Metrics: combined_score: 0.9985, mape: 0.0000, rmse: 0.0031\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 121 lines with 137 lines\n- Metrics: combined_score: 0.9985, mape: 0.0000, rmse: 0.0031\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 17 lines with 8 lines\nChange 2: Replace 18 lines with 32 lines\n- Metrics: combined_score: 0.9985, mape: 0.0000, rmse: 0.0031\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.9985)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_target_stats(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Add simple target-encoding style statistics based only on the training data.\"\"\"\n    train = train.copy()\n    test = test.copy()\n\n    global_mean = train[\"yield\"].mean()\n    state_mean = train.groupby(\"state\")[\"yield\"].mean()\n    month_mean = train.groupby(\"month\")[\"yield\"].mean()\n    year_mean = train.groupby(\"year\")[\"yield\"].mean()\n    state_month_mean = train.groupby([\"state\", \"month\"])[\"yield\"].mean().to_dict()\n    state_year_mean = train.groupby([\"state\", \"year\"])[\"yield\"].mean().to_dict()\n\n    for df in (train, test):\n        df[\"state_yield_mean\"] = df[\"state\"].map(state_mean)\n        df[\"month_yield_mean\"] = df[\"month\"].map(month_mean)\n        df[\"year_yield_mean\"] = df[\"year\"].map(year_mean)\n\n        key_sm = list(zip(df[\"state\"], df[\"month\"]))\n        key_sy = list(zip(df[\"state\"], df[\"year\"]))\n        df[\"state_month_yield_mean\"] = pd.Series(key_sm, index=df.index).map(state_month_mean)\n        df[\"state_year_yield_mean\"] = pd.Series(key_sy, index=df.index).map(state_year_mean)\n\n        cols = [\n            \"state_yield_mean\",\n            \"month_yield_mean\",\n            \"year_yield_mean\",\n            \"state_month_yield_mean\",\n            \"state_year_yield_mean\",\n        ]\n        df[cols] = df[cols].fillna(global_mean)\n\n    return train, test\n\n\ndef train_and_predict() -> Path:\n\n    # ---------- read ----------\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_target_stats(train, test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [col for col in train.columns if col != \"yield\" and train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train single LightGBM model ----------\n    # Slightly smaller learning rate and more trees + a bit more capacity\n    # often improve generalisation on tabular problems like this.\n    model = lgb.LGBMRegressor(\n        n_estimators=3000,\n        learning_rate=0.025,\n        num_leaves=96,\n        min_child_samples=5,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n    # Treat state, month and year as categorical for more flexible splits\n    model.fit(train[features], train[target], categorical_feature=[\"state_enc\", \"month\", \"year\"])\n\n    # ---------- predict with stacked linear calibration ----------\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    # baseline from historical state-month mean yield\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n\n    y_true = train[target].values\n\n    # linear regression with intercept: y \u2248 w0 + w1 * model_pred + w2 * baseline\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    try:\n        coef, _, _, _ = np.linalg.lstsq(X, y_true, rcond=None)\n        train_pred_cal = X @ coef\n        X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n        test_pred = X_test @ coef\n    except np.linalg.LinAlgError:\n        # fallback to original 1D variance-based calibration\n        var = np.var(train_pred)\n        if var > 0:\n            cov = np.cov(train_pred, y_true, bias=True)[0, 1]\n            b = cov / var\n            a = y_true.mean() - b * train_pred.mean()\n            train_pred_cal = a + b * train_pred\n            test_pred = a + b * test_pred\n        else:\n            # degenerate case: keep uncalibrated predictions\n            train_pred_cal = train_pred\n\n    # bias correction by (state, year) using training residuals\n    residual = y_true - train_pred_cal\n    tmp = train[[\"state\", \"year\"]].copy()\n    tmp[\"residual\"] = residual\n    sy_residual = tmp.groupby([\"state\", \"year\"])[\"residual\"].mean().to_dict()\n    key_sy_test = list(zip(test[\"state\"], test[\"year\"]))\n    correction = pd.Series(key_sy_test, index=test.index).map(sy_residual).fillna(0.0).to_numpy()\n    test_pred = test_pred + correction\n\n    # clip calibrated predictions (after bias correction) to the observed training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n\n    # ---------- output ----------\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model training finished, predictions saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.9985), Performs well on mape (0.0000), Performs well on rmse (0.0031)\n\n### Program 2 (Score: 0.9985)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_target_stats(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Add simple target-encoding style statistics based only on the training data.\"\"\"\n    train = train.copy()\n    test = test.copy()\n\n    global_mean = train[\"yield\"].mean()\n    state_mean = train.groupby(\"state\")[\"yield\"].mean()\n    month_mean = train.groupby(\"month\")[\"yield\"].mean()\n    year_mean = train.groupby(\"year\")[\"yield\"].mean()\n    state_month_mean = train.groupby([\"state\", \"month\"])[\"yield\"].mean().to_dict()\n    state_year_mean = train.groupby([\"state\", \"year\"])[\"yield\"].mean().to_dict()\n\n    for df in (train, test):\n        df[\"state_yield_mean\"] = df[\"state\"].map(state_mean)\n        df[\"month_yield_mean\"] = df[\"month\"].map(month_mean)\n        df[\"year_yield_mean\"] = df[\"year\"].map(year_mean)\n\n        key_sm = list(zip(df[\"state\"], df[\"month\"]))\n        key_sy = list(zip(df[\"state\"], df[\"year\"]))\n        df[\"state_month_yield_mean\"] = pd.Series(key_sm, index=df.index).map(state_month_mean)\n        df[\"state_year_yield_mean\"] = pd.Series(key_sy, index=df.index).map(state_year_mean)\n\n        df[\n            [\n                \"state_yield_mean\",\n                \"month_yield_mean\",\n                \"year_yield_mean\",\n                \"state_month_yield_mean\",\n                \"state_year_yield_mean\",\n            ]\n        ] = df[\n            [\n                \"state_yield_mean\",\n                \"month_yield_mean\",\n                \"year_yield_mean\",\n                \"state_month_yield_mean\",\n                \"state_year_yield_mean\",\n            ]\n        ].fillna(global_mean)\n\n    return train, test\n\n\ndef train_and_predict() -> Path:\n\n    # ---------- read ----------\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_target_stats(train, test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [col for col in train.columns if col != \"yield\" and train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train single LightGBM model ----------\n    # Slightly smaller learning rate and more trees + a bit more capacity\n    # often improve generalisation on tabular problems like this.\n    model = lgb.LGBMRegressor(\n        n_estimators=3000,\n        learning_rate=0.025,\n        num_leaves=96,\n        min_child_samples=5,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n    # Treat state, month and year as categorical for more flexible splits\n    model.fit(train[features], train[target], categorical_feature=[\"state_enc\", \"month\", \"year\"])\n\n    # ---------- predict with stacked linear calibration ----------\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    # baseline from historical state-month mean yield\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n\n    y_true = train[target].values\n\n    # linear regression with intercept: y \u2248 w0 + w1 * model_pred + w2 * baseline\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    try:\n        coef, _, _, _ = np.linalg.lstsq(X, y_true, rcond=None)\n        X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n        test_pred = X_test @ coef\n    except np.linalg.LinAlgError:\n        # fallback to original 1D variance-based calibration\n        var = np.var(train_pred)\n        if var > 0:\n            cov = np.cov(train_pred, y_true, bias=True)[0, 1]\n            b = cov / var\n            a = y_true.mean() - b * train_pred.mean()\n            test_pred = a + b * test_pred\n\n    # clip calibrated predictions to the observed training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n\n    # ---------- output ----------\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model training finished, predictions saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.9985), Performs well on mape (0.0000), Performs well on rmse (0.0031)\n\n### Program 3 (Score: 0.9985)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_target_stats(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    train = train.copy()\n    test = test.copy()\n\n    global_mean = train[\"yield\"].mean()\n    state_mean = train.groupby(\"state\")[\"yield\"].mean()\n    month_mean = train.groupby(\"month\")[\"yield\"].mean()\n    year_mean = train.groupby(\"year\")[\"yield\"].mean()\n    state_month_mean = train.groupby([\"state\", \"month\"])[\"yield\"].mean().to_dict()\n    state_year_mean = train.groupby([\"state\", \"year\"])[\"yield\"].mean().to_dict()\n\n    cols = [\n        \"state_yield_mean\",\n        \"month_yield_mean\",\n        \"year_yield_mean\",\n        \"state_month_yield_mean\",\n        \"state_year_yield_mean\",\n    ]\n\n    for df in (train, test):\n        df[\"state_yield_mean\"] = df[\"state\"].map(state_mean)\n        df[\"month_yield_mean\"] = df[\"month\"].map(month_mean)\n        df[\"year_yield_mean\"] = df[\"year\"].map(year_mean)\n\n        key_sm = list(zip(df[\"state\"], df[\"month\"]))\n        key_sy = list(zip(df[\"state\"], df[\"year\"]))\n        df[\"state_month_yield_mean\"] = pd.Series(key_sm, index=df.index).map(state_month_mean)\n        df[\"state_year_yield_mean\"] = pd.Series(key_sy, index=df.index).map(state_year_mean)\n\n        df[cols] = df[cols].fillna(global_mean)\n\n    return train, test\n\n\ndef train_and_predict() -> Path:\n\n    # ---------- read ----------\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_target_stats(train, test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [col for col in train.columns if col != \"yield\" and train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train single LightGBM model ----------\n    # Slightly smaller learning rate and more trees + a bit more capacity\n    # often improve generalisation on tabular problems like this.\n    model = lgb.LGBMRegressor(\n        n_estimators=3000,\n        learning_rate=0.025,\n        num_leaves=96,\n        min_child_samples=5,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n    # Treat state, month and year as categorical for more flexible splits\n    model.fit(train[features], train[target], categorical_feature=[\"state_enc\", \"month\", \"year\"])\n\n    # ---------- predict with stacked linear calibration ----------\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    # baseline from historical state-month mean yield\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n\n    y_true = train[target].values\n\n    # linear regression with intercept: y \u2248 w0 + w1 * model_pred + w2 * baseline\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    try:\n        coef, _, _, _ = np.linalg.lstsq(X, y_true, rcond=None)\n        X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n        test_pred = X_test @ coef\n    except np.linalg.LinAlgError:\n        # fallback to original 1D variance-based calibration\n        var = np.var(train_pred)\n        if var > 0:\n            cov = np.cov(train_pred, y_true, bias=True)[0, 1]\n            b = cov / var\n            a = y_true.mean() - b * train_pred.mean()\n            test_pred = a + b * test_pred\n\n    # clip calibrated predictions to the observed training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n\n    # ---------- output ----------\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model training finished, predictions saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.9985), Performs well on mape (0.0000), Performs well on rmse (0.0031)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.9985)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_target_stats(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    train = train.copy()\n    test = test.copy()\n\n    global_mean = train[\"yield\"].mean()\n    state_mean = train.groupby(\"state\")[\"yield\"].mean()\n    month_mean = train.groupby(\"month\")[\"yield\"].mean()\n    year_mean = train.groupby(\"year\")[\"yield\"].mean()\n    state_month_mean = train.groupby([\"state\", \"month\"])[\"yield\"].mean().to_dict()\n    state_year_mean = train.groupby([\"state\", \"year\"])[\"yield\"].mean().to_dict()\n\n    cols = [\n        \"state_yield_mean\",\n        \"month_yield_mean\",\n        \"year_yield_mean\",\n        \"state_month_yield_mean\",\n        \"state_year_yield_mean\",\n    ]\n\n    for df in (train, test):\n        df[\"state_yield_mean\"] = df[\"state\"].map(state_mean)\n        df[\"month_yield_mean\"] = df[\"month\"].map(month_mean)\n        df[\"year_yield_mean\"] = df[\"year\"].map(year_mean)\n\n        key_sm = list(zip(df[\"state\"], df[\"month\"]))\n        key_sy = list(zip(df[\"state\"], df[\"year\"]))\n        df[\"state_month_yield_mean\"] = pd.Series(key_sm, index=df.index).map(state_month_mean)\n        df[\"state_year_yield_mean\"] = pd.Series(key_sy, index=df.index).map(state_year_mean)\n\n        df[cols] = df[cols].fillna(global_mean)\n\n    return train, test\n\n\ndef train_and_predict() -> Path:\n\n    # ---------- read ----------\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_target_stats(train, test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [col for col in train.columns if col != \"yield\" and train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train single LightGBM model ----------\n    # Slightly smaller learning rate and more trees + a bit more capacity\n    # often improve generalisation on tabular problems like this.\n    model = lgb.LGBMRegressor(\n        n_estimators=3000,\n        learning_rate=0.025,\n        num_leaves=96,\n        min_child_samples=5,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n    # Treat state, month and year as categorical for more flexible splits\n    model.fit(train[features], train[target], categorical_feature=[\"state_enc\", \"month\", \"year\"])\n\n    # ---------- predict with stacked linear calibration ----------\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    # baseline from historical state-month mean yield\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n\n    y_true = train[target].values\n\n    # linear regression with intercept: y \u2248 w0 + w1 * model_pred + w2 * baseline\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    try:\n        coef, _, _, _ = np.linalg.lstsq(X, y_true, rcond=None)\n        X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n        test_pred = X_test @ coef\n    except np.linalg.LinAlgError:\n        # fallback to original 1D variance-based calibration\n        var = np.var(train_pred)\n        if var > 0:\n            cov = np.cov(train_pred, y_true, bias=True)[0, 1]\n            b = cov / var\n            a = y_true.mean() - b * train_pred.mean()\n            test_pred = a + b * test_pred\n\n    # clip calibrated predictions to the observed training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n\n    # ---------- output ----------\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model training finished, predictions saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.9985)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport numpy as np, pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_target_stats(train, test):\n    train = train.copy()\n    test = test.copy()\n\n    global_mean = train[\"yield\"].mean()\n    state_mean = train.groupby(\"state\")[\"yield\"].mean()\n    month_mean = train.groupby(\"month\")[\"yield\"].mean()\n    year_mean = train.groupby(\"year\")[\"yield\"].mean()\n    state_month_mean = train.groupby([\"state\", \"month\"])[\"yield\"].mean().to_dict()\n    state_year_mean = train.groupby([\"state\", \"year\"])[\"yield\"].mean().to_dict()\n\n    cols = [\n        \"state_yield_mean\",\n        \"month_yield_mean\",\n        \"year_yield_mean\",\n        \"state_month_yield_mean\",\n        \"state_year_yield_mean\",\n    ]\n\n    for df in (train, test):\n        df[\"state_yield_mean\"] = df[\"state\"].map(state_mean)\n        df[\"month_yield_mean\"] = df[\"month\"].map(month_mean)\n        df[\"year_yield_mean\"] = df[\"year\"].map(year_mean)\n\n        key_sm = list(zip(df[\"state\"], df[\"month\"]))\n        key_sy = list(zip(df[\"state\"], df[\"year\"]))\n        df[\"state_month_yield_mean\"] = pd.Series(key_sm, index=df.index).map(state_month_mean)\n        df[\"state_year_yield_mean\"] = pd.Series(key_sy, index=df.index).map(state_year_mean)\n\n        df[cols] = df[cols].fillna(global_mean)\n\n    return train, test\n\n\ndef train_and_predict():\n    # read\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_target_stats(train, test)\n\n    # feature selection\n    numeric_kinds = \"biufc\"\n    features = [col for col in train.columns if col != \"yield\" and train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # model\n    model = lgb.LGBMRegressor(\n        n_estimators=3000,\n        learning_rate=0.025,\n        num_leaves=96,\n        min_child_samples=5,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[target], categorical_feature=[\"state_enc\", \"month\", \"year\"])\n\n    # stacked linear calibration\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n    y_true = train[target].values\n\n    # linear regression stack: y \u2248 w0 + w1 * model_pred + w2 * baseline\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    coef, *_ = np.linalg.lstsq(X, y_true, rcond=None)\n    X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n    test_pred = X_test @ coef\n\n    # clip to training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n\n    # output\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model training finished, predictions saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.9985, Type: High-Performer)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_target_stats(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Add simple target-encoding style statistics based only on the training data.\"\"\"\n    train = train.copy()\n    test = test.copy()\n\n    global_mean = train[\"yield\"].mean()\n    state_mean = train.groupby(\"state\")[\"yield\"].mean()\n    month_mean = train.groupby(\"month\")[\"yield\"].mean()\n    year_mean = train.groupby(\"year\")[\"yield\"].mean()\n    state_month_mean = train.groupby([\"state\", \"month\"])[\"yield\"].mean().to_dict()\n    state_year_mean = train.groupby([\"state\", \"year\"])[\"yield\"].mean().to_dict()\n\n    for df in (train, test):\n        df[\"state_yield_mean\"] = df[\"state\"].map(state_mean)\n        df[\"month_yield_mean\"] = df[\"month\"].map(month_mean)\n        df[\"year_yield_mean\"] = df[\"year\"].map(year_mean)\n\n        key_sm = list(zip(df[\"state\"], df[\"month\"]))\n        key_sy = list(zip(df[\"state\"], df[\"year\"]))\n        df[\"state_month_yield_mean\"] = pd.Series(key_sm, index=df.index).map(state_month_mean)\n        df[\"state_year_yield_mean\"] = pd.Series(key_sy, index=df.index).map(state_year_mean)\n\n        cols = [\n            \"state_yield_mean\",\n            \"month_yield_mean\",\n            \"year_yield_mean\",\n            \"state_month_yield_mean\",\n            \"state_year_yield_mean\",\n        ]\n        df[cols] = df[cols].fillna(global_mean)\n\n    return train, test\n\n\ndef train_and_predict() -> Path:\n\n    # ---------- read ----------\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_target_stats(train, test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [col for col in train.columns if col != \"yield\" and train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train single LightGBM model ----------\n    # Slightly smaller learning rate and more trees + a bit more capacity\n    # often improve generalisation on tabular problems like this.\n    model = lgb.LGBMRegressor(\n        n_estimators=3000,\n        learning_rate=0.025,\n        num_leaves=96,\n        min_child_samples=5,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n    # Treat state, month and year as categorical for more flexible splits\n    model.fit(train[features], train[target], categorical_feature=[\"state_enc\", \"month\", \"year\"])\n\n    # ---------- predict with stacked linear calibration ----------\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    # baseline from historical state-month mean yield\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n\n    y_true = train[target].values\n\n    # linear regression with intercept: y \u2248 w0 + w1 * model_pred + w2 * baseline\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    try:\n        coef, _, _, _ = np.linalg.lstsq(X, y_true, rcond=None)\n        X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n        test_pred = X_test @ coef\n    except np.linalg.LinAlgError:\n        # fallback to original 1D variance-based calibration\n        var = np.var(train_pred)\n        if var > 0:\n            cov = np.cov(train_pred, y_true, bias=True)[0, 1]\n            b = cov / var\n            a = y_true.mean() - b * train_pred.mean()\n            test_pred = a + b * test_pred\n\n    # clip calibrated predictions to the observed training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n\n    # ---------- output ----------\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model training finished, predictions saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 42 lines with 28 lines, Excellent combined_score (0.998), Alternative mape approach\n\n### Inspiration 2 (Score: 0.9985, Type: High-Performer)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport numpy as np, pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_target_stats(train, test):\n    train = train.copy()\n    test = test.copy()\n\n    global_mean = train[\"yield\"].mean()\n    state_mean = train.groupby(\"state\")[\"yield\"].mean()\n    month_mean = train.groupby(\"month\")[\"yield\"].mean()\n    year_mean = train.groupby(\"year\")[\"yield\"].mean()\n    state_month_mean = train.groupby([\"state\", \"month\"])[\"yield\"].mean().to_dict()\n    state_year_mean = train.groupby([\"state\", \"year\"])[\"yield\"].mean().to_dict()\n\n    cols = [\n        \"state_yield_mean\",\n        \"month_yield_mean\",\n        \"year_yield_mean\",\n        \"state_month_yield_mean\",\n        \"state_year_yield_mean\",\n    ]\n\n    for df in (train, test):\n        df[\"state_yield_mean\"] = df[\"state\"].map(state_mean)\n        df[\"month_yield_mean\"] = df[\"month\"].map(month_mean)\n        df[\"year_yield_mean\"] = df[\"year\"].map(year_mean)\n\n        key_sm = list(zip(df[\"state\"], df[\"month\"]))\n        key_sy = list(zip(df[\"state\"], df[\"year\"]))\n        df[\"state_month_yield_mean\"] = pd.Series(key_sm, index=df.index).map(state_month_mean)\n        df[\"state_year_yield_mean\"] = pd.Series(key_sy, index=df.index).map(state_year_mean)\n\n        df[cols] = df[cols].fillna(global_mean)\n\n    return train, test\n\n\ndef train_and_predict():\n    # read\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_target_stats(train, test)\n\n    # feature selection\n    numeric_kinds = \"biufc\"\n    features = [col for col in train.columns if col != \"yield\" and train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # model\n    model = lgb.LGBMRegressor(\n        n_estimators=3000,\n        learning_rate=0.025,\n        num_leaves=96,\n        min_child_samples=5,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[target], categorical_feature=[\"state_enc\", \"month\", \"year\"])\n\n    # stacked linear calibration\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n    y_true = train[target].values\n\n    # linear regression stack: y \u2248 w0 + w1 * model_pred + w2 * baseline\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    coef, *_ = np.linalg.lstsq(X, y_true, rcond=None)\n    X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n    test_pred = X_test @ coef\n\n    # clip to training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n\n    # output\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model training finished, predictions saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 2 lines\nChange 2: Replace 12 lines with 5 lines, Excellent combined_score (0.998), Alternative mape approach\n\n### Inspiration 3 (Score: 0.9985, Type: High-Performer)\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_target_stats(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Add simple target-encoding style statistics based only on the training data.\"\"\"\n    train = train.copy()\n    test = test.copy()\n\n    global_mean = train[\"yield\"].mean()\n    state_mean = train.groupby(\"state\")[\"yield\"].mean()\n    month_mean = train.groupby(\"month\")[\"yield\"].mean()\n    year_mean = train.groupby(\"year\")[\"yield\"].mean()\n    state_month_mean = train.groupby([\"state\", \"month\"])[\"yield\"].mean().to_dict()\n    state_year_mean = train.groupby([\"state\", \"year\"])[\"yield\"].mean().to_dict()\n\n    for df in (train, test):\n        df[\"state_yield_mean\"] = df[\"state\"].map(state_mean)\n        df[\"month_yield_mean\"] = df[\"month\"].map(month_mean)\n        df[\"year_yield_mean\"] = df[\"year\"].map(year_mean)\n\n        key_sm = list(zip(df[\"state\"], df[\"month\"]))\n        key_sy = list(zip(df[\"state\"], df[\"year\"]))\n        df[\"state_month_yield_mean\"] = pd.Series(key_sm, index=df.index).map(state_month_mean)\n        df[\"state_year_yield_mean\"] = pd.Series(key_sy, index=df.index).map(state_year_mean)\n\n        df[\n            [\n                \"state_yield_mean\",\n                \"month_yield_mean\",\n                \"year_yield_mean\",\n                \"state_month_yield_mean\",\n                \"state_year_yield_mean\",\n            ]\n        ] = df[\n            [\n                \"state_yield_mean\",\n                \"month_yield_mean\",\n                \"year_yield_mean\",\n                \"state_month_yield_mean\",\n                \"state_year_yield_mean\",\n            ]\n        ].fillna(global_mean)\n\n    return train, test\n\n\ndef train_and_predict() -> Path:\n\n    # ---------- read ----------\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_target_stats(train, test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [col for col in train.columns if col != \"yield\" and train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train single LightGBM model ----------\n    # Slightly smaller learning rate and more trees + a bit more capacity\n    # often improve generalisation on tabular problems like this.\n    model = lgb.LGBMRegressor(\n        n_estimators=3000,\n        learning_rate=0.025,\n        num_leaves=96,\n        min_child_samples=5,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n    # Treat state, month and year as categorical for more flexible splits\n    model.fit(train[features], train[target], categorical_feature=[\"state_enc\", \"month\", \"year\"])\n\n    # ---------- predict with stacked linear calibration ----------\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    # baseline from historical state-month mean yield\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n\n    y_true = train[target].values\n\n    # linear regression with intercept: y \u2248 w0 + w1 * model_pred + w2 * baseline\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    try:\n        coef, _, _, _ = np.linalg.lstsq(X, y_true, rcond=None)\n        X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n        test_pred = X_test @ coef\n    except np.linalg.LinAlgError:\n        # fallback to original 1D variance-based calibration\n        var = np.var(train_pred)\n        if var > 0:\n            cov = np.cov(train_pred, y_true, bias=True)[0, 1]\n            b = cov / var\n            a = y_true.mean() - b * train_pred.mean()\n            test_pred = a + b * test_pred\n\n    # clip calibrated predictions to the observed training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n\n    # ---------- output ----------\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model training finished, predictions saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 15 lines with 17 lines, Excellent combined_score (0.998), Alternative mape approach\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean_test exp2.\n\nReads ./input/train.csv and ./input/test.csv, performs simple preprocessing\n(state encoding + months_since_crop_start), trains a LightGBM regressor, and\nwrites predictions to ./output/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# Paths\n# PROJECT_ROOT = Path(__file__).resolve().parent\nPROJECT_ROOT = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\bean_test\\exp1\")\nINPUT_DIR = PROJECT_ROOT / \"input\"\nOUTPUT_DIR = PROJECT_ROOT / \"output\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_PATH = INPUT_DIR / \"train.csv\"\nTEST_PATH = INPUT_DIR / \"test.csv\"\n\n# EVOLVE-BLOCK-START\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df, mapping=None):\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df):\n    df = df.copy()\n    df[\"months_since_crop_start\"] = (df[\"month\"] + 2) % 12\n    return df\n\n\ndef add_target_stats(train, test):\n    train = train.copy()\n    test = test.copy()\n\n    global_mean = train[\"yield\"].mean()\n    state_mean = train.groupby(\"state\")[\"yield\"].mean()\n    month_mean = train.groupby(\"month\")[\"yield\"].mean()\n    year_mean = train.groupby(\"year\")[\"yield\"].mean()\n    state_month_mean = train.groupby([\"state\", \"month\"])[\"yield\"].mean().to_dict()\n    state_year_mean = train.groupby([\"state\", \"year\"])[\"yield\"].mean().to_dict()\n\n    cols = [\n        \"state_yield_mean\",\n        \"month_yield_mean\",\n        \"year_yield_mean\",\n        \"state_month_yield_mean\",\n        \"state_year_yield_mean\",\n    ]\n\n    for df in (train, test):\n        df[\"state_yield_mean\"] = df[\"state\"].map(state_mean)\n        df[\"month_yield_mean\"] = df[\"month\"].map(month_mean)\n        df[\"year_yield_mean\"] = df[\"year\"].map(year_mean)\n\n        key_sm = list(zip(df[\"state\"], df[\"month\"]))\n        key_sy = list(zip(df[\"state\"], df[\"year\"]))\n        df[\"state_month_yield_mean\"] = pd.Series(key_sm, index=df.index).map(state_month_mean)\n        df[\"state_year_yield_mean\"] = pd.Series(key_sy, index=df.index).map(state_year_mean)\n\n        df[cols] = df[cols].fillna(global_mean)\n\n    return train, test\n\n\ndef train_and_predict():\n    # read\n    train = pd.read_csv(TRAIN_PATH)\n    test = pd.read_csv(TEST_PATH)\n\n    # encode & transform\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train, test = add_target_stats(train, test)\n\n    # feature selection\n    numeric_kinds = \"biufc\"\n    features = [col for col in train.columns if col != \"yield\" and train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # model\n    model = lgb.LGBMRegressor(\n        n_estimators=3000,\n        learning_rate=0.025,\n        num_leaves=96,\n        min_child_samples=5,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[target], categorical_feature=[\"state_enc\", \"month\", \"year\"])\n\n    # stacked linear calibration\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n    y_true = train[target].values\n\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    try:\n        coef, _, _, _ = np.linalg.lstsq(X, y_true, rcond=None)\n        X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n        test_pred = X_test @ coef\n    except np.linalg.LinAlgError:\n        var = np.var(train_pred)\n        if var > 0:\n            cov = np.cov(train_pred, y_true, bias=True)[0, 1]\n            b = cov / var\n            a = y_true.mean() - b * train_pred.mean()\n            test_pred = a + b * test_pred\n\n    # clip to training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n\n    # output\n    test_out = test[[\"year\", \"month\", \"state\"]].copy()\n    test_out[\"yield\"] = test_pred\n    out_path = OUTPUT_DIR / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model training finished, predictions saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    out_path = train_and_predict()\n    print(f\"Model trained; predictions saved to {out_path}\")\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n    # stacked linear calibration\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n    y_true = train[target].values\n\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    try:\n        coef, _, _, _ = np.linalg.lstsq(X, y_true, rcond=None)\n        X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n        test_pred = X_test @ coef\n    except np.linalg.LinAlgError:\n        var = np.var(train_pred)\n        if var > 0:\n            cov = np.cov(train_pred, y_true, bias=True)[0, 1]\n            b = cov / var\n            a = y_true.mean() - b * train_pred.mean()\n            test_pred = a + b * test_pred\n\n    # clip to training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n=======\n    # stacked linear calibration\n    train_pred = model.predict(train[features])\n    test_pred = model.predict(test[features])\n\n    train_baseline = train[\"state_month_yield_mean\"].values\n    test_baseline = test[\"state_month_yield_mean\"].values\n    y_true = train[target].values\n\n    # linear regression stack: y \u2248 w0 + w1 * model_pred + w2 * baseline\n    X = np.column_stack([np.ones_like(train_pred), train_pred, train_baseline])\n    try:\n        coef, _, _, _ = np.linalg.lstsq(X, y_true, rcond=None)\n        train_pred_cal = X @ coef\n        X_test = np.column_stack([np.ones_like(test_pred), test_pred, test_baseline])\n        test_pred = X_test @ coef\n    except np.linalg.LinAlgError:\n        # fallback to 1D variance-based calibration\n        var = np.var(train_pred)\n        if var > 0:\n            cov = np.cov(train_pred, y_true, bias=True)[0, 1]\n            b = cov / var\n            a = y_true.mean() - b * train_pred.mean()\n            train_pred_cal = a + b * train_pred\n            test_pred = a + b * test_pred\n        else:\n            # degenerate case: keep uncalibrated predictions\n            train_pred_cal = train_pred\n\n    # bias correction by (state, year) using training residuals\n    residual = y_true - train_pred_cal\n    tmp = train[[\"state\", \"year\"]].copy()\n    tmp[\"residual\"] = residual\n    sy_residual = tmp.groupby([\"state\", \"year\"])[\"residual\"].mean().to_dict()\n    key_sy_test = list(zip(test[\"state\"], test[\"year\"]))\n    correction = pd.Series(key_sy_test, index=test.index).map(sy_residual).fillna(0.0).to_numpy()\n    test_pred = test_pred + correction\n\n    # clip to training range\n    y_min, y_max = y_true.min(), y_true.max()\n    test_pred = np.clip(test_pred, y_min, y_max)\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}
{"id": "ecfa4e71-c8c8-4c85-808c-e139755e676e", "code": "\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\nrandom.seed(FIX_SEED)\nnp.random.seed(FIX_SEED)\ntorch.manual_seed(FIX_SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(FIX_SEED)\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\nclass TimeFeature:\n    def __init__(self):\n        pass\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        pass\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"()\"\n\n\nclass MillisecondOfMinute(TimeFeature):\n    \"\"\"Millisecond of minute encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        milliseconds = index.second * 1000 + index.microsecond // 1000\n        return milliseconds / 59999.0 - 0.5\n\n\nclass SecondOfMinute(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.second / 59.0 - 0.5\n\n\nclass MinuteOfHour(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5\n\n\nclass HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.hour / 23.0 - 0.5\n\n\nclass DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.dayofweek / 6.0 - 0.5\n\n\nclass DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.day - 1) / 30.0 - 0.5\n\n\nclass DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5\n\n\nclass MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.month - 1) / 11.0 - 0.5\n\n\nclass WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.isocalendar().week - 1) / 52.0 - 0.5\n\n\ndef time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"\n    Returns a list of time features that will be appropriate for the given frequency string.\n    Parameters\n    ----------\n    freq_str\n        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n    \"\"\"\n\n    features_by_offsets = {\n        offsets.YearEnd: [],\n        offsets.QuarterEnd: [MonthOfYear],\n        offsets.MonthEnd: [MonthOfYear],\n        offsets.Week: [DayOfMonth, WeekOfYear],\n        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Minute: [\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Second: [\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Milli: [\n            MillisecondOfMinute,\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n    }\n\n    offset = to_offset(freq_str)\n\n    for offset_type, feature_classes in features_by_offsets.items():\n        if isinstance(offset, offset_type):\n            return [cls() for cls in feature_classes]\n\n    supported_freq_msg = f\"\"\"\n    Unsupported frequency {freq_str}\n    The following frequencies are supported:\n        Y   - yearly\n            alias: A\n        M   - monthly\n        W   - weekly\n        D   - daily\n        B   - business days\n        H   - hourly\n        T   - minutely\n            alias: min\n        S   - secondly\n        ms  - milliseconds\n    \"\"\"\n    raise RuntimeError(supported_freq_msg)\n\n\ndef time_features(dates, freq='h'):\n    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# -----------------------------------------------------------------------------\n# Feature fusion helpers (derived from src/data_process/feature_fusion.py)\n# -----------------------------------------------------------------------------\n\nDEFAULT_FUSION_CONFIG = {\n    \"data_file\": \"data/iron/merged_data.csv\",\n    \"target_name\": \"FU00002776\",\n    \"output_file\": \"data/iron/datasets/final_features_01\u5408\u7ea6\u6536\u76d8\u4ef7_v2.csv\",\n    \"features\": {\n        \"supply\": [\n            {\n                \"feature_name\": \"ID01002312\",\n                \"file_path\": \"data/mysteel3/ID01002312_\u94c1\u77ff\uff1a\u8fdb\u53e3\uff1a\u5e93\u5b58\uff1a45\u4e2a\u6e2f\u53e3\uff08\u65e5\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"ffill\",\n            },\n            {\n                \"feature_name\": \"ID00186575\",\n                \"file_path\": \"data/mysteel3/ID00186575_\u94c1\u77ff\uff1a\u8239\u8236\u5230\u6e2f\u91cf\uff1a\u5317\u65b9\u6e2f\u53e3\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n        ],\n        \"demand\": [\n            {\n                \"feature_name\": \"ID00186100\",\n                \"file_path\": \"data/mysteel3/ID00186100_\u94c1\u77ff\uff1a\u8fdb\u53e3\uff1a\u65e5\u5747\u758f\u6e2f\u91cf\u5408\u8ba1\uff1a45\u4e2a\u6e2f\u53e3\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n            {\n                \"feature_name\": \"ID00183109\",\n                \"file_path\": \"data/mysteel3/ID00183109_247\u5bb6\u94a2\u94c1\u4f01\u4e1a\uff1a\u9ad8\u7089\u5f00\u5de5\u7387\uff1a\u4e2d\u56fd\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n        ],\n        \"macro\": [\n            {\n                \"feature_name\": \"GM0000033031\",\n                \"file_path\": \"data/mysteel3/GM0000033031_\u7f8e\u56fd\uff1a\u975e\u519c\u5c31\u4e1a\u4eba\u5458\uff1a\u5b63\u8c03\u4eba\u6570\u53d8\u52a8\uff08\u6708\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"monthly_lag1_daily\",\n            },\n            {\n                \"feature_name\": \"CM0000013263\",\n                \"file_path\": \"data/mysteel3/CM0000013263_\u7edf\u8ba1\u5c40\uff1a\u5236\u9020\u4e1aPMI\uff1a\u8d2d\u8fdb\u4ef7\uff08\u6708\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"monthly_lag1_daily\",\n            },\n        ],\n    },\n}\n\n\ndef resolve_path(base: Path, candidate: str | Path) -> Path:\n    candidate_path = Path(candidate)\n    if not candidate_path.is_absolute():\n        candidate_path = base / candidate_path\n    return candidate_path\n\n\ndef ensure_datetime_series(series: pd.Series) -> pd.Series:\n    if not pd.api.types.is_datetime64_any_dtype(series.index):\n        series.index = pd.to_datetime(series.index)\n    series = series.sort_index()\n    return series[~series.index.duplicated(keep='last')]\n\n\ndef infer_weekly_rule(index: pd.DatetimeIndex, fallback: str | None = None) -> str:\n    default_rule = fallback or 'W-FRI'\n    if index is None or len(index) == 0:\n        return default_rule\n    index = pd.to_datetime(index).sort_values()\n    freq = pd.infer_freq(index)\n    day_map = {0: 'MON', 1: 'TUE', 2: 'WED', 3: 'THU', 4: 'FRI', 5: 'SAT', 6: 'SUN'}\n    if freq and freq.startswith('W-'):\n        return freq\n    if freq == '7D':\n        anchor_day = int(index[-1].dayofweek)\n        return f\"W-{day_map.get(anchor_day, 'FRI')}\"\n    try:\n        anchor_day = int(pd.Series(index.dayofweek).mode().iloc[0])\n        return f\"W-{day_map.get(anchor_day, 'FRI')}\"\n    except Exception:\n        return default_rule\n\n\ndef resample_with_agg(series: pd.Series, rule: str, agg: str) -> pd.Series:\n    resampler = series.resample(rule, label='right', closed='right')\n    agg = (agg or 'last').lower()\n    if agg == 'mean':\n        return resampler.mean()\n    if agg == 'last':\n        return resampler.last()\n    if agg == 'sum':\n        return resampler.sum()\n    if agg == 'median':\n        return resampler.median()\n    raise ValueError(f\"Unsupported aggregation '{agg}' for rule '{rule}'\")\n\n\ndef apply_fill_method(\n    series: pd.Series,\n    method: str,\n    target_index: pd.DatetimeIndex,\n    weekly_rule: str,\n) -> pd.Series:\n    method = (method or 'ffill').lower()\n    aligned_series = ensure_datetime_series(series)\n    if method == 'weekly_lag1':\n        weekly_series = resample_with_agg(aligned_series, weekly_rule, 'last').shift(1)\n        filled = weekly_series.reindex(target_index)\n        return filled.ffill()\n    if method == 'monthly_lag1_daily':\n        monthly_series = resample_with_agg(aligned_series, 'M', 'last').shift(1)\n        filled = monthly_series.reindex(target_index)\n        return filled.ffill()\n    if method == 'ffill':\n        aligned = aligned_series.reindex(target_index)\n        filled = aligned.ffill()\n        if aligned.isna().sum() > aligned_series.isna().sum():\n            filled = filled.shift(1)\n        return filled\n    aligned = aligned_series.reindex(target_index)\n    return aligned.ffill()\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    fusion_cfg = copy.deepcopy(cfg.fusion_config or DEFAULT_FUSION_CONFIG)\n\n    data_path_str = cfg.raw_data_override or fusion_cfg.get('data_file')\n    if data_path_str is None:\n        raise ValueError(\"Fusion config must provide 'data_file'.\")\n    data_path = resolve_path(cfg.project_root, data_path_str)\n\n    data_df = pd.read_csv(data_path, parse_dates=['date'])\n    data_df = data_df.sort_values('date').drop_duplicates('date', keep='last')\n    data_df = data_df.set_index('date')\n\n    target_name = fusion_cfg['target_name']\n    target_freq = str(fusion_cfg.get('target_frequency', 'D')).upper()\n    target_agg = fusion_cfg.get('target_agg', 'last')\n\n    target_series = ensure_datetime_series(data_df[target_name])\n    weekly_rule = fusion_cfg.get('target_weekly_rule')\n\n    if target_freq.startswith('W'):\n        weekly_rule = weekly_rule or infer_weekly_rule(target_series.index)\n        target_series = resample_with_agg(target_series, weekly_rule, target_agg)\n    elif target_freq.startswith('M'):\n        target_series = resample_with_agg(target_series, 'M', target_agg)\n    target_df = target_series.dropna().to_frame(name='value')\n\n    target_index = target_df.index\n    weekly_rule = weekly_rule or infer_weekly_rule(target_index)\n\n    final_df = target_df.copy()\n    feature_groups = fusion_cfg.get('features', {})\n    for group_features in feature_groups.values():\n        for feature in group_features:\n            feature_name = feature['feature_name']\n            fill_method = feature.get('fill_method', 'ffill')\n            if feature_name not in data_df.columns:\n                raise KeyError(f\"Feature '{feature_name}' not found in raw dataset.\")\n            series = data_df[feature_name]\n            processed = apply_fill_method(series, fill_method, target_index, weekly_rule)\n            final_df[feature_name] = processed\n\n    final_df = final_df.sort_index().ffill().dropna()\n    final_df = final_df.reset_index().rename(columns={'index': 'date'})\n\n    return final_df\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\nclass DFTSeriesDecomp(nn.Module):\n    def __init__(self, top_k: int = 5):\n        super().__init__()\n        self.top_k = top_k\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        xf = torch.fft.rfft(x)\n        freq = torch.abs(xf)\n        freq[..., 0] = 0\n        top_k_freq, _ = torch.topk(freq, self.top_k)\n        xf = torch.where(freq > top_k_freq.min(), xf, torch.zeros_like(xf))\n        x_season = torch.fft.irfft(xf, n=x.size(1))\n        x_trend = x - x_season\n        return x_season, x_trend\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\nclass TemporalEmbedding(nn.Module):\n    def __init__(self, d_model: int, embed_type: str = 'fixed', freq: str = 'h'):\n        super().__init__()\n        minute_size = 4\n        hour_size = 24\n        weekday_size = 7\n        day_size = 32\n        month_size = 13\n        embed_cls = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n        if freq == 't':\n            self.minute_embed = embed_cls(minute_size, d_model)\n        if freq in ['t', 'h']:\n            self.hour_embed = embed_cls(hour_size, d_model)\n        self.weekday_embed = embed_cls(weekday_size, d_model)\n        self.day_embed = embed_cls(day_size, d_model)\n        self.month_embed = embed_cls(month_size, d_model)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.long()\n        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n        hour_x = self.hour_embed(x[:, :, 3]) if hasattr(self, 'hour_embed') else 0.\n        weekday_x = self.weekday_embed(x[:, :, 2])\n        day_x = self.day_embed(x[:, :, 1])\n        month_x = self.month_embed(x[:, :, 0])\n        return hour_x + weekday_x + day_x + month_x + minute_x\n\n\nclass FixedEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        w = torch.zeros(c_in, d_model).float()\n        position = torch.arange(0, c_in).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n        w[:, 0::2] = torch.sin(position * div_term)\n        w[:, 1::2] = torch.cos(position * div_term)\n        self.emb = nn.Embedding(c_in, d_model)\n        self.emb.weight = nn.Parameter(w, requires_grad=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.emb(x).detach()\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        if embed_type == 'timeF':\n            self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        else:\n            self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.seq_len = configs.seq_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.layer_norm = nn.LayerNorm(configs.d_model)\n        self.dropout = nn.Dropout(configs.dropout)\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method == 'moving_avg':\n            self.decomposition = SeriesDecomp(configs.moving_avg)\n        elif configs.decomp_method == 'dft_decomp':\n            self.decomposition = DFTSeriesDecomp(configs.top_k)\n        else:\n            raise ValueError('Unsupported decomposition method')\n        if configs.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list = []\n        trend_list = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            dir_out_channels = 1 if self.channel_independence == 1 else configs.c_out\n            self.direction_head = nn.Linear(configs.d_model, dir_out_channels, bias=True)\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n        elif self.task_name in ['imputation', 'anomaly_detection']:\n            out_dim = 1 if self.channel_independence == 1 else configs.c_out\n            self.projection_layer = nn.Linear(configs.d_model, out_dim, bias=True)\n        elif self.task_name == 'classification':\n            self.act = F.gelu\n            self.dropout = nn.Dropout(configs.dropout)\n            self.projection = nn.Linear(configs.d_model * configs.seq_len, configs.num_class)\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        self.future_time_embed = None\n        if self.use_future_temporal_feature and x_mark_dec is not None:\n            B, _, N = x_enc.size()\n            future_mark = x_mark_dec[:, -self.pred_len:, :]\n            if self.channel_independence == 1:\n                future_mark = future_mark.repeat(N, 1, 1)\n            self.future_time_embed = self.enc_embedding(None, future_mark)\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list = []\n        x_mark_processed = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out = torch.stack(dec_out_list, dim=-1).sum(-1)\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        dec_out_list = []\n        if self.channel_independence == 1:\n            x_list = x_list[0]\n            for i, enc_out in zip(range(len(x_list)), enc_out_list):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n                if self.use_future_temporal_feature and self.future_time_embed is not None:\n                    fusion = torch.cat([dec_out, self.future_time_embed], dim=-1)\n                    gate = torch.sigmoid(self.future_gate(fusion))\n                    dec_out = dec_out + gate * (self.future_time_embed - dec_out)\n                dir_logits = self.direction_head(dec_out)\n                dec_out = self.projection_layer(dec_out)\n                dec_out = dec_out.reshape(B, self.configs.c_out, self.pred_len).permute(0, 2, 1).contiguous()\n                dir_logits = dir_logits.reshape(B, 1, self.pred_len).permute(0, 2, 1).contiguous()\n                dec_out_list.append(dec_out)\n        else:\n            enc_inputs, out_res_list = x_list\n            for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n                if self.use_future_temporal_feature and self.future_time_embed is not None:\n                    fusion = torch.cat([dec_out, self.future_time_embed], dim=-1)\n                    gate = torch.sigmoid(self.future_gate(fusion))\n                    dec_out = dec_out + gate * (self.future_time_embed - dec_out)\n                dec_out = self.out_projection(dec_out, i, out_res)\n                dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        if self.task_name == 'imputation':\n            raise NotImplementedError('Imputation path is not required for this script')\n        if self.task_name == 'anomaly_detection':\n            raise NotImplementedError('Anomaly detection path is not required for this script')\n        if self.task_name == 'classification':\n            raise NotImplementedError('Classification path is not required for this script')\n        raise ValueError('Unknown task')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # Use current working directory as the base so the script can be run from anywhere\n    project_root: Path = Path.cwd()\n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    # EVOLVE-BLOCK-START\n    seq_len: int = 48\n    # EVOLVE-BLOCK-END\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 5\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\" / \"standalone_iron_daily\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    fusion_df = build_feature_fusion_dataset(cfg)\n    fusion_df[\"date\"] = pd.to_datetime(fusion_df[\"date\"])\n    fusion_df = fusion_df.sort_values(\"date\").reset_index(drop=True)\n    return fusion_df\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    ratios = cfg.split_ratio\n    train_ratio = float(ratios.get(\"train\", 0.8))\n    val_ratio = float(ratios.get(\"val\", 0.1))\n    test_ratio = float(ratios.get(\"test\", 0.1))\n    ratio_sum = train_ratio + val_ratio + test_ratio\n    if ratio_sum <= 0:\n        raise ValueError(\"Split ratios must sum to a positive value.\")\n    train_ratio /= ratio_sum\n    val_ratio /= ratio_sum\n    test_ratio = 1.0 - train_ratio - val_ratio\n\n    num_train = int(total_len * train_ratio)\n    num_val = int(total_len * val_ratio)\n    num_test = total_len - num_train - num_val\n    if num_train <= 0 or num_test <= 0:\n        raise ValueError(\"Insufficient data after applying split ratios.\")\n\n    border1s = [0, max(num_train - cfg.seq_len, 0), total_len - num_test - cfg.seq_len]\n    border2s = [num_train, num_train + num_val, total_len]\n    return border1s, border2s\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_custom_style_data(df: pd.DataFrame, cfg: IronDailyConfig):\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    feature_cols = [c for c in df.columns if c != 'date']\n    data_values = df[feature_cols].values.astype(np.float32)\n    total_len = len(df)\n    border1s, border2s = compute_split_borders(total_len, cfg)\n    split_info = {}\n    names = ['train', 'val', 'test']\n    for idx, name in enumerate(names):\n        b1, b2 = border1s[idx], border2s[idx]\n        data_slice = data_values[b1:b2]\n        stamp_slice = build_time_mark_array(df['date'].iloc[b1:b2], cfg)\n        split_info[name] = {\n            'data': data_slice,\n            'stamp': stamp_slice.astype(np.float32),\n            'length': len(data_slice),\n            'dates': df['date'].iloc[b1:b2].to_numpy(),\n        }\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n) -> Tuple[float, float, float, float]:\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            if cfg.down_sampling_layers == 0:\n                dec_inp = torch.zeros_like(batch_y[:, -cfg.pred_len:, :]).float()\n                dec_inp = torch.cat([batch_y[:, :cfg.label_len, :], dec_inp], dim=1).to(device)\n            else:\n                dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None)))\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef train_pipeline(cfg: IronDailyConfig) -> None:\n    print(\"1) \u6570\u636e\u5bf9\u9f50\uff1a\u5bf9\u539f\u59cb\u5e8f\u5217\u8fdb\u884c\u5de5\u4f5c\u65e5\u9891\u7387\u91cd\u91c7\u6837\u5e76\u586b\u5145...\")\n    fused_df = fuse_and_align_features(cfg)\n    print(f\"   \u5bf9\u9f50\u540e\u6837\u672c\u6570: {len(fused_df)}\")\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u590d\u7528\u65e5\u9891\u4efb\u52a1\u6240\u9700\u7684\u6240\u6709\u53d8\u6362...\")\n    fe_df = run_feature_engineering(fused_df, cfg)\n    print(f\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u5269\u4f59\u6837\u672c: {len(fe_df)}\")\n\n    print(\"3) \u6570\u636e\u96c6\u5207\u5206\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_custom_style_data(fe_df, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {split: len(loader.dataset) for split, loader in loaders.items()}\n    loader_steps = {split: len(loader) for split, loader in loaders.items()}\n    logger.info(\n        \"Dataset windows -> train:%d, val:%d, test:%d\",\n        dataset_sizes.get(\"train\", 0),\n        dataset_sizes.get(\"val\", 0),\n        dataset_sizes.get(\"test\", 0),\n    )\n    print(\n        f\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1atrain={dataset_sizes.get('train', 0)}, \"\n        f\"val={dataset_sizes.get('val', 0)}, test={dataset_sizes.get('test', 0)}\"\n    )\n    logger.info(\n        \"Loader steps/epoch -> train:%d, val:%d, test:%d\",\n        loader_steps.get(\"train\", 0),\n        loader_steps.get(\"val\", 0),\n        loader_steps.get(\"test\", 0),\n    )\n    print(\n        f\"   Dataloader\u6b65\u6570\uff1atrain={loader_steps.get('train', 0)}, \"\n        f\"val={loader_steps.get('val', 0)}, test={loader_steps.get('test', 0)}\"\n    )\n    test_dataset = loaders[\"test\"].dataset\n    print(\"   Test\u7a97\u53e3\u65f6\u95f4\u8de8\u5ea6\uff1a\")\n    for idx in range(len(test_dataset)):\n        start_date, end_date = test_dataset.window_bounds(idx)\n        print(f\"     \u6ce2\u6bb5{idx + 1:02d}: {start_date.strftime('%Y-%m-%d')} -> {end_date.strftime('%Y-%m-%d')}\")\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n    criterion = nn.MSELoss()\n    logger.info(\n        \"Training params | epochs=%d, batch=%d, lr=%.4f, patience=%d, seq_len=%d, pred_len=%d, d_model=%d, d_ff=%d\",\n        cfg.train_epochs,\n        cfg.batch_size,\n        cfg.learning_rate,\n        cfg.patience,\n        cfg.seq_len,\n        cfg.pred_len,\n        cfg.d_model,\n        cfg.d_ff,\n    )\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, batch={cfg.batch_size}, lr={cfg.learning_rate}, \"\n        f\"patience={cfg.patience}, seq_len={cfg.seq_len}, pred_len={cfg.pred_len}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}\"\n    )\n    logger.info(\n        \"Model depth | e_layers=%d, d_layers=%d, down_sampling_layers=%d, down_window=%d\",\n        cfg.e_layers,\n        cfg.d_layers,\n        cfg.down_sampling_layers,\n        cfg.down_sampling_window,\n    )\n    print(\n        f\"   \u6a21\u578b\u7ed3\u6784\uff1ae_layers={cfg.e_layers}, d_layers={cfg.d_layers}, \"\n        f\"down_layers={cfg.down_sampling_layers}, down_window={cfg.down_sampling_window}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            if cfg.down_sampling_layers == 0:\n                dec_inp = torch.zeros_like(batch_y[:, -cfg.pred_len:, :]).float()\n                dec_inp = torch.cat([batch_y[:, :cfg.label_len, :], dec_inp], dim=1).to(cfg.device_obj)\n            else:\n                dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            loss = criterion(pred_y, true_y)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Standalone iron_future_01_daily pipeline\")\n    parser.add_argument(\n        \"--raw_data\",\n        type=str,\n        default=None,\n        help=\"Path to the merged raw dataset (overrides config data_file)\",\n    )\n    args = parser.parse_args()\n\n    configuration = IronDailyConfig(raw_data_override=args.raw_data)\n    train_pipeline(configuration)\n    \n    # \u8fd0\u884c\u547d\u4ee4\uff1auv run src/test_task/iron_future_01_daily_pipeline.py --raw_data data/iron/merged_data.csv\n", "language": "python", "parent_id": "fc89f2c2-cc42-4e37-b900-a674e827d0ca", "generation": 1, "timestamp": 1764604918.1575704, "iteration_found": 3, "metrics": {"combined_score": 0.8870422840118408, "val_mse": 0.004151239059865475, "val_mae": 0.044654980301856995, "val_mape": 0.045757219195365906, "val_da": 0.49836199836199835, "test_mse": 0.000648976827505976, "test_mae": 0.019455835223197937, "test_mape": 0.01949221082031727, "test_da": 0.47474747474747475}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: 'seq_len: int = 48' to 'seq_len: int = 96  # Capture longer historical context'\nChange 2: 'learning_rate: float = 1e-2' to 'learning_rate: float = 5e-3  # Smaller learning rate for smoother convergence'\nChange 3: 'batch_size: int = 16' to 'batch_size: int = 32  # Larger batch size for better gradient estimates'\nChange 4: 'df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])' to 'df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 10, 15, 30])  # Add more window sizes'\nChange 5: 'dropout: float = 0.1' to 'dropout: float = 0.05  # Lower dropout for better learning'\nChange 6: 'optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)' to 'optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=1e-4)  # Use AdamW with weight decay'\nChange 7: Replace for epoch in range(cfg.train_epochs): with 2 lines\nChange 8: Replace return df_stamp[['month', 'day', 'weekday', 'hour']].values with 4 lines\nChange 9: 'patience: int = 5' to 'patience: int = 10  # Allow more epochs before early stopping'\nChange 10: Replace data_values = df[feature_cols].values.astype(np.float32) with 5 lines\nChange 11: Replace 4 lines with 6 lines\nChange 12: Replace print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\") with 2 lines\nChange 13: 'moving_avg: int = 25' to 'moving_avg: int = 15  # Adjust decomposition window'", "parent_metrics": {"combined_score": 0.8870422840118408, "val_mse": 0.004151239059865475, "val_mae": 0.044654980301856995, "val_mape": 0.045757219195365906, "val_da": 0.49836199836199835, "test_mse": 0.000648976827505976, "test_mae": 0.019455835223197937, "test_mape": 0.01949221082031727, "test_da": 0.47474747474747475}, "island": 0}, "prompts": {"diff_user": {"system": "You are optimizing a PyTorch time-series forecasting pipeline (TimeMixer) for iron 2601 daily price prediction with a 12-day horizon (input 48days, output 12 days).\nImprove data preparation, feature engineering, model hyperparameters, and training logic to lower validation/test MSE & MAPE and improve directional accuracy.\nKeep the code executable as a standalone script that loads from merged_data.csv via the evaluator. Avoid breaking the API expected by evaluator.py.\n", "user": "# Current Program Information\n- Fitness: 0.8870\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.0000 \u2192 0.8870\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Change 1: 'seq_len: int = 48' to 'seq_len: int = 96  # Capture longer historical context'\nChange 2: 'learning_rate: float = 1e-2' to 'learning_rate: float = 5e-3  # Smaller learning rate for smoother convergence'\nChange 3: 'batch_size: int = 16' to 'batch_size: int = 32  # Larger batch size for better gradient estimates'\nChange 4: 'df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])' to 'df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 10, 15, 30])  # Add more window sizes'\nChange 5: 'dropout: float = 0.1' to 'dropout: float = 0.05  # Lower dropout for better learning'\nChange 6: 'optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)' to 'optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=1e-4)  # Use AdamW with weight decay'\nChange 7: 'patience: int = 5' to 'patience: int = 10  # Allow more epochs before early stopping'\nChange 8: Replace for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]: with 2 lines\nChange 9: Replace avg_loss = running_loss / max(len(loaders[\"train\"]), 1) with 2 lines\nChange 10: Replace data_values = df[feature_cols].values.astype(np.float32) with 3 lines\nChange 11: Replace return df_stamp[['month', 'day', 'weekday', 'hour']].values with 3 lines\nChange 12: Replace print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\") with 2 lines\nChange 13: 'moving_avg: int = 25' to 'moving_avg: int = 15  # Adjust decomposition window'\nChange 14: Replace 4 lines with 6 lines\n- Metrics: combined_score: 0.0000, error: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test/2601daily/evaluator.py\", line 175, in evaluate\n    metrics = _train_and_eval(module, cfg)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test/2601daily/evaluator.py\", line 77, in _train_and_eval\n    split_info, feature_cols = module.prepare_custom_style_data(fe_df, cfg)\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\87273\\AppData\\Local\\Temp\\tmpl6k8z_7p.py\", line 1120, in prepare_custom_style_data\n    from sklearn.preprocessing import StandardScaler  # Import StandardScaler\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'sklearn'\n\n- Outcome: Regression in all metrics\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.8870, val_mse: 0.0042, val_mae: 0.0447, val_mape: 0.0458, val_da: 0.4984, test_mse: 0.0006, test_mae: 0.0195, test_mape: 0.0195, test_da: 0.4747\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.8870)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\nrandom.seed(FIX_SEED)\nnp.random.seed(FIX_SEED)\ntorch.manual_seed(FIX_SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(FIX_SEED)\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\nclass TimeFeature:\n    def __init__(self):\n        pass\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        pass\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"()\"\n\n\nclass MillisecondOfMinute(TimeFeature):\n    \"\"\"Millisecond of minute encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        milliseconds = index.second * 1000 + index.microsecond // 1000\n        return milliseconds / 59999.0 - 0.5\n\n\nclass SecondOfMinute(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.second / 59.0 - 0.5\n\n\nclass MinuteOfHour(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5\n\n\nclass HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.hour / 23.0 - 0.5\n\n\nclass DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.dayofweek / 6.0 - 0.5\n\n\nclass DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.day - 1) / 30.0 - 0.5\n\n\nclass DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5\n\n\nclass MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.month - 1) / 11.0 - 0.5\n\n\nclass WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.isocalendar().week - 1) / 52.0 - 0.5\n\n\ndef time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"\n    Returns a list of time features that will be appropriate for the given frequency string.\n    Parameters\n    ----------\n    freq_str\n        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n    \"\"\"\n\n    features_by_offsets = {\n        offsets.YearEnd: [],\n        offsets.QuarterEnd: [MonthOfYear],\n        offsets.MonthEnd: [MonthOfYear],\n        offsets.Week: [DayOfMonth, WeekOfYear],\n        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Minute: [\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Second: [\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Milli: [\n            MillisecondOfMinute,\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n    }\n\n    offset = to_offset(freq_str)\n\n    for offset_type, feature_classes in features_by_offsets.items():\n        if isinstance(offset, offset_type):\n            return [cls() for cls in feature_classes]\n\n    supported_freq_msg = f\"\"\"\n    Unsupported frequency {freq_str}\n    The following frequencies are supported:\n        Y   - yearly\n            alias: A\n        M   - monthly\n        W   - weekly\n        D   - daily\n        B   - business days\n        H   - hourly\n        T   - minutely\n            alias: min\n        S   - secondly\n        ms  - milliseconds\n    \"\"\"\n    raise RuntimeError(supported_freq_msg)\n\n\ndef time_features(dates, freq='h'):\n    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# -----------------------------------------------------------------------------\n# Feature fusion helpers (derived from src/data_process/feature_fusion.py)\n# -----------------------------------------------------------------------------\n\nDEFAULT_FUSION_CONFIG = {\n    \"data_file\": \"data/iron/merged_data.csv\",\n    \"target_name\": \"FU00002776\",\n    \"output_file\": \"data/iron/datasets/final_features_01\u5408\u7ea6\u6536\u76d8\u4ef7_v2.csv\",\n    \"features\": {\n        \"supply\": [\n            {\n                \"feature_name\": \"ID01002312\",\n                \"file_path\": \"data/mysteel3/ID01002312_\u94c1\u77ff\uff1a\u8fdb\u53e3\uff1a\u5e93\u5b58\uff1a45\u4e2a\u6e2f\u53e3\uff08\u65e5\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"ffill\",\n            },\n            {\n                \"feature_name\": \"ID00186575\",\n                \"file_path\": \"data/mysteel3/ID00186575_\u94c1\u77ff\uff1a\u8239\u8236\u5230\u6e2f\u91cf\uff1a\u5317\u65b9\u6e2f\u53e3\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n        ],\n        \"demand\": [\n            {\n                \"feature_name\": \"ID00186100\",\n                \"file_path\": \"data/mysteel3/ID00186100_\u94c1\u77ff\uff1a\u8fdb\u53e3\uff1a\u65e5\u5747\u758f\u6e2f\u91cf\u5408\u8ba1\uff1a45\u4e2a\u6e2f\u53e3\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n            {\n                \"feature_name\": \"ID00183109\",\n                \"file_path\": \"data/mysteel3/ID00183109_247\u5bb6\u94a2\u94c1\u4f01\u4e1a\uff1a\u9ad8\u7089\u5f00\u5de5\u7387\uff1a\u4e2d\u56fd\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n        ],\n        \"macro\": [\n            {\n                \"feature_name\": \"GM0000033031\",\n                \"file_path\": \"data/mysteel3/GM0000033031_\u7f8e\u56fd\uff1a\u975e\u519c\u5c31\u4e1a\u4eba\u5458\uff1a\u5b63\u8c03\u4eba\u6570\u53d8\u52a8\uff08\u6708\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"monthly_lag1_daily\",\n            },\n            {\n                \"feature_name\": \"CM0000013263\",\n                \"file_path\": \"data/mysteel3/CM0000013263_\u7edf\u8ba1\u5c40\uff1a\u5236\u9020\u4e1aPMI\uff1a\u8d2d\u8fdb\u4ef7\uff08\u6708\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"monthly_lag1_daily\",\n            },\n        ],\n    },\n}\n\n\ndef resolve_path(base: Path, candidate: str | Path) -> Path:\n    candidate_path = Path(candidate)\n    if not candidate_path.is_absolute():\n        candidate_path = base / candidate_path\n    return candidate_path\n\n\ndef ensure_datetime_series(series: pd.Series) -> pd.Series:\n    if not pd.api.types.is_datetime64_any_dtype(series.index):\n        series.index = pd.to_datetime(series.index)\n    series = series.sort_index()\n    return series[~series.index.duplicated(keep='last')]\n\n\ndef infer_weekly_rule(index: pd.DatetimeIndex, fallback: str | None = None) -> str:\n    default_rule = fallback or 'W-FRI'\n    if index is None or len(index) == 0:\n        return default_rule\n    index = pd.to_datetime(index).sort_values()\n    freq = pd.infer_freq(index)\n    day_map = {0: 'MON', 1: 'TUE', 2: 'WED', 3: 'THU', 4: 'FRI', 5: 'SAT', 6: 'SUN'}\n    if freq and freq.startswith('W-'):\n        return freq\n    if freq == '7D':\n        anchor_day = int(index[-1].dayofweek)\n        return f\"W-{day_map.get(anchor_day, 'FRI')}\"\n    try:\n        anchor_day = int(pd.Series(index.dayofweek).mode().iloc[0])\n        return f\"W-{day_map.get(anchor_day, 'FRI')}\"\n    except Exception:\n        return default_rule\n\n\ndef resample_with_agg(series: pd.Series, rule: str, agg: str) -> pd.Series:\n    resampler = series.resample(rule, label='right', closed='right')\n    agg = (agg or 'last').lower()\n    if agg == 'mean':\n        return resampler.mean()\n    if agg == 'last':\n        return resampler.last()\n    if agg == 'sum':\n        return resampler.sum()\n    if agg == 'median':\n        return resampler.median()\n    raise ValueError(f\"Unsupported aggregation '{agg}' for rule '{rule}'\")\n\n\ndef apply_fill_method(\n    series: pd.Series,\n    method: str,\n    target_index: pd.DatetimeIndex,\n    weekly_rule: str,\n) -> pd.Series:\n    method = (method or 'ffill').lower()\n    aligned_series = ensure_datetime_series(series)\n    if method == 'weekly_lag1':\n        weekly_series = resample_with_agg(aligned_series, weekly_rule, 'last').shift(1)\n        filled = weekly_series.reindex(target_index)\n        return filled.ffill()\n    if method == 'monthly_lag1_daily':\n        monthly_series = resample_with_agg(aligned_series, 'M', 'last').shift(1)\n        filled = monthly_series.reindex(target_index)\n        return filled.ffill()\n    if method == 'ffill':\n        aligned = aligned_series.reindex(target_index)\n        filled = aligned.ffill()\n        if aligned.isna().sum() > aligned_series.isna().sum():\n            filled = filled.shift(1)\n        return filled\n    aligned = aligned_series.reindex(target_index)\n    return aligned.ffill()\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    fusion_cfg = copy.deepcopy(cfg.fusion_config or DEFAULT_FUSION_CONFIG)\n\n    data_path_str = cfg.raw_data_override or fusion_cfg.get('data_file')\n    if data_path_str is None:\n        raise ValueError(\"Fusion config must provide 'data_file'.\")\n    data_path = resolve_path(cfg.project_root, data_path_str)\n\n    data_df = pd.read_csv(data_path, parse_dates=['date'])\n    data_df = data_df.sort_values('date').drop_duplicates('date', keep='last')\n    data_df = data_df.set_index('date')\n\n    target_name = fusion_cfg['target_name']\n    target_freq = str(fusion_cfg.get('target_frequency', 'D')).upper()\n    target_agg = fusion_cfg.get('target_agg', 'last')\n\n    target_series = ensure_datetime_series(data_df[target_name])\n    weekly_rule = fusion_cfg.get('target_weekly_rule')\n\n    if target_freq.startswith('W'):\n        weekly_rule = weekly_rule or infer_weekly_rule(target_series.index)\n        target_series = resample_with_agg(target_series, weekly_rule, target_agg)\n    elif target_freq.startswith('M'):\n        target_series = resample_with_agg(target_series, 'M', target_agg)\n    target_df = target_series.dropna().to_frame(name='value')\n\n    target_index = target_df.index\n    weekly_rule = weekly_rule or infer_weekly_rule(target_index)\n\n    final_df = target_df.copy()\n    feature_groups = fusion_cfg.get('features', {})\n    for group_features in feature_groups.values():\n        for feature in group_features:\n            feature_name = feature['feature_name']\n            fill_method = feature.get('fill_method', 'ffill')\n            if feature_name not in data_df.columns:\n                raise KeyError(f\"Feature '{feature_name}' not found in raw dataset.\")\n            series = data_df[feature_name]\n            processed = apply_fill_method(series, fill_method, target_index, weekly_rule)\n            final_df[feature_name] = processed\n\n    final_df = final_df.sort_index().ffill().dropna()\n    final_df = final_df.reset_index().rename(columns={'index': 'date'})\n\n    return final_df\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\nclass DFTSeriesDecomp(nn.Module):\n    def __init__(self, top_k: int = 5):\n        super().__init__()\n        self.top_k = top_k\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        xf = torch.fft.rfft(x)\n        freq = torch.abs(xf)\n        freq[..., 0] = 0\n        top_k_freq, _ = torch.topk(freq, self.top_k)\n        xf = torch.where(freq > top_k_freq.min(), xf, torch.zeros_like(xf))\n        x_season = torch.fft.irfft(xf, n=x.size(1))\n        x_trend = x - x_season\n        return x_season, x_trend\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\nclass TemporalEmbedding(nn.Module):\n    def __init__(self, d_model: int, embed_type: str = 'fixed', freq: str = 'h'):\n        super().__init__()\n        minute_size = 4\n        hour_size = 24\n        weekday_size = 7\n        day_size = 32\n        month_size = 13\n        embed_cls = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n        if freq == 't':\n            self.minute_embed = embed_cls(minute_size, d_model)\n        if freq in ['t', 'h']:\n            self.hour_embed = embed_cls(hour_size, d_model)\n        self.weekday_embed = embed_cls(weekday_size, d_model)\n        self.day_embed = embed_cls(day_size, d_model)\n        self.month_embed = embed_cls(month_size, d_model)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.long()\n        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n        hour_x = self.hour_embed(x[:, :, 3]) if hasattr(self, 'hour_embed') else 0.\n        weekday_x = self.weekday_embed(x[:, :, 2])\n        day_x = self.day_embed(x[:, :, 1])\n        month_x = self.month_embed(x[:, :, 0])\n        return hour_x + weekday_x + day_x + month_x + minute_x\n\n\nclass FixedEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        w = torch.zeros(c_in, d_model).float()\n        position = torch.arange(0, c_in).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n        w[:, 0::2] = torch.sin(position * div_term)\n        w[:, 1::2] = torch.cos(position * div_term)\n        self.emb = nn.Embedding(c_in, d_model)\n        self.emb.weight = nn.Parameter(w, requires_grad=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.emb(x).detach()\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        if embed_type == 'timeF':\n            self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        else:\n            self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.seq_len = configs.seq_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.layer_norm = nn.LayerNorm(configs.d_model)\n        self.dropout = nn.Dropout(configs.dropout)\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method == 'moving_avg':\n            self.decomposition = SeriesDecomp(configs.moving_avg)\n        elif configs.decomp_method == 'dft_decomp':\n            self.decomposition = DFTSeriesDecomp(configs.top_k)\n        else:\n            raise ValueError('Unsupported decomposition method')\n        if configs.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list = []\n        trend_list = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            dir_out_channels = 1 if self.channel_independence == 1 else configs.c_out\n            self.direction_head = nn.Linear(configs.d_model, dir_out_channels, bias=True)\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n        elif self.task_name in ['imputation', 'anomaly_detection']:\n            out_dim = 1 if self.channel_independence == 1 else configs.c_out\n            self.projection_layer = nn.Linear(configs.d_model, out_dim, bias=True)\n        elif self.task_name == 'classification':\n            self.act = F.gelu\n            self.dropout = nn.Dropout(configs.dropout)\n            self.projection = nn.Linear(configs.d_model * configs.seq_len, configs.num_class)\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        self.future_time_embed = None\n        if self.use_future_temporal_feature and x_mark_dec is not None:\n            B, _, N = x_enc.size()\n            future_mark = x_mark_dec[:, -self.pred_len:, :]\n            if self.channel_independence == 1:\n                future_mark = future_mark.repeat(N, 1, 1)\n            self.future_time_embed = self.enc_embedding(None, future_mark)\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list = []\n        x_mark_processed = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out = torch.stack(dec_out_list, dim=-1).sum(-1)\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        dec_out_list = []\n        if self.channel_independence == 1:\n            x_list = x_list[0]\n            for i, enc_out in zip(range(len(x_list)), enc_out_list):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n                if self.use_future_temporal_feature and self.future_time_embed is not None:\n                    fusion = torch.cat([dec_out, self.future_time_embed], dim=-1)\n                    gate = torch.sigmoid(self.future_gate(fusion))\n                    dec_out = dec_out + gate * (self.future_time_embed - dec_out)\n                dir_logits = self.direction_head(dec_out)\n                dec_out = self.projection_layer(dec_out)\n                dec_out = dec_out.reshape(B, self.configs.c_out, self.pred_len).permute(0, 2, 1).contiguous()\n                dir_logits = dir_logits.reshape(B, 1, self.pred_len).permute(0, 2, 1).contiguous()\n                dec_out_list.append(dec_out)\n        else:\n            enc_inputs, out_res_list = x_list\n            for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n                if self.use_future_temporal_feature and self.future_time_embed is not None:\n                    fusion = torch.cat([dec_out, self.future_time_embed], dim=-1)\n                    gate = torch.sigmoid(self.future_gate(fusion))\n                    dec_out = dec_out + gate * (self.future_time_embed - dec_out)\n                dec_out = self.out_projection(dec_out, i, out_res)\n                dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        if self.task_name == 'imputation':\n            raise NotImplementedError('Imputation path is not required for this script')\n        if self.task_name == 'anomaly_detection':\n            raise NotImplementedError('Anomaly detection path is not required for this script')\n        if self.task_name == 'classification':\n            raise NotImplementedError('Classification path is not required for this script')\n        raise ValueError('Unknown task')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # Use current working directory as the base so the script can be run from anywhere\n    project_root: Path = Path.cwd()\n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    # EVOLVE-BLOCK-START\n    seq_len: int = 48\n    # EVOLVE-BLOCK-END\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 5\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\" / \"standalone_iron_daily\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    fusion_df = build_feature_fusion_dataset(cfg)\n    fusion_df[\"date\"] = pd.to_datetime(fusion_df[\"date\"])\n    fusion_df = fusion_df.sort_values(\"date\").reset_index(drop=True)\n    return fusion_df\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    ratios = cfg.split_ratio\n    train_ratio = float(ratios.get(\"train\", 0.8))\n    val_ratio = float(ratios.get(\"val\", 0.1))\n    test_ratio = float(ratios.get(\"test\", 0.1))\n    ratio_sum = train_ratio + val_ratio + test_ratio\n    if ratio_sum <= 0:\n        raise ValueError(\"Split ratios must sum to a positive value.\")\n    train_ratio /= ratio_sum\n    val_ratio /= ratio_sum\n    test_ratio = 1.0 - train_ratio - val_ratio\n\n    num_train = int(total_len * train_ratio)\n    num_val = int(total_len * val_ratio)\n    num_test = total_len - num_train - num_val\n    if num_train <= 0 or num_test <= 0:\n        raise ValueError(\"Insufficient data after applying split ratios.\")\n\n    border1s = [0, max(num_train - cfg.seq_len, 0), total_len - num_test - cfg.seq_len]\n    border2s = [num_train, num_train + num_val, total_len]\n    return border1s, border2s\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_custom_style_data(df: pd.DataFrame, cfg: IronDailyConfig):\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    feature_cols = [c for c in df.columns if c != 'date']\n    data_values = df[feature_cols].values.astype(np.float32)\n    total_len = len(df)\n    border1s, border2s = compute_split_borders(total_len, cfg)\n    split_info = {}\n    names = ['train', 'val', 'test']\n    for idx, name in enumerate(names):\n        b1, b2 = border1s[idx], border2s[idx]\n        data_slice = data_values[b1:b2]\n        stamp_slice = build_time_mark_array(df['date'].iloc[b1:b2], cfg)\n        split_info[name] = {\n            'data': data_slice,\n            'stamp': stamp_slice.astype(np.float32),\n            'length': len(data_slice),\n            'dates': df['date'].iloc[b1:b2].to_numpy(),\n        }\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n) -> Tuple[float, float, float, float]:\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            if cfg.down_sampling_layers == 0:\n                dec_inp = torch.zeros_like(batch_y[:, -cfg.pred_len:, :]).float()\n                dec_inp = torch.cat([batch_y[:, :cfg.label_len, :], dec_inp], dim=1).to(device)\n            else:\n                dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None)))\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef train_pipeline(cfg: IronDailyConfig) -> None:\n    print(\"1) \u6570\u636e\u5bf9\u9f50\uff1a\u5bf9\u539f\u59cb\u5e8f\u5217\u8fdb\u884c\u5de5\u4f5c\u65e5\u9891\u7387\u91cd\u91c7\u6837\u5e76\u586b\u5145...\")\n    fused_df = fuse_and_align_features(cfg)\n    print(f\"   \u5bf9\u9f50\u540e\u6837\u672c\u6570: {len(fused_df)}\")\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u590d\u7528\u65e5\u9891\u4efb\u52a1\u6240\u9700\u7684\u6240\u6709\u53d8\u6362...\")\n    fe_df = run_feature_engineering(fused_df, cfg)\n    print(f\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u5269\u4f59\u6837\u672c: {len(fe_df)}\")\n\n    print(\"3) \u6570\u636e\u96c6\u5207\u5206\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_custom_style_data(fe_df, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {split: len(loader.dataset) for split, loader in loaders.items()}\n    loader_steps = {split: len(loader) for split, loader in loaders.items()}\n    logger.info(\n        \"Dataset windows -> train:%d, val:%d, test:%d\",\n        dataset_sizes.get(\"train\", 0),\n        dataset_sizes.get(\"val\", 0),\n        dataset_sizes.get(\"test\", 0),\n    )\n    print(\n        f\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1atrain={dataset_sizes.get('train', 0)}, \"\n        f\"val={dataset_sizes.get('val', 0)}, test={dataset_sizes.get('test', 0)}\"\n    )\n    logger.info(\n        \"Loader steps/epoch -> train:%d, val:%d, test:%d\",\n        loader_steps.get(\"train\", 0),\n        loader_steps.get(\"val\", 0),\n        loader_steps.get(\"test\", 0),\n    )\n    print(\n        f\"   Dataloader\u6b65\u6570\uff1atrain={loader_steps.get('train', 0)}, \"\n        f\"val={loader_steps.get('val', 0)}, test={loader_steps.get('test', 0)}\"\n    )\n    test_dataset = loaders[\"test\"].dataset\n    print(\"   Test\u7a97\u53e3\u65f6\u95f4\u8de8\u5ea6\uff1a\")\n    for idx in range(len(test_dataset)):\n        start_date, end_date = test_dataset.window_bounds(idx)\n        print(f\"     \u6ce2\u6bb5{idx + 1:02d}: {start_date.strftime('%Y-%m-%d')} -> {end_date.strftime('%Y-%m-%d')}\")\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n    criterion = nn.MSELoss()\n    logger.info(\n        \"Training params | epochs=%d, batch=%d, lr=%.4f, patience=%d, seq_len=%d, pred_len=%d, d_model=%d, d_ff=%d\",\n        cfg.train_epochs,\n        cfg.batch_size,\n        cfg.learning_rate,\n        cfg.patience,\n        cfg.seq_len,\n        cfg.pred_len,\n        cfg.d_model,\n        cfg.d_ff,\n    )\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, batch={cfg.batch_size}, lr={cfg.learning_rate}, \"\n        f\"patience={cfg.patience}, seq_len={cfg.seq_len}, pred_len={cfg.pred_len}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}\"\n    )\n    logger.info(\n        \"Model depth | e_layers=%d, d_layers=%d, down_sampling_layers=%d, down_window=%d\",\n        cfg.e_layers,\n        cfg.d_layers,\n        cfg.down_sampling_layers,\n        cfg.down_sampling_window,\n    )\n    print(\n        f\"   \u6a21\u578b\u7ed3\u6784\uff1ae_layers={cfg.e_layers}, d_layers={cfg.d_layers}, \"\n        f\"down_layers={cfg.down_sampling_layers}, down_window={cfg.down_sampling_window}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            if cfg.down_sampling_layers == 0:\n                dec_inp = torch.zeros_like(batch_y[:, -cfg.pred_len:, :]).float()\n                dec_inp = torch.cat([batch_y[:, :cfg.label_len, :], dec_inp], dim=1).to(cfg.device_obj)\n            else:\n                dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            loss = criterion(pred_y, true_y)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Standalone iron_future_01_daily pipeline\")\n    parser.add_argument(\n        \"--raw_data\",\n        type=str,\n        default=None,\n        help=\"Path to the merged raw dataset (overrides config data_file)\",\n    )\n    args = parser.parse_args()\n\n    configuration = IronDailyConfig(raw_data_override=args.raw_data)\n    train_pipeline(configuration)\n    \n    # \u8fd0\u884c\u547d\u4ee4\uff1auv run src/test_task/iron_future_01_daily_pipeline.py --raw_data data/iron/merged_data.csv\n\n```\nKey features: Performs well on combined_score (0.8870), Performs well on val_mse (0.0042), Performs well on val_mae (0.0447), Performs well on val_mape (0.0458), Performs well on val_da (0.4984), Performs well on test_mse (0.0006), Performs well on test_mae (0.0195), Performs well on test_mape (0.0195), Performs well on test_da (0.4747)\n\n### Program 2 (Score: 0.0000)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\nrandom.seed(FIX_SEED)\nnp.random.seed(FIX_SEED)\ntorch.manual_seed(FIX_SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(FIX_SEED)\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\nclass TimeFeature:\n    def __init__(self):\n        pass\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        pass\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"()\"\n\n\nclass MillisecondOfMinute(TimeFeature):\n    \"\"\"Millisecond of minute encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        milliseconds = index.second * 1000 + index.microsecond // 1000\n        return milliseconds / 59999.0 - 0.5\n\n\nclass SecondOfMinute(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.second / 59.0 - 0.5\n\n\nclass MinuteOfHour(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5\n\n\nclass HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.hour / 23.0 - 0.5\n\n\nclass DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.dayofweek / 6.0 - 0.5\n\n\nclass DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.day - 1) / 30.0 - 0.5\n\n\nclass DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5\n\n\nclass MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.month - 1) / 11.0 - 0.5\n\n\nclass WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.isocalendar().week - 1) / 52.0 - 0.5\n\n\ndef time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"\n    Returns a list of time features that will be appropriate for the given frequency string.\n    Parameters\n    ----------\n    freq_str\n        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n    \"\"\"\n\n    features_by_offsets = {\n        offsets.YearEnd: [],\n        offsets.QuarterEnd: [MonthOfYear],\n        offsets.MonthEnd: [MonthOfYear],\n        offsets.Week: [DayOfMonth, WeekOfYear],\n        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Minute: [\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Second: [\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Milli: [\n            MillisecondOfMinute,\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n    }\n\n    offset = to_offset(freq_str)\n\n    for offset_type, feature_classes in features_by_offsets.items():\n        if isinstance(offset, offset_type):\n            return [cls() for cls in feature_classes]\n\n    supported_freq_msg = f\"\"\"\n    Unsupported frequency {freq_str}\n    The following frequencies are supported:\n        Y   - yearly\n            alias: A\n        M   - monthly\n        W   - weekly\n        D   - daily\n        B   - business days\n        H   - hourly\n        T   - minutely\n            alias: min\n        S   - secondly\n        ms  - milliseconds\n    \"\"\"\n    raise RuntimeError(supported_freq_msg)\n\n\ndef time_features(dates, freq='h'):\n    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# -----------------------------------------------------------------------------\n# Feature fusion helpers (derived from src/data_process/feature_fusion.py)\n# -----------------------------------------------------------------------------\n\nDEFAULT_FUSION_CONFIG = {\n    \"data_file\": \"data/iron/merged_data.csv\",\n    \"target_name\": \"FU00002776\",\n    \"output_file\": \"data/iron/datasets/final_features_01\u5408\u7ea6\u6536\u76d8\u4ef7_v2.csv\",\n    \"features\": {\n        \"supply\": [\n            {\n                \"feature_name\": \"ID01002312\",\n                \"file_path\": \"data/mysteel3/ID01002312_\u94c1\u77ff\uff1a\u8fdb\u53e3\uff1a\u5e93\u5b58\uff1a45\u4e2a\u6e2f\u53e3\uff08\u65e5\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"ffill\",\n            },\n            {\n                \"feature_name\": \"ID00186575\",\n                \"file_path\": \"data/mysteel3/ID00186575_\u94c1\u77ff\uff1a\u8239\u8236\u5230\u6e2f\u91cf\uff1a\u5317\u65b9\u6e2f\u53e3\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n        ],\n        \"demand\": [\n            {\n                \"feature_name\": \"ID00186100\",\n                \"file_path\": \"data/mysteel3/ID00186100_\u94c1\u77ff\uff1a\u8fdb\u53e3\uff1a\u65e5\u5747\u758f\u6e2f\u91cf\u5408\u8ba1\uff1a45\u4e2a\u6e2f\u53e3\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n            {\n                \"feature_name\": \"ID00183109\",\n                \"file_path\": \"data/mysteel3/ID00183109_247\u5bb6\u94a2\u94c1\u4f01\u4e1a\uff1a\u9ad8\u7089\u5f00\u5de5\u7387\uff1a\u4e2d\u56fd\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n        ],\n        \"macro\": [\n            {\n                \"feature_name\": \"GM0000033031\",\n                \"file_path\": \"data/mysteel3/GM0000033031_\u7f8e\u56fd\uff1a\u975e\u519c\u5c31\u4e1a\u4eba\u5458\uff1a\u5b63\u8c03\u4eba\u6570\u53d8\u52a8\uff08\u6708\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"monthly_lag1_daily\",\n            },\n            {\n                \"feature_name\": \"CM0000013263\",\n                \"file_path\": \"data/mysteel3/CM0000013263_\u7edf\u8ba1\u5c40\uff1a\u5236\u9020\u4e1aPMI\uff1a\u8d2d\u8fdb\u4ef7\uff08\u6708\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"monthly_lag1_daily\",\n            },\n        ],\n    },\n}\n\n\ndef resolve_path(base: Path, candidate: str | Path) -> Path:\n    candidate_path = Path(candidate)\n    if not candidate_path.is_absolute():\n        candidate_path = base / candidate_path\n    return candidate_path\n\n\ndef ensure_datetime_series(series: pd.Series) -> pd.Series:\n    if not pd.api.types.is_datetime64_any_dtype(series.index):\n        series.index = pd.to_datetime(series.index)\n    series = series.sort_index()\n    return series[~series.index.duplicated(keep='last')]\n\n\ndef infer_weekly_rule(index: pd.DatetimeIndex, fallback: str | None = None) -> str:\n    default_rule = fallback or 'W-FRI'\n    if index is None or len(index) == 0:\n        return default_rule\n    index = pd.to_datetime(index).sort_values()\n    freq = pd.infer_freq(index)\n    day_map = {0: 'MON', 1: 'TUE', 2: 'WED', 3: 'THU', 4: 'FRI', 5: 'SAT', 6: 'SUN'}\n    if freq and freq.startswith('W-'):\n        return freq\n    if freq == '7D':\n        anchor_day = int(index[-1].dayofweek)\n        return f\"W-{day_map.get(anchor_day, 'FRI')}\"\n    try:\n        anchor_day = int(pd.Series(index.dayofweek).mode().iloc[0])\n        return f\"W-{day_map.get(anchor_day, 'FRI')}\"\n    except Exception:\n        return default_rule\n\n\ndef resample_with_agg(series: pd.Series, rule: str, agg: str) -> pd.Series:\n    resampler = series.resample(rule, label='right', closed='right')\n    agg = (agg or 'last').lower()\n    if agg == 'mean':\n        return resampler.mean()\n    if agg == 'last':\n        return resampler.last()\n    if agg == 'sum':\n        return resampler.sum()\n    if agg == 'median':\n        return resampler.median()\n    raise ValueError(f\"Unsupported aggregation '{agg}' for rule '{rule}'\")\n\n\ndef apply_fill_method(\n    series: pd.Series,\n    method: str,\n    target_index: pd.DatetimeIndex,\n    weekly_rule: str,\n) -> pd.Series:\n    method = (method or 'ffill').lower()\n    aligned_series = ensure_datetime_series(series)\n    if method == 'weekly_lag1':\n        weekly_series = resample_with_agg(aligned_series, weekly_rule, 'last').shift(1)\n        filled = weekly_series.reindex(target_index)\n        return filled.ffill()\n    if method == 'monthly_lag1_daily':\n        monthly_series = resample_with_agg(aligned_series, 'M', 'last').shift(1)\n        filled = monthly_series.reindex(target_index)\n        return filled.ffill()\n    if method == 'ffill':\n        aligned = aligned_series.reindex(target_index)\n        filled = aligned.ffill()\n        if aligned.isna().sum() > aligned_series.isna().sum():\n            filled = filled.shift(1)\n        return filled\n    aligned = aligned_series.reindex(target_index)\n    return aligned.ffill()\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    fusion_cfg = copy.deepcopy(cfg.fusion_config or DEFAULT_FUSION_CONFIG)\n\n    data_path_str = cfg.raw_data_override or fusion_cfg.get('data_file')\n    if data_path_str is None:\n        raise ValueError(\"Fusion config must provide 'data_file'.\")\n    data_path = resolve_path(cfg.project_root, data_path_str)\n\n    data_df = pd.read_csv(data_path, parse_dates=['date'])\n    data_df = data_df.sort_values('date').drop_duplicates('date', keep='last')\n    data_df = data_df.set_index('date')\n\n    target_name = fusion_cfg['target_name']\n    target_freq = str(fusion_cfg.get('target_frequency', 'D')).upper()\n    target_agg = fusion_cfg.get('target_agg', 'last')\n\n    target_series = ensure_datetime_series(data_df[target_name])\n    weekly_rule = fusion_cfg.get('target_weekly_rule')\n\n    if target_freq.startswith('W'):\n        weekly_rule = weekly_rule or infer_weekly_rule(target_series.index)\n        target_series = resample_with_agg(target_series, weekly_rule, target_agg)\n    elif target_freq.startswith('M'):\n        target_series = resample_with_agg(target_series, 'M', target_agg)\n    target_df = target_series.dropna().to_frame(name='value')\n\n    target_index = target_df.index\n    weekly_rule = weekly_rule or infer_weekly_rule(target_index)\n\n    final_df = target_df.copy()\n    feature_groups = fusion_cfg.get('features', {})\n    for group_features in feature_groups.values():\n        for feature in group_features:\n            feature_name = feature['feature_name']\n            fill_method = feature.get('fill_method', 'ffill')\n            if feature_name not in data_df.columns:\n                raise KeyError(f\"Feature '{feature_name}' not found in raw dataset.\")\n            series = data_df[feature_name]\n            processed = apply_fill_method(series, fill_method, target_index, weekly_rule)\n            final_df[feature_name] = processed\n\n    final_df = final_df.sort_index().ffill().dropna()\n    final_df = final_df.reset_index().rename(columns={'index': 'date'})\n\n    return final_df\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\nclass DFTSeriesDecomp(nn.Module):\n    def __init__(self, top_k: int = 5):\n        super().__init__()\n        self.top_k = top_k\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        xf = torch.fft.rfft(x)\n        freq = torch.abs(xf)\n        freq[..., 0] = 0\n        top_k_freq, _ = torch.topk(freq, self.top_k)\n        xf = torch.where(freq > top_k_freq.min(), xf, torch.zeros_like(xf))\n        x_season = torch.fft.irfft(xf, n=x.size(1))\n        x_trend = x - x_season\n        return x_season, x_trend\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\nclass TemporalEmbedding(nn.Module):\n    def __init__(self, d_model: int, embed_type: str = 'fixed', freq: str = 'h'):\n        super().__init__()\n        minute_size = 4\n        hour_size = 24\n        weekday_size = 7\n        day_size = 32\n        month_size = 13\n        embed_cls = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n        if freq == 't':\n            self.minute_embed = embed_cls(minute_size, d_model)\n        if freq in ['t', 'h']:\n            self.hour_embed = embed_cls(hour_size, d_model)\n        self.weekday_embed = embed_cls(weekday_size, d_model)\n        self.day_embed = embed_cls(day_size, d_model)\n        self.month_embed = embed_cls(month_size, d_model)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.long()\n        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n        hour_x = self.hour_embed(x[:, :, 3]) if hasattr(self, 'hour_embed') else 0.\n        weekday_x = self.weekday_embed(x[:, :, 2])\n        day_x = self.day_embed(x[:, :, 1])\n        month_x = self.month_embed(x[:, :, 0])\n        return hour_x + weekday_x + day_x + month_x + minute_x\n\n\nclass FixedEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        w = torch.zeros(c_in, d_model).float()\n        position = torch.arange(0, c_in).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n        w[:, 0::2] = torch.sin(position * div_term)\n        w[:, 1::2] = torch.cos(position * div_term)\n        self.emb = nn.Embedding(c_in, d_model)\n        self.emb.weight = nn.Parameter(w, requires_grad=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.emb(x).detach()\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        if embed_type == 'timeF':\n            self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        else:\n            self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.seq_len = configs.seq_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.layer_norm = nn.LayerNorm(configs.d_model)\n        self.dropout = nn.Dropout(configs.dropout)\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method == 'moving_avg':\n            self.decomposition = SeriesDecomp(configs.moving_avg)\n        elif configs.decomp_method == 'dft_decomp':\n            self.decomposition = DFTSeriesDecomp(configs.top_k)\n        else:\n            raise ValueError('Unsupported decomposition method')\n        if configs.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list = []\n        trend_list = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            dir_out_channels = 1 if self.channel_independence == 1 else configs.c_out\n            self.direction_head = nn.Linear(configs.d_model, dir_out_channels, bias=True)\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n        elif self.task_name in ['imputation', 'anomaly_detection']:\n            out_dim = 1 if self.channel_independence == 1 else configs.c_out\n            self.projection_layer = nn.Linear(configs.d_model, out_dim, bias=True)\n        elif self.task_name == 'classification':\n            self.act = F.gelu\n            self.dropout = nn.Dropout(configs.dropout)\n            self.projection = nn.Linear(configs.d_model * configs.seq_len, configs.num_class)\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        self.future_time_embed = None\n        if self.use_future_temporal_feature and x_mark_dec is not None:\n            B, _, N = x_enc.size()\n            future_mark = x_mark_dec[:, -self.pred_len:, :]\n            if self.channel_independence == 1:\n                future_mark = future_mark.repeat(N, 1, 1)\n            self.future_time_embed = self.enc_embedding(None, future_mark)\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list = []\n        x_mark_processed = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out = torch.stack(dec_out_list, dim=-1).sum(-1)\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        dec_out_list = []\n        if self.channel_independence == 1:\n            x_list = x_list[0]\n            for i, enc_out in zip(range(len(x_list)), enc_out_list):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n                if self.use_future_temporal_feature and self.future_time_embed is not None:\n                    fusion = torch.cat([dec_out, self.future_time_embed], dim=-1)\n                    gate = torch.sigmoid(self.future_gate(fusion))\n                    dec_out = dec_out + gate * (self.future_time_embed - dec_out)\n                dir_logits = self.direction_head(dec_out)\n                dec_out = self.projection_layer(dec_out)\n                dec_out = dec_out.reshape(B, self.configs.c_out, self.pred_len).permute(0, 2, 1).contiguous()\n                dir_logits = dir_logits.reshape(B, 1, self.pred_len).permute(0, 2, 1).contiguous()\n                dec_out_list.append(dec_out)\n        else:\n            enc_inputs, out_res_list = x_list\n            for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n                if self.use_future_temporal_feature and self.future_time_embed is not None:\n                    fusion = torch.cat([dec_out, self.future_time_embed], dim=-1)\n                    gate = torch.sigmoid(self.future_gate(fusion))\n                    dec_out = dec_out + gate * (self.future_time_embed - dec_out)\n                dec_out = self.out_projection(dec_out, i, out_res)\n                dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        if self.task_name == 'imputation':\n            raise NotImplementedError('Imputation path is not required for this script')\n        if self.task_name == 'anomaly_detection':\n            raise NotImplementedError('Anomaly detection path is not required for this script')\n        if self.task_name == 'classification':\n            raise NotImplementedError('Classification path is not required for this script')\n        raise ValueError('Unknown task')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # Use current working directory as the base so the script can be run from anywhere\n    project_root: Path = Path.cwd()\n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    # EVOLVE-BLOCK-START\n    seq_len: int = 96  # Capture longer historical context\n    # EVOLVE-BLOCK-END\n    batch_size: int = 32  # Larger batch size for better gradient estimates\n    learning_rate: float = 5e-3  # Smaller learning rate for smoother convergence\n    train_epochs: int = 10\n    patience: int = 10  # Allow more epochs before early stopping\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.05  # Lower dropout for better learning\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 15  # Adjust decomposition window\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\" / \"standalone_iron_daily\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    fusion_df = build_feature_fusion_dataset(cfg)\n    fusion_df[\"date\"] = pd.to_datetime(fusion_df[\"date\"])\n    fusion_df = fusion_df.sort_values(\"date\").reset_index(drop=True)\n    return fusion_df\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 10, 15, 30])  # Add more window sizes\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    ratios = cfg.split_ratio\n    train_ratio = float(ratios.get(\"train\", 0.8))\n    val_ratio = float(ratios.get(\"val\", 0.1))\n    test_ratio = float(ratios.get(\"test\", 0.1))\n    ratio_sum = train_ratio + val_ratio + test_ratio\n    if ratio_sum <= 0:\n        raise ValueError(\"Split ratios must sum to a positive value.\")\n    train_ratio /= ratio_sum\n    val_ratio /= ratio_sum\n    test_ratio = 1.0 - train_ratio - val_ratio\n\n    num_train = int(total_len * train_ratio)\n    num_val = int(total_len * val_ratio)\n    num_test = total_len - num_train - num_val\n    if num_train <= 0 or num_test <= 0:\n        raise ValueError(\"Insufficient data after applying split ratios.\")\n\n    border1s = [0, max(num_train - cfg.seq_len, 0), total_len - num_test - cfg.seq_len]\n    border2s = [num_train, num_train + num_val, total_len]\n    return border1s, border2s\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    df_stamp['quarter'] = df_stamp['date'].dt.quarter  # Add quarter\n    df_stamp['year'] = df_stamp['date'].dt.year - df_stamp['date'].dt.year.min()  # Normalize year\n    return df_stamp[['month', 'day', 'weekday', 'hour', 'quarter', 'year']].values  # Include new features\n\n\ndef prepare_custom_style_data(df: pd.DataFrame, cfg: IronDailyConfig):\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    feature_cols = [c for c in df.columns if c != 'date']\n    from sklearn.preprocessing import StandardScaler  # Import StandardScaler\n    scaler = StandardScaler()  # Initialize scaler\n    data_values = scaler.fit_transform(df[feature_cols].values.astype(np.float32))  # Scale features\n    total_len = len(df)\n    border1s, border2s = compute_split_borders(total_len, cfg)\n    split_info = {}\n    names = ['train', 'val', 'test']\n    for idx, name in enumerate(names):\n        b1, b2 = border1s[idx], border2s[idx]\n        data_slice = data_values[b1:b2]\n        stamp_slice = build_time_mark_array(df['date'].iloc[b1:b2], cfg)\n        split_info[name] = {\n            'data': data_slice,\n            'stamp': stamp_slice.astype(np.float32),\n            'length': len(data_slice),\n            'dates': df['date'].iloc[b1:b2].to_numpy(),\n        }\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n) -> Tuple[float, float, float, float]:\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            if cfg.down_sampling_layers == 0:\n                dec_inp = torch.zeros_like(batch_y[:, -cfg.pred_len:, :]).float()\n                dec_inp = torch.cat([batch_y[:, :cfg.label_len, :], dec_inp], dim=1).to(device)\n            else:\n                dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None)))\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef train_pipeline(cfg: IronDailyConfig) -> None:\n    print(\"1) \u6570\u636e\u5bf9\u9f50\uff1a\u5bf9\u539f\u59cb\u5e8f\u5217\u8fdb\u884c\u5de5\u4f5c\u65e5\u9891\u7387\u91cd\u91c7\u6837\u5e76\u586b\u5145...\")\n    fused_df = fuse_and_align_features(cfg)\n    print(f\"   \u5bf9\u9f50\u540e\u6837\u672c\u6570: {len(fused_df)}\")\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u590d\u7528\u65e5\u9891\u4efb\u52a1\u6240\u9700\u7684\u6240\u6709\u53d8\u6362...\")\n    fe_df = run_feature_engineering(fused_df, cfg)\n    print(f\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u5269\u4f59\u6837\u672c: {len(fe_df)}\")\n\n    print(\"3) \u6570\u636e\u96c6\u5207\u5206\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_custom_style_data(fe_df, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {split: len(loader.dataset) for split, loader in loaders.items()}\n    loader_steps = {split: len(loader) for split, loader in loaders.items()}\n    logger.info(\n        \"Dataset windows -> train:%d, val:%d, test:%d\",\n        dataset_sizes.get(\"train\", 0),\n        dataset_sizes.get(\"val\", 0),\n        dataset_sizes.get(\"test\", 0),\n    )\n    print(\n        f\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1atrain={dataset_sizes.get('train', 0)}, \"\n        f\"val={dataset_sizes.get('val', 0)}, test={dataset_sizes.get('test', 0)}\"\n    )\n    logger.info(\n        \"Loader steps/epoch -> train:%d, val:%d, test:%d\",\n        loader_steps.get(\"train\", 0),\n        loader_steps.get(\"val\", 0),\n        loader_steps.get(\"test\", 0),\n    )\n    print(\n        f\"   Dataloader\u6b65\u6570\uff1atrain={loader_steps.get('train', 0)}, \"\n        f\"val={loader_steps.get('val', 0)}, test={loader_steps.get('test', 0)}\"\n    )\n    test_dataset = loaders[\"test\"].dataset\n    print(\"   Test\u7a97\u53e3\u65f6\u95f4\u8de8\u5ea6\uff1a\")\n    for idx in range(len(test_dataset)):\n        start_date, end_date = test_dataset.window_bounds(idx)\n        print(f\"     \u6ce2\u6bb5{idx + 1:02d}: {start_date.strftime('%Y-%m-%d')} -> {end_date.strftime('%Y-%m-%d')}\")\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=1e-4)  # Use AdamW with weight decay\n    criterion = nn.MSELoss()\n    logger.info(\n        \"Training params | epochs=%d, batch=%d, lr=%.4f, patience=%d, seq_len=%d, pred_len=%d, d_model=%d, d_ff=%d\",\n        cfg.train_epochs,\n        cfg.batch_size,\n        cfg.learning_rate,\n        cfg.patience,\n        cfg.seq_len,\n        cfg.pred_len,\n        cfg.d_model,\n        cfg.d_ff,\n    )\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, batch={cfg.batch_size}, lr={cfg.learning_rate}, \"\n        f\"patience={cfg.patience}, seq_len={cfg.seq_len}, pred_len={cfg.pred_len}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}\"\n    )\n    logger.info(\n        \"Model depth | e_layers=%d, d_layers=%d, down_sampling_layers=%d, down_window=%d\",\n        cfg.e_layers,\n        cfg.d_layers,\n        cfg.down_sampling_layers,\n        cfg.down_sampling_window,\n    )\n    print(\n        f\"   \u6a21\u578b\u7ed3\u6784\uff1ae_layers={cfg.e_layers}, d_layers={cfg.d_layers}, \"\n        f\"down_layers={cfg.down_sampling_layers}, down_window={cfg.down_sampling_window}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)  # Add LR scheduler\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            if cfg.down_sampling_layers == 0:\n                dec_inp = torch.zeros_like(batch_y[:, -cfg.pred_len:, :]).float()\n                dec_inp = torch.cat([batch_y[:, :cfg.label_len, :], dec_inp], dim=1).to(cfg.device_obj)\n            else:\n                dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            loss = criterion(pred_y, true_y)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        scheduler.step(avg_loss)  # Step the scheduler after each epoch\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        logger.info(\"Epoch %d | train_loss=%.4f, val_mse=%.4f\", epoch + 1, avg_loss, val_mse)  # Log metrics\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n    np.save(cfg.checkpoint_dir / \"test_predictions.npy\", preds_arr)  # Save test predictions for analysis\n    np.save(cfg.checkpoint_dir / \"test_truth.npy\", trues_arr)  # Save test ground truth\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Standalone iron_future_01_daily pipeline\")\n    parser.add_argument(\n        \"--raw_data\",\n        type=str,\n        default=None,\n        help=\"Path to the merged raw dataset (overrides config data_file)\",\n    )\n    args = parser.parse_args()\n\n    configuration = IronDailyConfig(raw_data_override=args.raw_data)\n    train_pipeline(configuration)\n    \n    # \u8fd0\u884c\u547d\u4ee4\uff1auv run src/test_task/iron_future_01_daily_pipeline.py --raw_data data/iron/merged_data.csv\n\n```\nKey features: Performs well on combined_score (0.0000), Performs well on error (No module named 'sklearn'\nTraceback (most recent call last):\n  File \"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test/2601daily/evaluator.py\", line 175, in evaluate\n    metrics = _train_and_eval(module, cfg)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test/2601daily/evaluator.py\", line 77, in _train_and_eval\n    split_info, feature_cols = module.prepare_custom_style_data(fe_df, cfg)\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\87273\\AppData\\Local\\Temp\\tmpl6k8z_7p.py\", line 1120, in prepare_custom_style_data\n    from sklearn.preprocessing import StandardScaler  # Import StandardScaler\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'sklearn'\n)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\nrandom.seed(FIX_SEED)\nnp.random.seed(FIX_SEED)\ntorch.manual_seed(FIX_SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(FIX_SEED)\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\nclass TimeFeature:\n    def __init__(self):\n        pass\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        pass\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"()\"\n\n\nclass MillisecondOfMinute(TimeFeature):\n    \"\"\"Millisecond of minute encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        milliseconds = index.second * 1000 + index.microsecond // 1000\n        return milliseconds / 59999.0 - 0.5\n\n\nclass SecondOfMinute(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.second / 59.0 - 0.5\n\n\nclass MinuteOfHour(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5\n\n\nclass HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.hour / 23.0 - 0.5\n\n\nclass DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.dayofweek / 6.0 - 0.5\n\n\nclass DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.day - 1) / 30.0 - 0.5\n\n\nclass DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5\n\n\nclass MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.month - 1) / 11.0 - 0.5\n\n\nclass WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.isocalendar().week - 1) / 52.0 - 0.5\n\n\ndef time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"\n    Returns a list of time features that will be appropriate for the given frequency string.\n    Parameters\n    ----------\n    freq_str\n        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n    \"\"\"\n\n    features_by_offsets = {\n        offsets.YearEnd: [],\n        offsets.QuarterEnd: [MonthOfYear],\n        offsets.MonthEnd: [MonthOfYear],\n        offsets.Week: [DayOfMonth, WeekOfYear],\n        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Minute: [\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Second: [\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Milli: [\n            MillisecondOfMinute,\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n    }\n\n    offset = to_offset(freq_str)\n\n    for offset_type, feature_classes in features_by_offsets.items():\n        if isinstance(offset, offset_type):\n            return [cls() for cls in feature_classes]\n\n    supported_freq_msg = f\"\"\"\n    Unsupported frequency {freq_str}\n    The following frequencies are supported:\n        Y   - yearly\n            alias: A\n        M   - monthly\n        W   - weekly\n        D   - daily\n        B   - business days\n        H   - hourly\n        T   - minutely\n            alias: min\n        S   - secondly\n        ms  - milliseconds\n    \"\"\"\n    raise RuntimeError(supported_freq_msg)\n\n\ndef time_features(dates, freq='h'):\n    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# -----------------------------------------------------------------------------\n# Feature fusion helpers (derived from src/data_process/feature_fusion.py)\n# -----------------------------------------------------------------------------\n\nDEFAULT_FUSION_CONFIG = {\n    \"data_file\": \"data/iron/merged_data.csv\",\n    \"target_name\": \"FU00002776\",\n    \"output_file\": \"data/iron/datasets/final_features_01\u5408\u7ea6\u6536\u76d8\u4ef7_v2.csv\",\n    \"features\": {\n        \"supply\": [\n            {\n                \"feature_name\": \"ID01002312\",\n                \"file_path\": \"data/mysteel3/ID01002312_\u94c1\u77ff\uff1a\u8fdb\u53e3\uff1a\u5e93\u5b58\uff1a45\u4e2a\u6e2f\u53e3\uff08\u65e5\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"ffill\",\n            },\n            {\n                \"feature_name\": \"ID00186575\",\n                \"file_path\": \"data/mysteel3/ID00186575_\u94c1\u77ff\uff1a\u8239\u8236\u5230\u6e2f\u91cf\uff1a\u5317\u65b9\u6e2f\u53e3\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n        ],\n        \"demand\": [\n            {\n                \"feature_name\": \"ID00186100\",\n                \"file_path\": \"data/mysteel3/ID00186100_\u94c1\u77ff\uff1a\u8fdb\u53e3\uff1a\u65e5\u5747\u758f\u6e2f\u91cf\u5408\u8ba1\uff1a45\u4e2a\u6e2f\u53e3\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n            {\n                \"feature_name\": \"ID00183109\",\n                \"file_path\": \"data/mysteel3/ID00183109_247\u5bb6\u94a2\u94c1\u4f01\u4e1a\uff1a\u9ad8\u7089\u5f00\u5de5\u7387\uff1a\u4e2d\u56fd\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n        ],\n        \"macro\": [\n            {\n                \"feature_name\": \"GM0000033031\",\n                \"file_path\": \"data/mysteel3/GM0000033031_\u7f8e\u56fd\uff1a\u975e\u519c\u5c31\u4e1a\u4eba\u5458\uff1a\u5b63\u8c03\u4eba\u6570\u53d8\u52a8\uff08\u6708\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"monthly_lag1_daily\",\n            },\n            {\n                \"feature_name\": \"CM0000013263\",\n                \"file_path\": \"data/mysteel3/CM0000013263_\u7edf\u8ba1\u5c40\uff1a\u5236\u9020\u4e1aPMI\uff1a\u8d2d\u8fdb\u4ef7\uff08\u6708\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"monthly_lag1_daily\",\n            },\n        ],\n    },\n}\n\n\ndef resolve_path(base: Path, candidate: str | Path) -> Path:\n    candidate_path = Path(candidate)\n    if not candidate_path.is_absolute():\n        candidate_path = base / candidate_path\n    return candidate_path\n\n\ndef ensure_datetime_series(series: pd.Series) -> pd.Series:\n    if not pd.api.types.is_datetime64_any_dtype(series.index):\n        series.index = pd.to_datetime(series.index)\n    series = series.sort_index()\n    return series[~series.index.duplicated(keep='last')]\n\n\ndef infer_weekly_rule(index: pd.DatetimeIndex, fallback: str | None = None) -> str:\n    default_rule = fallback or 'W-FRI'\n    if index is None or len(index) == 0:\n        return default_rule\n    index = pd.to_datetime(index).sort_values()\n    freq = pd.infer_freq(index)\n    day_map = {0: 'MON', 1: 'TUE', 2: 'WED', 3: 'THU', 4: 'FRI', 5: 'SAT', 6: 'SUN'}\n    if freq and freq.startswith('W-'):\n        return freq\n    if freq == '7D':\n        anchor_day = int(index[-1].dayofweek)\n        return f\"W-{day_map.get(anchor_day, 'FRI')}\"\n    try:\n        anchor_day = int(pd.Series(index.dayofweek).mode().iloc[0])\n        return f\"W-{day_map.get(anchor_day, 'FRI')}\"\n    except Exception:\n        return default_rule\n\n\ndef resample_with_agg(series: pd.Series, rule: str, agg: str) -> pd.Series:\n    resampler = series.resample(rule, label='right', closed='right')\n    agg = (agg or 'last').lower()\n    if agg == 'mean':\n        return resampler.mean()\n    if agg == 'last':\n        return resampler.last()\n    if agg == 'sum':\n        return resampler.sum()\n    if agg == 'median':\n        return resampler.median()\n    raise ValueError(f\"Unsupported aggregation '{agg}' for rule '{rule}'\")\n\n\ndef apply_fill_method(\n    series: pd.Series,\n    method: str,\n    target_index: pd.DatetimeIndex,\n    weekly_rule: str,\n) -> pd.Series:\n    method = (method or 'ffill').lower()\n    aligned_series = ensure_datetime_series(series)\n    if method == 'weekly_lag1':\n        weekly_series = resample_with_agg(aligned_series, weekly_rule, 'last').shift(1)\n        filled = weekly_series.reindex(target_index)\n        return filled.ffill()\n    if method == 'monthly_lag1_daily':\n        monthly_series = resample_with_agg(aligned_series, 'M', 'last').shift(1)\n        filled = monthly_series.reindex(target_index)\n        return filled.ffill()\n    if method == 'ffill':\n        aligned = aligned_series.reindex(target_index)\n        filled = aligned.ffill()\n        if aligned.isna().sum() > aligned_series.isna().sum():\n            filled = filled.shift(1)\n        return filled\n    aligned = aligned_series.reindex(target_index)\n    return aligned.ffill()\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    fusion_cfg = copy.deepcopy(cfg.fusion_config or DEFAULT_FUSION_CONFIG)\n\n    data_path_str = cfg.raw_data_override or fusion_cfg.get('data_file')\n    if data_path_str is None:\n        raise ValueError(\"Fusion config must provide 'data_file'.\")\n    data_path = resolve_path(cfg.project_root, data_path_str)\n\n    data_df = pd.read_csv(data_path, parse_dates=['date'])\n    data_df = data_df.sort_values('date').drop_duplicates('date', keep='last')\n    data_df = data_df.set_index('date')\n\n    target_name = fusion_cfg['target_name']\n    target_freq = str(fusion_cfg.get('target_frequency', 'D')).upper()\n    target_agg = fusion_cfg.get('target_agg', 'last')\n\n    target_series = ensure_datetime_series(data_df[target_name])\n    weekly_rule = fusion_cfg.get('target_weekly_rule')\n\n    if target_freq.startswith('W'):\n        weekly_rule = weekly_rule or infer_weekly_rule(target_series.index)\n        target_series = resample_with_agg(target_series, weekly_rule, target_agg)\n    elif target_freq.startswith('M'):\n        target_series = resample_with_agg(target_series, 'M', target_agg)\n    target_df = target_series.dropna().to_frame(name='value')\n\n    target_index = target_df.index\n    weekly_rule = weekly_rule or infer_weekly_rule(target_index)\n\n    final_df = target_df.copy()\n    feature_groups = fusion_cfg.get('features', {})\n    for group_features in feature_groups.values():\n        for feature in group_features:\n            feature_name = feature['feature_name']\n            fill_method = feature.get('fill_method', 'ffill')\n            if feature_name not in data_df.columns:\n                raise KeyError(f\"Feature '{feature_name}' not found in raw dataset.\")\n            series = data_df[feature_name]\n            processed = apply_fill_method(series, fill_method, target_index, weekly_rule)\n            final_df[feature_name] = processed\n\n    final_df = final_df.sort_index().ffill().dropna()\n    final_df = final_df.reset_index().rename(columns={'index': 'date'})\n\n    return final_df\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\nclass DFTSeriesDecomp(nn.Module):\n    def __init__(self, top_k: int = 5):\n        super().__init__()\n        self.top_k = top_k\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        xf = torch.fft.rfft(x)\n        freq = torch.abs(xf)\n        freq[..., 0] = 0\n        top_k_freq, _ = torch.topk(freq, self.top_k)\n        xf = torch.where(freq > top_k_freq.min(), xf, torch.zeros_like(xf))\n        x_season = torch.fft.irfft(xf, n=x.size(1))\n        x_trend = x - x_season\n        return x_season, x_trend\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\nclass TemporalEmbedding(nn.Module):\n    def __init__(self, d_model: int, embed_type: str = 'fixed', freq: str = 'h'):\n        super().__init__()\n        minute_size = 4\n        hour_size = 24\n        weekday_size = 7\n        day_size = 32\n        month_size = 13\n        embed_cls = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n        if freq == 't':\n            self.minute_embed = embed_cls(minute_size, d_model)\n        if freq in ['t', 'h']:\n            self.hour_embed = embed_cls(hour_size, d_model)\n        self.weekday_embed = embed_cls(weekday_size, d_model)\n        self.day_embed = embed_cls(day_size, d_model)\n        self.month_embed = embed_cls(month_size, d_model)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.long()\n        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n        hour_x = self.hour_embed(x[:, :, 3]) if hasattr(self, 'hour_embed') else 0.\n        weekday_x = self.weekday_embed(x[:, :, 2])\n        day_x = self.day_embed(x[:, :, 1])\n        month_x = self.month_embed(x[:, :, 0])\n        return hour_x + weekday_x + day_x + month_x + minute_x\n\n\nclass FixedEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        w = torch.zeros(c_in, d_model).float()\n        position = torch.arange(0, c_in).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n        w[:, 0::2] = torch.sin(position * div_term)\n        w[:, 1::2] = torch.cos(position * div_term)\n        self.emb = nn.Embedding(c_in, d_model)\n        self.emb.weight = nn.Parameter(w, requires_grad=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.emb(x).detach()\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        if embed_type == 'timeF':\n            self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        else:\n            self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.seq_len = configs.seq_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.layer_norm = nn.LayerNorm(configs.d_model)\n        self.dropout = nn.Dropout(configs.dropout)\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method == 'moving_avg':\n            self.decomposition = SeriesDecomp(configs.moving_avg)\n        elif configs.decomp_method == 'dft_decomp':\n            self.decomposition = DFTSeriesDecomp(configs.top_k)\n        else:\n            raise ValueError('Unsupported decomposition method')\n        if configs.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list = []\n        trend_list = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            dir_out_channels = 1 if self.channel_independence == 1 else configs.c_out\n            self.direction_head = nn.Linear(configs.d_model, dir_out_channels, bias=True)\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n        elif self.task_name in ['imputation', 'anomaly_detection']:\n            out_dim = 1 if self.channel_independence == 1 else configs.c_out\n            self.projection_layer = nn.Linear(configs.d_model, out_dim, bias=True)\n        elif self.task_name == 'classification':\n            self.act = F.gelu\n            self.dropout = nn.Dropout(configs.dropout)\n            self.projection = nn.Linear(configs.d_model * configs.seq_len, configs.num_class)\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        self.future_time_embed = None\n        if self.use_future_temporal_feature and x_mark_dec is not None:\n            B, _, N = x_enc.size()\n            future_mark = x_mark_dec[:, -self.pred_len:, :]\n            if self.channel_independence == 1:\n                future_mark = future_mark.repeat(N, 1, 1)\n            self.future_time_embed = self.enc_embedding(None, future_mark)\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list = []\n        x_mark_processed = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out = torch.stack(dec_out_list, dim=-1).sum(-1)\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        dec_out_list = []\n        if self.channel_independence == 1:\n            x_list = x_list[0]\n            for i, enc_out in zip(range(len(x_list)), enc_out_list):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n                if self.use_future_temporal_feature and self.future_time_embed is not None:\n                    fusion = torch.cat([dec_out, self.future_time_embed], dim=-1)\n                    gate = torch.sigmoid(self.future_gate(fusion))\n                    dec_out = dec_out + gate * (self.future_time_embed - dec_out)\n                dir_logits = self.direction_head(dec_out)\n                dec_out = self.projection_layer(dec_out)\n                dec_out = dec_out.reshape(B, self.configs.c_out, self.pred_len).permute(0, 2, 1).contiguous()\n                dir_logits = dir_logits.reshape(B, 1, self.pred_len).permute(0, 2, 1).contiguous()\n                dec_out_list.append(dec_out)\n        else:\n            enc_inputs, out_res_list = x_list\n            for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n                if self.use_future_temporal_feature and self.future_time_embed is not None:\n                    fusion = torch.cat([dec_out, self.future_time_embed], dim=-1)\n                    gate = torch.sigmoid(self.future_gate(fusion))\n                    dec_out = dec_out + gate * (self.future_time_embed - dec_out)\n                dec_out = self.out_projection(dec_out, i, out_res)\n                dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        if self.task_name == 'imputation':\n            raise NotImplementedError('Imputation path is not required for this script')\n        if self.task_name == 'anomaly_detection':\n            raise NotImplementedError('Anomaly detection path is not required for this script')\n        if self.task_name == 'classification':\n            raise NotImplementedError('Classification path is not required for this script')\n        raise ValueError('Unknown task')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # Use current working directory as the base so the script can be run from anywhere\n    project_root: Path = Path.cwd()\n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    # EVOLVE-BLOCK-START\n    seq_len: int = 96  # Capture longer historical context\n    # EVOLVE-BLOCK-END\n    batch_size: int = 32  # Larger batch size for better gradient estimates\n    learning_rate: float = 5e-3  # Smaller learning rate for smoother convergence\n    train_epochs: int = 10\n    patience: int = 10  # Allow more epochs before early stopping\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.05  # Lower dropout for better learning\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 15  # Adjust decomposition window\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\" / \"standalone_iron_daily\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    fusion_df = build_feature_fusion_dataset(cfg)\n    fusion_df[\"date\"] = pd.to_datetime(fusion_df[\"date\"])\n    fusion_df = fusion_df.sort_values(\"date\").reset_index(drop=True)\n    return fusion_df\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 10, 15, 30])  # Add more window sizes\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    ratios = cfg.split_ratio\n    train_ratio = float(ratios.get(\"train\", 0.8))\n    val_ratio = float(ratios.get(\"val\", 0.1))\n    test_ratio = float(ratios.get(\"test\", 0.1))\n    ratio_sum = train_ratio + val_ratio + test_ratio\n    if ratio_sum <= 0:\n        raise ValueError(\"Split ratios must sum to a positive value.\")\n    train_ratio /= ratio_sum\n    val_ratio /= ratio_sum\n    test_ratio = 1.0 - train_ratio - val_ratio\n\n    num_train = int(total_len * train_ratio)\n    num_val = int(total_len * val_ratio)\n    num_test = total_len - num_train - num_val\n    if num_train <= 0 or num_test <= 0:\n        raise ValueError(\"Insufficient data after applying split ratios.\")\n\n    border1s = [0, max(num_train - cfg.seq_len, 0), total_len - num_test - cfg.seq_len]\n    border2s = [num_train, num_train + num_val, total_len]\n    return border1s, border2s\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    df_stamp['quarter'] = df_stamp['date'].dt.quarter  # Add quarter\n    df_stamp['year'] = df_stamp['date'].dt.year - df_stamp['date'].dt.year.min()  # Normalize year\n    return df_stamp[['month', 'day', 'weekday', 'hour', 'quarter', 'year']].values  # Include new features\n\n\ndef prepare_custom_style_data(df: pd.DataFrame, cfg: IronDailyConfig):\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    feature_cols = [c for c in df.columns if c != 'date']\n    from sklearn.preprocessing import StandardScaler  # Import StandardScaler\n    scaler = StandardScaler()  # Initialize scaler\n    data_values = scaler.fit_transform(df[feature_cols].values.astype(np.float32))  # Scale features\n    total_len = len(df)\n    border1s, border2s = compute_split_borders(total_len, cfg)\n    split_info = {}\n    names = ['train', 'val', 'test']\n    for idx, name in enumerate(names):\n        b1, b2 = border1s[idx], border2s[idx]\n        data_slice = data_values[b1:b2]\n        stamp_slice = build_time_mark_array(df['date'].iloc[b1:b2], cfg)\n        split_info[name] = {\n            'data': data_slice,\n            'stamp': stamp_slice.astype(np.float32),\n            'length': len(data_slice),\n            'dates': df['date'].iloc[b1:b2].to_numpy(),\n        }\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n) -> Tuple[float, float, float, float]:\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            if cfg.down_sampling_layers == 0:\n                dec_inp = torch.zeros_like(batch_y[:, -cfg.pred_len:, :]).float()\n                dec_inp = torch.cat([batch_y[:, :cfg.label_len, :], dec_inp], dim=1).to(device)\n            else:\n                dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None)))\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef train_pipeline(cfg: IronDailyConfig) -> None:\n    print(\"1) \u6570\u636e\u5bf9\u9f50\uff1a\u5bf9\u539f\u59cb\u5e8f\u5217\u8fdb\u884c\u5de5\u4f5c\u65e5\u9891\u7387\u91cd\u91c7\u6837\u5e76\u586b\u5145...\")\n    fused_df = fuse_and_align_features(cfg)\n    print(f\"   \u5bf9\u9f50\u540e\u6837\u672c\u6570: {len(fused_df)}\")\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u590d\u7528\u65e5\u9891\u4efb\u52a1\u6240\u9700\u7684\u6240\u6709\u53d8\u6362...\")\n    fe_df = run_feature_engineering(fused_df, cfg)\n    print(f\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u5269\u4f59\u6837\u672c: {len(fe_df)}\")\n\n    print(\"3) \u6570\u636e\u96c6\u5207\u5206\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_custom_style_data(fe_df, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {split: len(loader.dataset) for split, loader in loaders.items()}\n    loader_steps = {split: len(loader) for split, loader in loaders.items()}\n    logger.info(\n        \"Dataset windows -> train:%d, val:%d, test:%d\",\n        dataset_sizes.get(\"train\", 0),\n        dataset_sizes.get(\"val\", 0),\n        dataset_sizes.get(\"test\", 0),\n    )\n    print(\n        f\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1atrain={dataset_sizes.get('train', 0)}, \"\n        f\"val={dataset_sizes.get('val', 0)}, test={dataset_sizes.get('test', 0)}\"\n    )\n    logger.info(\n        \"Loader steps/epoch -> train:%d, val:%d, test:%d\",\n        loader_steps.get(\"train\", 0),\n        loader_steps.get(\"val\", 0),\n        loader_steps.get(\"test\", 0),\n    )\n    print(\n        f\"   Dataloader\u6b65\u6570\uff1atrain={loader_steps.get('train', 0)}, \"\n        f\"val={loader_steps.get('val', 0)}, test={loader_steps.get('test', 0)}\"\n    )\n    test_dataset = loaders[\"test\"].dataset\n    print(\"   Test\u7a97\u53e3\u65f6\u95f4\u8de8\u5ea6\uff1a\")\n    for idx in range(len(test_dataset)):\n        start_date, end_date = test_dataset.window_bounds(idx)\n        print(f\"     \u6ce2\u6bb5{idx + 1:02d}: {start_date.strftime('%Y-%m-%d')} -> {end_date.strftime('%Y-%m-%d')}\")\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=1e-4)  # Use AdamW with weight decay\n    criterion = nn.MSELoss()\n    logger.info(\n        \"Training params | epochs=%d, batch=%d, lr=%.4f, patience=%d, seq_len=%d, pred_len=%d, d_model=%d, d_ff=%d\",\n        cfg.train_epochs,\n        cfg.batch_size,\n        cfg.learning_rate,\n        cfg.patience,\n        cfg.seq_len,\n        cfg.pred_len,\n        cfg.d_model,\n        cfg.d_ff,\n    )\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, batch={cfg.batch_size}, lr={cfg.learning_rate}, \"\n        f\"patience={cfg.patience}, seq_len={cfg.seq_len}, pred_len={cfg.pred_len}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}\"\n    )\n    logger.info(\n        \"Model depth | e_layers=%d, d_layers=%d, down_sampling_layers=%d, down_window=%d\",\n        cfg.e_layers,\n        cfg.d_layers,\n        cfg.down_sampling_layers,\n        cfg.down_sampling_window,\n    )\n    print(\n        f\"   \u6a21\u578b\u7ed3\u6784\uff1ae_layers={cfg.e_layers}, d_layers={cfg.d_layers}, \"\n        f\"down_layers={cfg.down_sampling_layers}, down_window={cfg.down_sampling_window}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)  # Add LR scheduler\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            if cfg.down_sampling_layers == 0:\n                dec_inp = torch.zeros_like(batch_y[:, -cfg.pred_len:, :]).float()\n                dec_inp = torch.cat([batch_y[:, :cfg.label_len, :], dec_inp], dim=1).to(cfg.device_obj)\n            else:\n                dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            loss = criterion(pred_y, true_y)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        scheduler.step(avg_loss)  # Step the scheduler after each epoch\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        logger.info(\"Epoch %d | train_loss=%.4f, val_mse=%.4f\", epoch + 1, avg_loss, val_mse)  # Log metrics\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n    np.save(cfg.checkpoint_dir / \"test_predictions.npy\", preds_arr)  # Save test predictions for analysis\n    np.save(cfg.checkpoint_dir / \"test_truth.npy\", trues_arr)  # Save test ground truth\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Standalone iron_future_01_daily pipeline\")\n    parser.add_argument(\n        \"--raw_data\",\n        type=str,\n        default=None,\n        help=\"Path to the merged raw dataset (overrides config data_file)\",\n    )\n    args = parser.parse_args()\n\n    configuration = IronDailyConfig(raw_data_override=args.raw_data)\n    train_pipeline(configuration)\n    \n    # \u8fd0\u884c\u547d\u4ee4\uff1auv run src/test_task/iron_future_01_daily_pipeline.py --raw_data data/iron/merged_data.csv\n\n```\nUnique approach: Alternative combined_score approach, Object-oriented approach, NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\nrandom.seed(FIX_SEED)\nnp.random.seed(FIX_SEED)\ntorch.manual_seed(FIX_SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(FIX_SEED)\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\nclass TimeFeature:\n    def __init__(self):\n        pass\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        pass\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"()\"\n\n\nclass MillisecondOfMinute(TimeFeature):\n    \"\"\"Millisecond of minute encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        milliseconds = index.second * 1000 + index.microsecond // 1000\n        return milliseconds / 59999.0 - 0.5\n\n\nclass SecondOfMinute(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.second / 59.0 - 0.5\n\n\nclass MinuteOfHour(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5\n\n\nclass HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.hour / 23.0 - 0.5\n\n\nclass DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.dayofweek / 6.0 - 0.5\n\n\nclass DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.day - 1) / 30.0 - 0.5\n\n\nclass DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5\n\n\nclass MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.month - 1) / 11.0 - 0.5\n\n\nclass WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.isocalendar().week - 1) / 52.0 - 0.5\n\n\ndef time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"\n    Returns a list of time features that will be appropriate for the given frequency string.\n    Parameters\n    ----------\n    freq_str\n        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n    \"\"\"\n\n    features_by_offsets = {\n        offsets.YearEnd: [],\n        offsets.QuarterEnd: [MonthOfYear],\n        offsets.MonthEnd: [MonthOfYear],\n        offsets.Week: [DayOfMonth, WeekOfYear],\n        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n        offsets.Minute: [\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Second: [\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n        offsets.Milli: [\n            MillisecondOfMinute,\n            SecondOfMinute,\n            MinuteOfHour,\n            HourOfDay,\n            DayOfWeek,\n            DayOfMonth,\n            DayOfYear,\n        ],\n    }\n\n    offset = to_offset(freq_str)\n\n    for offset_type, feature_classes in features_by_offsets.items():\n        if isinstance(offset, offset_type):\n            return [cls() for cls in feature_classes]\n\n    supported_freq_msg = f\"\"\"\n    Unsupported frequency {freq_str}\n    The following frequencies are supported:\n        Y   - yearly\n            alias: A\n        M   - monthly\n        W   - weekly\n        D   - daily\n        B   - business days\n        H   - hourly\n        T   - minutely\n            alias: min\n        S   - secondly\n        ms  - milliseconds\n    \"\"\"\n    raise RuntimeError(supported_freq_msg)\n\n\ndef time_features(dates, freq='h'):\n    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# -----------------------------------------------------------------------------\n# Feature fusion helpers (derived from src/data_process/feature_fusion.py)\n# -----------------------------------------------------------------------------\n\nDEFAULT_FUSION_CONFIG = {\n    \"data_file\": \"data/iron/merged_data.csv\",\n    \"target_name\": \"FU00002776\",\n    \"output_file\": \"data/iron/datasets/final_features_01\u5408\u7ea6\u6536\u76d8\u4ef7_v2.csv\",\n    \"features\": {\n        \"supply\": [\n            {\n                \"feature_name\": \"ID01002312\",\n                \"file_path\": \"data/mysteel3/ID01002312_\u94c1\u77ff\uff1a\u8fdb\u53e3\uff1a\u5e93\u5b58\uff1a45\u4e2a\u6e2f\u53e3\uff08\u65e5\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"ffill\",\n            },\n            {\n                \"feature_name\": \"ID00186575\",\n                \"file_path\": \"data/mysteel3/ID00186575_\u94c1\u77ff\uff1a\u8239\u8236\u5230\u6e2f\u91cf\uff1a\u5317\u65b9\u6e2f\u53e3\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n        ],\n        \"demand\": [\n            {\n                \"feature_name\": \"ID00186100\",\n                \"file_path\": \"data/mysteel3/ID00186100_\u94c1\u77ff\uff1a\u8fdb\u53e3\uff1a\u65e5\u5747\u758f\u6e2f\u91cf\u5408\u8ba1\uff1a45\u4e2a\u6e2f\u53e3\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n            {\n                \"feature_name\": \"ID00183109\",\n                \"file_path\": \"data/mysteel3/ID00183109_247\u5bb6\u94a2\u94c1\u4f01\u4e1a\uff1a\u9ad8\u7089\u5f00\u5de5\u7387\uff1a\u4e2d\u56fd\uff08\u5468\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"weekly_lag1\",\n            },\n        ],\n        \"macro\": [\n            {\n                \"feature_name\": \"GM0000033031\",\n                \"file_path\": \"data/mysteel3/GM0000033031_\u7f8e\u56fd\uff1a\u975e\u519c\u5c31\u4e1a\u4eba\u5458\uff1a\u5b63\u8c03\u4eba\u6570\u53d8\u52a8\uff08\u6708\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"monthly_lag1_daily\",\n            },\n            {\n                \"feature_name\": \"CM0000013263\",\n                \"file_path\": \"data/mysteel3/CM0000013263_\u7edf\u8ba1\u5c40\uff1a\u5236\u9020\u4e1aPMI\uff1a\u8d2d\u8fdb\u4ef7\uff08\u6708\uff09.csv\",\n                \"source_column\": \"value\",\n                \"fill_method\": \"monthly_lag1_daily\",\n            },\n        ],\n    },\n}\n\n\ndef resolve_path(base: Path, candidate: str | Path) -> Path:\n    candidate_path = Path(candidate)\n    if not candidate_path.is_absolute():\n        candidate_path = base / candidate_path\n    return candidate_path\n\n\ndef ensure_datetime_series(series: pd.Series) -> pd.Series:\n    if not pd.api.types.is_datetime64_any_dtype(series.index):\n        series.index = pd.to_datetime(series.index)\n    series = series.sort_index()\n    return series[~series.index.duplicated(keep='last')]\n\n\ndef infer_weekly_rule(index: pd.DatetimeIndex, fallback: str | None = None) -> str:\n    default_rule = fallback or 'W-FRI'\n    if index is None or len(index) == 0:\n        return default_rule\n    index = pd.to_datetime(index).sort_values()\n    freq = pd.infer_freq(index)\n    day_map = {0: 'MON', 1: 'TUE', 2: 'WED', 3: 'THU', 4: 'FRI', 5: 'SAT', 6: 'SUN'}\n    if freq and freq.startswith('W-'):\n        return freq\n    if freq == '7D':\n        anchor_day = int(index[-1].dayofweek)\n        return f\"W-{day_map.get(anchor_day, 'FRI')}\"\n    try:\n        anchor_day = int(pd.Series(index.dayofweek).mode().iloc[0])\n        return f\"W-{day_map.get(anchor_day, 'FRI')}\"\n    except Exception:\n        return default_rule\n\n\ndef resample_with_agg(series: pd.Series, rule: str, agg: str) -> pd.Series:\n    resampler = series.resample(rule, label='right', closed='right')\n    agg = (agg or 'last').lower()\n    if agg == 'mean':\n        return resampler.mean()\n    if agg == 'last':\n        return resampler.last()\n    if agg == 'sum':\n        return resampler.sum()\n    if agg == 'median':\n        return resampler.median()\n    raise ValueError(f\"Unsupported aggregation '{agg}' for rule '{rule}'\")\n\n\ndef apply_fill_method(\n    series: pd.Series,\n    method: str,\n    target_index: pd.DatetimeIndex,\n    weekly_rule: str,\n) -> pd.Series:\n    method = (method or 'ffill').lower()\n    aligned_series = ensure_datetime_series(series)\n    if method == 'weekly_lag1':\n        weekly_series = resample_with_agg(aligned_series, weekly_rule, 'last').shift(1)\n        filled = weekly_series.reindex(target_index)\n        return filled.ffill()\n    if method == 'monthly_lag1_daily':\n        monthly_series = resample_with_agg(aligned_series, 'M', 'last').shift(1)\n        filled = monthly_series.reindex(target_index)\n        return filled.ffill()\n    if method == 'ffill':\n        aligned = aligned_series.reindex(target_index)\n        filled = aligned.ffill()\n        if aligned.isna().sum() > aligned_series.isna().sum():\n            filled = filled.shift(1)\n        return filled\n    aligned = aligned_series.reindex(target_index)\n    return aligned.ffill()\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    fusion_cfg = copy.deepcopy(cfg.fusion_config or DEFAULT_FUSION_CONFIG)\n\n    data_path_str = cfg.raw_data_override or fusion_cfg.get('data_file')\n    if data_path_str is None:\n        raise ValueError(\"Fusion config must provide 'data_file'.\")\n    data_path = resolve_path(cfg.project_root, data_path_str)\n\n    data_df = pd.read_csv(data_path, parse_dates=['date'])\n    data_df = data_df.sort_values('date').drop_duplicates('date', keep='last')\n    data_df = data_df.set_index('date')\n\n    target_name = fusion_cfg['target_name']\n    target_freq = str(fusion_cfg.get('target_frequency', 'D')).upper()\n    target_agg = fusion_cfg.get('target_agg', 'last')\n\n    target_series = ensure_datetime_series(data_df[target_name])\n    weekly_rule = fusion_cfg.get('target_weekly_rule')\n\n    if target_freq.startswith('W'):\n        weekly_rule = weekly_rule or infer_weekly_rule(target_series.index)\n        target_series = resample_with_agg(target_series, weekly_rule, target_agg)\n    elif target_freq.startswith('M'):\n        target_series = resample_with_agg(target_series, 'M', target_agg)\n    target_df = target_series.dropna().to_frame(name='value')\n\n    target_index = target_df.index\n    weekly_rule = weekly_rule or infer_weekly_rule(target_index)\n\n    final_df = target_df.copy()\n    feature_groups = fusion_cfg.get('features', {})\n    for group_features in feature_groups.values():\n        for feature in group_features:\n            feature_name = feature['feature_name']\n            fill_method = feature.get('fill_method', 'ffill')\n            if feature_name not in data_df.columns:\n                raise KeyError(f\"Feature '{feature_name}' not found in raw dataset.\")\n            series = data_df[feature_name]\n            processed = apply_fill_method(series, fill_method, target_index, weekly_rule)\n            final_df[feature_name] = processed\n\n    final_df = final_df.sort_index().ffill().dropna()\n    final_df = final_df.reset_index().rename(columns={'index': 'date'})\n\n    return final_df\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\nclass DFTSeriesDecomp(nn.Module):\n    def __init__(self, top_k: int = 5):\n        super().__init__()\n        self.top_k = top_k\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        xf = torch.fft.rfft(x)\n        freq = torch.abs(xf)\n        freq[..., 0] = 0\n        top_k_freq, _ = torch.topk(freq, self.top_k)\n        xf = torch.where(freq > top_k_freq.min(), xf, torch.zeros_like(xf))\n        x_season = torch.fft.irfft(xf, n=x.size(1))\n        x_trend = x - x_season\n        return x_season, x_trend\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\nclass TemporalEmbedding(nn.Module):\n    def __init__(self, d_model: int, embed_type: str = 'fixed', freq: str = 'h'):\n        super().__init__()\n        minute_size = 4\n        hour_size = 24\n        weekday_size = 7\n        day_size = 32\n        month_size = 13\n        embed_cls = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n        if freq == 't':\n            self.minute_embed = embed_cls(minute_size, d_model)\n        if freq in ['t', 'h']:\n            self.hour_embed = embed_cls(hour_size, d_model)\n        self.weekday_embed = embed_cls(weekday_size, d_model)\n        self.day_embed = embed_cls(day_size, d_model)\n        self.month_embed = embed_cls(month_size, d_model)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.long()\n        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n        hour_x = self.hour_embed(x[:, :, 3]) if hasattr(self, 'hour_embed') else 0.\n        weekday_x = self.weekday_embed(x[:, :, 2])\n        day_x = self.day_embed(x[:, :, 1])\n        month_x = self.month_embed(x[:, :, 0])\n        return hour_x + weekday_x + day_x + month_x + minute_x\n\n\nclass FixedEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        w = torch.zeros(c_in, d_model).float()\n        position = torch.arange(0, c_in).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n        w[:, 0::2] = torch.sin(position * div_term)\n        w[:, 1::2] = torch.cos(position * div_term)\n        self.emb = nn.Embedding(c_in, d_model)\n        self.emb.weight = nn.Parameter(w, requires_grad=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.emb(x).detach()\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        if embed_type == 'timeF':\n            self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        else:\n            self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.seq_len = configs.seq_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.layer_norm = nn.LayerNorm(configs.d_model)\n        self.dropout = nn.Dropout(configs.dropout)\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method == 'moving_avg':\n            self.decomposition = SeriesDecomp(configs.moving_avg)\n        elif configs.decomp_method == 'dft_decomp':\n            self.decomposition = DFTSeriesDecomp(configs.top_k)\n        else:\n            raise ValueError('Unsupported decomposition method')\n        if configs.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list = []\n        trend_list = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            dir_out_channels = 1 if self.channel_independence == 1 else configs.c_out\n            self.direction_head = nn.Linear(configs.d_model, dir_out_channels, bias=True)\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n        elif self.task_name in ['imputation', 'anomaly_detection']:\n            out_dim = 1 if self.channel_independence == 1 else configs.c_out\n            self.projection_layer = nn.Linear(configs.d_model, out_dim, bias=True)\n        elif self.task_name == 'classification':\n            self.act = F.gelu\n            self.dropout = nn.Dropout(configs.dropout)\n            self.projection = nn.Linear(configs.d_model * configs.seq_len, configs.num_class)\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        self.future_time_embed = None\n        if self.use_future_temporal_feature and x_mark_dec is not None:\n            B, _, N = x_enc.size()\n            future_mark = x_mark_dec[:, -self.pred_len:, :]\n            if self.channel_independence == 1:\n                future_mark = future_mark.repeat(N, 1, 1)\n            self.future_time_embed = self.enc_embedding(None, future_mark)\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list = []\n        x_mark_processed = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out = torch.stack(dec_out_list, dim=-1).sum(-1)\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        dec_out_list = []\n        if self.channel_independence == 1:\n            x_list = x_list[0]\n            for i, enc_out in zip(range(len(x_list)), enc_out_list):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n                if self.use_future_temporal_feature and self.future_time_embed is not None:\n                    fusion = torch.cat([dec_out, self.future_time_embed], dim=-1)\n                    gate = torch.sigmoid(self.future_gate(fusion))\n                    dec_out = dec_out + gate * (self.future_time_embed - dec_out)\n                dir_logits = self.direction_head(dec_out)\n                dec_out = self.projection_layer(dec_out)\n                dec_out = dec_out.reshape(B, self.configs.c_out, self.pred_len).permute(0, 2, 1).contiguous()\n                dir_logits = dir_logits.reshape(B, 1, self.pred_len).permute(0, 2, 1).contiguous()\n                dec_out_list.append(dec_out)\n        else:\n            enc_inputs, out_res_list = x_list\n            for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n                if self.use_future_temporal_feature and self.future_time_embed is not None:\n                    fusion = torch.cat([dec_out, self.future_time_embed], dim=-1)\n                    gate = torch.sigmoid(self.future_gate(fusion))\n                    dec_out = dec_out + gate * (self.future_time_embed - dec_out)\n                dec_out = self.out_projection(dec_out, i, out_res)\n                dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        if self.task_name == 'imputation':\n            raise NotImplementedError('Imputation path is not required for this script')\n        if self.task_name == 'anomaly_detection':\n            raise NotImplementedError('Anomaly detection path is not required for this script')\n        if self.task_name == 'classification':\n            raise NotImplementedError('Classification path is not required for this script')\n        raise ValueError('Unknown task')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # Use current working directory as the base so the script can be run from anywhere\n    project_root: Path = Path.cwd()\n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    # EVOLVE-BLOCK-START\n    seq_len: int = 48\n    # EVOLVE-BLOCK-END\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 5\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\" / \"standalone_iron_daily\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    fusion_df = build_feature_fusion_dataset(cfg)\n    fusion_df[\"date\"] = pd.to_datetime(fusion_df[\"date\"])\n    fusion_df = fusion_df.sort_values(\"date\").reset_index(drop=True)\n    return fusion_df\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    ratios = cfg.split_ratio\n    train_ratio = float(ratios.get(\"train\", 0.8))\n    val_ratio = float(ratios.get(\"val\", 0.1))\n    test_ratio = float(ratios.get(\"test\", 0.1))\n    ratio_sum = train_ratio + val_ratio + test_ratio\n    if ratio_sum <= 0:\n        raise ValueError(\"Split ratios must sum to a positive value.\")\n    train_ratio /= ratio_sum\n    val_ratio /= ratio_sum\n    test_ratio = 1.0 - train_ratio - val_ratio\n\n    num_train = int(total_len * train_ratio)\n    num_val = int(total_len * val_ratio)\n    num_test = total_len - num_train - num_val\n    if num_train <= 0 or num_test <= 0:\n        raise ValueError(\"Insufficient data after applying split ratios.\")\n\n    border1s = [0, max(num_train - cfg.seq_len, 0), total_len - num_test - cfg.seq_len]\n    border2s = [num_train, num_train + num_val, total_len]\n    return border1s, border2s\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_custom_style_data(df: pd.DataFrame, cfg: IronDailyConfig):\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    feature_cols = [c for c in df.columns if c != 'date']\n    data_values = df[feature_cols].values.astype(np.float32)\n    total_len = len(df)\n    border1s, border2s = compute_split_borders(total_len, cfg)\n    split_info = {}\n    names = ['train', 'val', 'test']\n    for idx, name in enumerate(names):\n        b1, b2 = border1s[idx], border2s[idx]\n        data_slice = data_values[b1:b2]\n        stamp_slice = build_time_mark_array(df['date'].iloc[b1:b2], cfg)\n        split_info[name] = {\n            'data': data_slice,\n            'stamp': stamp_slice.astype(np.float32),\n            'length': len(data_slice),\n            'dates': df['date'].iloc[b1:b2].to_numpy(),\n        }\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n) -> Tuple[float, float, float, float]:\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            if cfg.down_sampling_layers == 0:\n                dec_inp = torch.zeros_like(batch_y[:, -cfg.pred_len:, :]).float()\n                dec_inp = torch.cat([batch_y[:, :cfg.label_len, :], dec_inp], dim=1).to(device)\n            else:\n                dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None)))\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef train_pipeline(cfg: IronDailyConfig) -> None:\n    print(\"1) \u6570\u636e\u5bf9\u9f50\uff1a\u5bf9\u539f\u59cb\u5e8f\u5217\u8fdb\u884c\u5de5\u4f5c\u65e5\u9891\u7387\u91cd\u91c7\u6837\u5e76\u586b\u5145...\")\n    fused_df = fuse_and_align_features(cfg)\n    print(f\"   \u5bf9\u9f50\u540e\u6837\u672c\u6570: {len(fused_df)}\")\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u590d\u7528\u65e5\u9891\u4efb\u52a1\u6240\u9700\u7684\u6240\u6709\u53d8\u6362...\")\n    fe_df = run_feature_engineering(fused_df, cfg)\n    print(f\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u5269\u4f59\u6837\u672c: {len(fe_df)}\")\n\n    print(\"3) \u6570\u636e\u96c6\u5207\u5206\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_custom_style_data(fe_df, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {split: len(loader.dataset) for split, loader in loaders.items()}\n    loader_steps = {split: len(loader) for split, loader in loaders.items()}\n    logger.info(\n        \"Dataset windows -> train:%d, val:%d, test:%d\",\n        dataset_sizes.get(\"train\", 0),\n        dataset_sizes.get(\"val\", 0),\n        dataset_sizes.get(\"test\", 0),\n    )\n    print(\n        f\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1atrain={dataset_sizes.get('train', 0)}, \"\n        f\"val={dataset_sizes.get('val', 0)}, test={dataset_sizes.get('test', 0)}\"\n    )\n    logger.info(\n        \"Loader steps/epoch -> train:%d, val:%d, test:%d\",\n        loader_steps.get(\"train\", 0),\n        loader_steps.get(\"val\", 0),\n        loader_steps.get(\"test\", 0),\n    )\n    print(\n        f\"   Dataloader\u6b65\u6570\uff1atrain={loader_steps.get('train', 0)}, \"\n        f\"val={loader_steps.get('val', 0)}, test={loader_steps.get('test', 0)}\"\n    )\n    test_dataset = loaders[\"test\"].dataset\n    print(\"   Test\u7a97\u53e3\u65f6\u95f4\u8de8\u5ea6\uff1a\")\n    for idx in range(len(test_dataset)):\n        start_date, end_date = test_dataset.window_bounds(idx)\n        print(f\"     \u6ce2\u6bb5{idx + 1:02d}: {start_date.strftime('%Y-%m-%d')} -> {end_date.strftime('%Y-%m-%d')}\")\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n    criterion = nn.MSELoss()\n    logger.info(\n        \"Training params | epochs=%d, batch=%d, lr=%.4f, patience=%d, seq_len=%d, pred_len=%d, d_model=%d, d_ff=%d\",\n        cfg.train_epochs,\n        cfg.batch_size,\n        cfg.learning_rate,\n        cfg.patience,\n        cfg.seq_len,\n        cfg.pred_len,\n        cfg.d_model,\n        cfg.d_ff,\n    )\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, batch={cfg.batch_size}, lr={cfg.learning_rate}, \"\n        f\"patience={cfg.patience}, seq_len={cfg.seq_len}, pred_len={cfg.pred_len}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}\"\n    )\n    logger.info(\n        \"Model depth | e_layers=%d, d_layers=%d, down_sampling_layers=%d, down_window=%d\",\n        cfg.e_layers,\n        cfg.d_layers,\n        cfg.down_sampling_layers,\n        cfg.down_sampling_window,\n    )\n    print(\n        f\"   \u6a21\u578b\u7ed3\u6784\uff1ae_layers={cfg.e_layers}, d_layers={cfg.d_layers}, \"\n        f\"down_layers={cfg.down_sampling_layers}, down_window={cfg.down_sampling_window}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            if cfg.down_sampling_layers == 0:\n                dec_inp = torch.zeros_like(batch_y[:, -cfg.pred_len:, :]).float()\n                dec_inp = torch.cat([batch_y[:, :cfg.label_len, :], dec_inp], dim=1).to(cfg.device_obj)\n            else:\n                dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            loss = criterion(pred_y, true_y)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Standalone iron_future_01_daily pipeline\")\n    parser.add_argument(\n        \"--raw_data\",\n        type=str,\n        default=None,\n        help=\"Path to the merged raw dataset (overrides config data_file)\",\n    )\n    args = parser.parse_args()\n\n    configuration = IronDailyConfig(raw_data_override=args.raw_data)\n    train_pipeline(configuration)\n    \n    # \u8fd0\u884c\u547d\u4ee4\uff1auv run src/test_task/iron_future_01_daily_pipeline.py --raw_data data/iron/merged_data.csv\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Here are targeted suggestions to improve the program's FITNESS SCORE. The changes aim to optimize data preparation, feature engineering, and training logic while maintaining compatibility with the evaluator API.\n\n---\n\n### Change 1: **Increase Historical Context for Better Learning**\nRationale: Increasing the sequence length (`seq_len`) allows the model to capture longer-term dependencies in the time series data, which is critical for forecasting tasks.\n\n```diff\n<<<<<<< SEARCH\nseq_len: int = 48\n=======\nseq_len: int = 96  # Capture longer historical context\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 2: **Use a Smaller Learning Rate for Smoother Convergence**\nRationale: Reducing the learning rate ensures smoother gradient updates, which can lead to better convergence, particularly for deep models.\n\n```diff\n<<<<<<< SEARCH\nlearning_rate: float = 1e-2\n=======\nlearning_rate: float = 5e-3  # Smaller learning rate for smoother convergence\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 3: **Increase Batch Size for Better Gradient Estimates**\nRationale: A larger batch size provides more stable gradient estimates, which can help improve model generalization and convergence.\n\n```diff\n<<<<<<< SEARCH\nbatch_size: int = 16\n=======\nbatch_size: int = 32  # Larger batch size for better gradient estimates\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 4: **Add More Rolling Window Sizes to Feature Engineering**\nRationale: Adding diverse rolling window features can capture trends and seasonality at different time scales, which are important for time-series forecasting.\n\n```diff\n<<<<<<< SEARCH\ndf = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n=======\ndf = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 10, 15, 30])  # Add more window sizes\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 5: **Lower Dropout Rate for Better Learning**\nRationale: A lower dropout rate helps preserve more information during training, which can improve performance when the dataset is not excessively large.\n\n```diff\n<<<<<<< SEARCH\ndropout: float = 0.1\n=======\ndropout: float = 0.05  # Lower dropout for better learning\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 6: **Switch to AdamW Optimizer with Weight Decay**\nRationale: AdamW is an improved version of Adam that decouples weight decay from the learning rate, helping to avoid overfitting.\n\n```diff\n<<<<<<< SEARCH\noptimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n=======\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=1e-4)  # Use AdamW with weight decay\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 7: **Add Learning Rate Scheduler**\nRationale: A learning rate scheduler reduces the learning rate dynamically when the validation loss plateaus, helping the model converge to a better local minimum.\n\n```diff\n<<<<<<< SEARCH\nfor epoch in range(cfg.train_epochs):\n=======\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)  # Add LR scheduler\nfor epoch in range(cfg.train_epochs):\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 8: **Add Quarter and Year to Time Features**\nRationale: Including quarter and normalized year as time features may capture additional temporal patterns in the data.\n\n```diff\n<<<<<<< SEARCH\nreturn df_stamp[['month', 'day', 'weekday', 'hour']].values\n=======\n# Add quarter and normalized year\ndf_stamp['quarter'] = df_stamp['date'].dt.quarter\ndf_stamp['year'] = df_stamp['date'].dt.year - df_stamp['date'].dt.year.min()  # Normalize year\nreturn df_stamp[['month', 'day', 'weekday', 'hour', 'quarter', 'year']].values  # Include new features\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 9: **Increase Early Stopping Patience**\nRationale: Allowing more epochs before early stopping can give the model more time to converge, especially with the reduced learning rate.\n\n```diff\n<<<<<<< SEARCH\npatience: int = 5\n=======\npatience: int = 10  # Allow more epochs before early stopping\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 10: **Normalize Features Using MinMaxScaler**\nRationale: Normalizing input features to a [0, 1] range ensures consistent scaling, which can improve training stability and performance. Avoid `sklearn` for compatibility.\n\n```diff\n<<<<<<< SEARCH\ndata_values = df[feature_cols].values.astype(np.float32)\n=======\n# Normalize features using MinMax scaling\ndata_values = df[feature_cols].values.astype(np.float32)\nmin_vals = np.min(data_values, axis=0, keepdims=True)\nmax_vals = np.max(data_values, axis=0, keepdims=True)\ndata_values = (data_values - min_vals) / (max_vals - min_vals + 1e-6)\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 11: **Save Predictions and Ground Truth for Analysis**\nRationale: Saving the test predictions and ground truth allows for further analysis and debugging outside the training pipeline.\n\n```diff\n<<<<<<< SEARCH\nprint(\n    f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n    f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n)\n=======\nprint(\n    f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n    f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n)\nnp.save(cfg.checkpoint_dir / \"test_predictions.npy\", preds_arr)  # Save test predictions for analysis\nnp.save(cfg.checkpoint_dir / \"test_truth.npy\", trues_arr)  # Save test ground truth\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 12: **Improve Logging for Better Debugging**\nRationale: Adding more detailed logging helps monitor the training process and identify potential issues.\n\n```diff\n<<<<<<< SEARCH\nprint(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n=======\nprint(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\nlogger.info(\"Epoch %d | train_loss=%.4f, val_mse=%.4f\", epoch + 1, avg_loss, val_mse)  # Log metrics\n>>>>>>> REPLACE\n```\n\n---\n\n### Change 13: **Reduce Moving Average Window for Decomposition**\nRationale: A smaller moving average window makes the trend decomposition more sensitive to short-term changes, which may improve forecasting accuracy.\n\n```diff\n<<<<<<< SEARCH\nmoving_avg: int = 25\n=======\nmoving_avg: int = 15  # Adjust decomposition window\n>>>>>>> REPLACE\n```\n\n---\n\nThese targeted changes improve the model's ability to learn temporal patterns, stabilize training, and enhance interpretability. They collectively aim to lower validation/test MSE & MAPE while improving directional accuracy."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}
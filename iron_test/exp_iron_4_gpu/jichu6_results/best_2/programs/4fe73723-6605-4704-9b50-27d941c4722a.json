{"id": "4fe73723-6605-4704-9b50-27d941c4722a", "code": "\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nimport os\n\n# Ensure deterministic CuBLAS workspace for CUDA deterministic algorithms\nif torch.cuda.is_available():\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\n\ndef _set_global_seed(seed: int = 2021) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        try:\n            torch.use_deterministic_algorithms(True)\n        except Exception:\n            # Older torch versions may not have this or certain ops may not support it\n            pass\n\n_set_global_seed(FIX_SEED)\n\n# Generator for DataLoader to keep shuffling deterministic\n_shared_generator = torch.Generator()\n_shared_generator.manual_seed(FIX_SEED)\n\ndef _worker_init_fn(worker_id: int) -> None:\n    # Ensure each worker has a deterministic seed derived from global seed\n    worker_seed = FIX_SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n# EVOLVE-BLOCK-START\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\ndef time_features(dates, freq: str = \"b\") -> np.ndarray:\n    \"\"\"Business-day calendar features (dow/dom/doy scaled to [-0.5, 0.5]).\"\"\"\n    dates = pd.to_datetime(dates)\n    dow = dates.dayofweek / 6.0 - 0.5\n    dom = (dates.day - 1) / 30.0 - 0.5\n    doy = (dates.dayofyear - 1) / 365.0 - 0.5\n    return np.vstack([dow, dom, doy])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# Feature fusion is disabled in this standalone script; keep a tiny placeholder.\nDEFAULT_FUSION_CONFIG: Dict[str, Any] = {}\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:  # pragma: no cover\n    raise NotImplementedError(\n        \"Feature fusion is disabled; provide cached train_raw/val_raw/test_raw CSVs instead.\"\n    )\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\n# DFTSeriesDecomp is unnecessary here because decomp_method is fixed to 'moving_avg'.\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\n# TemporalEmbedding / FixedEmbedding stubs are not needed since embed='timeF'\n# always routes through TimeFeatureEmbedding in DataEmbeddingWoPos.\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        # For this task we always use calendar time features (embed='timeF')\n        self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method != 'moving_avg':\n            raise ValueError('Unsupported decomposition method')\n        self.decomposition = SeriesDecomp(configs.moving_avg)\n        if self.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list: List[torch.Tensor] = []\n        trend_list: List[torch.Tensor] = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list: List[torch.Tensor] = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n            # Learnable weights for aggregating multi-scale predictions instead of a simple sum\n            self.scale_weights = nn.Parameter(torch.ones(configs.down_sampling_layers + 1))\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        # In this task we always set use_future_temporal_feature=0, so we skip\n        # the unused future-time gating logic and directly build multi-scale\n        # encoder inputs. This keeps the forward pass compact but is behaviour-\n        # equivalent for the current configuration.\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list: List[torch.Tensor] = []\n        x_mark_processed: List[torch.Tensor] = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list: List[torch.Tensor] = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        # Multi-scale regression + projection\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out_stack = torch.stack(dec_out_list, dim=-1)\n        # Aggregate predictions from different scales using learnable softmax weights\n        if hasattr(self, \"scale_weights\"):\n            weights = torch.softmax(self.scale_weights, dim=0)\n            dec_out = (dec_out_stack * weights.view(1, 1, 1, -1)).sum(-1)\n        else:\n            dec_out = dec_out_stack.sum(-1)\n        # Denormalise back to the original scale of encoder inputs\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        # With channel_independence fixed to 0 in this pipeline, we only need\n        # the shared multi-scale regression path, which removes unused branches\n        # and slightly reduces overhead without changing behaviour.\n        enc_inputs, out_res_list = x_list\n        dec_out_list: List[torch.Tensor] = []\n        for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n            dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n            dec_out = self.out_projection(dec_out, i, out_res)\n            dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        raise ValueError('Unsupported task name for TimeMixer')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # project_root: Path = Path(__file__).resolve().parents[0]\n    # project_root: Path = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test\\exp_iron_4_gpu\") \n    project_root: Path = Path(r\"/home/jovyan/research/kaikai/c3i/AutoMLAgent/openevolve/iron_test/exp_iron_4_gpu\") \n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    cached_split_dir: Path | None = None\n    use_cached_splits: bool = True\n    seq_len: int = 48\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 1000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # device: str = \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n    blend_alpha: float = 0.8\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.cached_split_dir is None:\n            self.cached_split_dir = self.project_root / \"data\"\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_split_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    \"\"\"Unused in this standalone pipeline; cached CSV splits are loaded instead.\"\"\"\n    raise NotImplementedError(\n        \"fuse_and_align_features is unused; cached train/val/test splits are loaded instead.\"\n    )\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    # unused helper; cached CSV splits are required\n    raise NotImplementedError(\"compute_split_borders is disabled; cached train/val/test splits are required.\")\n\n\ndef get_split_cache_paths(cfg: IronDailyConfig) -> Dict[str, Path]:\n    names = ['train', 'val', 'test']\n    return {name: cfg.cached_split_dir / f\"{name}_raw.csv\" for name in names}\n\n\ndef split_raw_dataframe(fused_df: pd.DataFrame, cfg: IronDailyConfig) -> Dict[str, pd.DataFrame]:\n    # unused helper; cached train/val/test splits must be provided instead\n    raise NotImplementedError(\"split_raw_dataframe is unused in this pipeline; cached splits must be provided.\")\n\n\ndef load_splits_data(\n    cfg: IronDailyConfig,\n) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path], bool]:\n    split_paths = get_split_cache_paths(cfg)\n    if cfg.use_cached_splits and all(path.exists() for path in split_paths.values()):\n        logger.info(\"Loading cached splits from %s\", cfg.cached_split_dir)\n        splits = {\n            name: pd.read_csv(path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n            for name, path in split_paths.items()\n        }\n        return splits, split_paths\n\n\ndef run_feature_engineering_on_splits(\n    raw_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Dict[str, pd.DataFrame]:\n    fe_splits: Dict[str, pd.DataFrame] = {}\n    for name, df in raw_splits.items():\n        fe_df = run_feature_engineering(df, cfg)\n        fe_splits[name] = fe_df\n    return fe_splits\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_single_split_data(\n    df: pd.DataFrame,\n    cfg: IronDailyConfig,\n    feature_cols: List[str] | None = None,\n) -> Tuple[Dict[str, np.ndarray], List[str]]:\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    if feature_cols is None:\n        feature_cols = [c for c in df.columns if c != 'date']\n    missing_cols = [c for c in feature_cols if c not in df.columns]\n    if missing_cols:\n        raise KeyError(f\"Missing expected feature columns: {missing_cols}\")\n    df = df[['date'] + feature_cols]\n    data_values = df[feature_cols].values.astype(np.float32)\n    stamp_slice = build_time_mark_array(df['date'], cfg)\n    split_entry = {\n        'data': data_values,\n        'stamp': stamp_slice.astype(np.float32),\n        'length': len(data_values),\n        'dates': df['date'].to_numpy(),\n    }\n    return split_entry, feature_cols\n\n\ndef prepare_splits_after_engineering(\n    fe_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Tuple[Dict[str, Dict[str, np.ndarray]], List[str]]:\n    split_info: Dict[str, Dict[str, np.ndarray]] = {}\n    feature_cols: List[str] | None = None\n    for name in ['train', 'val', 'test']:\n        if name not in fe_splits:\n            raise KeyError(f\"Missing split '{name}' in engineered datasets.\")\n        split_entry, feature_cols = prepare_single_split_data(fe_splits[name], cfg, feature_cols)\n        split_info[name] = split_entry\n\n    # \u6807\u51c6\u5316\u9664\u76ee\u6807\u5217\u4e4b\u5916\u7684\u7279\u5f81\uff08\u4f7f\u7528\u8bad\u7ec3\u96c6\u7edf\u8ba1\u91cf\uff09\uff0c\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    if 'train' in split_info:\n        train_data = split_info['train']['data']\n        if isinstance(train_data, np.ndarray) and train_data.ndim == 2 and train_data.shape[1] > 1:\n            num_features = train_data.shape[1]\n            feat_slice = slice(0, num_features - 1)  # \u6700\u540e\u4e00\u5217\u4e3a\u76ee\u6807y\uff0c\u4fdd\u6301\u539f\u5c3a\u5ea6\n            mean = train_data[:, feat_slice].mean(axis=0, keepdims=True)\n            std = train_data[:, feat_slice].std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            for name in ['train', 'val', 'test']:\n                data = split_info[name]['data'].astype(np.float32)\n                data[:, feat_slice] = (data[:, feat_slice] - mean) / std\n                split_info[name]['data'] = data\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=False,\n            worker_init_fn=_worker_init_fn,\n            generator=_shared_generator,\n        )\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef _collect_log_forecasts(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:\n    \"\"\"Helper that returns (preds, trues, naive) in log space.\"\"\"\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    naives: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            # \u5f53\u524d\u914d\u7f6e\u4e2d\u603b\u662f\u4f7f\u7528\u591a\u5c42\u4e0b\u91c7\u6837\uff0c\u56e0\u6b64\u89e3\u7801\u5668\u8f93\u5165\u6052\u4e3a None\n            dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n\n            # Naive baseline: repeat last observed target value over the horizon\n            if cfg.c_out == 1:\n                last_val = batch_x[:, -1:, -1:]\n            else:\n                last_val = batch_x[:, -1:, 0:1]\n            naive_y = last_val.repeat(1, cfg.pred_len, 1)\n\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n            naives.append(naive_y.cpu().numpy())\n    if not preds:\n        return None, None, None\n\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    naive_arr = np.concatenate(naives, axis=0)\n    return preds_arr, trues_arr, naive_arr\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n    calibr: Tuple[float, float] | None = None,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Evaluate model on a loader and compute error metrics.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n\n    # Blend model and naive forecasts in log space\n    alpha = getattr(cfg, \"blend_alpha\", 0.8)\n    preds_arr = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n    # Optional linear calibration in log-space: y \u2248 w * y_pred + b\n    if calibr is not None:\n        w, b = calibr\n        preds_arr = preds_arr * float(w) + float(b)\n\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(\n        np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None))\n    )\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Tune blend_alpha and affine log calibration using a proxy close to final fitness.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    def _combo_score(pred_log: np.ndarray) -> float:\n        \"\"\"Lower is better: combines log/linear errors and directional accuracy.\"\"\"\n        err = pred_log - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        pred_val = np.expm1(pred_log)\n        true_val = np.expm1(trues_arr)\n        mape = float(\n            np.mean(\n                np.abs(pred_val - true_val) / np.clip(true_val, 1e-6, None)\n            )\n        )\n        da = compute_directional_accuracy(pred_val, true_val)\n        if not np.isfinite(da):\n            da = 0.5\n        return mse + mae + mape + (1.0 - da)\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # \u5728 [0,1] \u533a\u95f4\u4e0a\u4f7f\u7528 0.05 \u6b65\u957f\u641c\u7d22 alpha\uff0c\u5e76\u540c\u65f6\u6bd4\u8f83\u201c\u5e26\u7ebf\u6027\u6821\u51c6\u201d\u548c\u201c\u6052\u7b49\u6620\u5c04\u201d\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        # \u95ed\u5f0f\u89e3\u62df\u5408 log \u7a7a\u95f4\u7ebf\u6027\u6821\u51c6 y \u2248 w * blended + b\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        calibrated = blended * float(w) + float(b)\n        score_cal = _combo_score(calibrated)\n        score_id = _combo_score(blended)\n\n        if score_id <= score_cal:\n            score = score_id\n            w, b = 1.0, 0.0\n        else:\n            score = score_cal\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n\n\ndef tune_blend_alpha(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> float:\n    # Deprecated: blend_alpha is now tuned inside compute_log_calibration.\n    return float(getattr(cfg, \"blend_alpha\", 0.8))\n\n\ndef train_predict_evaluate() -> None:\n    cfg = IronDailyConfig()\n    print(\"1) \u52a0\u8f7d\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6...\")\n    raw_splits, split_paths = load_splits_data(cfg)\n    print(f\"   \u5df2\u52a0\u8f7d\u6570\u636e\uff1a{', '.join(str(p.name) for p in split_paths.values())}\")\n\n    print(\"   \u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in raw_splits.items()})\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u5bf9\u62c6\u5206\u540e\u7684\u6570\u636e\u5206\u522b\u53d8\u6362...\")\n    fe_splits = run_feature_engineering_on_splits(raw_splits, cfg)\n    print(\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in fe_splits.items()})\n\n    print(\"3) \u6570\u636e\u7a97\u53e3\u6784\u5efa\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_splits_after_engineering(fe_splits, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {name: len(loader.dataset) for name, loader in loaders.items()}\n    print(\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1a\", dataset_sizes)\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    # \u4f7f\u7528\u8f7b\u5fae\u7684\u6743\u91cd\u8870\u51cf\u63d0\u5347\u6cdb\u5316\u80fd\u529b\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=5e-4)\n    criterion = nn.MSELoss()\n    # \u7a0d\u5f31\u7684\u65b9\u5411\u6b63\u5219\uff0c\u66f4\u597d\u517c\u987eMSE/MAE\u4e0e\u65b9\u5411\u4e00\u81f4\u6027\n    lambda_dir = 0.08\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, lr={cfg.learning_rate}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}, down_layers={cfg.down_sampling_layers}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        # \u65e9\u505c\u76f4\u63a5\u4ee5\u9a8c\u8bc1\u96c6 MSE \u4e3a\u51c6\uff0c\u4fdd\u6301\u76ee\u6807\u7b80\u5355\u7a33\u5b9a\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    # \u5728\u9a8c\u8bc1\u96c6\u4e0a\u8054\u5408\u641c\u7d22blend_alpha\u5e76\u62df\u5408\u7b80\u5355\u7ebf\u6027\u6821\u51c6\u53c2\u6570\n    calib_w, calib_b = compute_log_calibration(model, loaders[\"val\"], cfg, cfg.device_obj)\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj, calibr=(calib_w, calib_b)\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n# EVOLVE-BLOCK-END\n    return test_mse, test_mae, test_mape, test_da\n\nif __name__ == \"__main__\":\n    test_mse, test_mae, test_mape, test_da = train_predict_evaluate()\n    \n", "language": "python", "parent_id": "c2ec36cb-9785-4671-b166-517a52b8550b", "generation": 28, "timestamp": 1765071320.0014672, "iteration_found": 751, "metrics": {"combined_score": 0.6659707670044619, "test_mse": 0.0006789066246710718, "test_mae": 0.019637947902083397, "test_mape": 0.019678272306919098, "test_da": 0.5738636363636364}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 73 lines with 59 lines\nChange 2: Replace 47 lines with 43 lines", "parent_metrics": {"combined_score": 0.6659707670044619, "test_mse": 0.0006789066246710718, "test_mae": 0.019637947902083397, "test_mape": 0.019678272306919098, "test_da": 0.5738636363636364}, "island": 1}, "prompts": {"diff_user": {"system": "You are optimizing the iron 2601 price time-series forecasting pipeline (TimeMixer baseline) on iron futures daily data.\nObjective: minimize test_mse/test_mae/test_mape and maximize test_da returned by train_predict_evaluate() (evaluator combines them).\n\nHard constraints:\n  - Only modify code inside the EVOLVE-BLOCK; do not touch paths, helper functions, or signatures outside the block.\n  - Keep the script runnable standalone in its folder: load merged_data.csv via existing config/path logic; keep I/O and train_predict_evaluate() interface (returns test_mse, test_mae, test_mape, test_da).\n  - Preserve required data processing/feature steps unless you improve them inside the block; do not break dataset splits or output shapes.\n  - You must keep pred_len=12 for all models.\n  - You must use the existing data (train_raw.csv, val_raw.csv, test_raw.csv)\n  - You can not modify _set_global_seed()\n\nFreedom:\n  - Inside EVOLVE-BLOCK you may change model architecture (TimeMixer tweaks, other DL/ML models), hyperparameters, feature engineering, optimization schedule, augmentation, calibration. The only object is getting higher score.\n\nOutput format:\n  - Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If no valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.6660\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.6660\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 4 lines with 4 lines\nChange 2: Replace 4 lines with 3 lines\nChange 3: Replace 5 lines with 2 lines\nChange 4: Replace 14 lines with 9 lines\nChange 5: Replace 3 lines with 3 lines\n- Metrics: combined_score: 0.6660, test_mse: 0.0007, test_mae: 0.0196, test_mape: 0.0197, test_da: 0.5739\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 8 lines with \n- Metrics: combined_score: 0.6660, test_mse: 0.0007, test_mae: 0.0196, test_mape: 0.0197, test_da: 0.5739\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 76 lines with 60 lines\n- Metrics: combined_score: 0.6660, test_mse: 0.0007, test_mae: 0.0196, test_mape: 0.0197, test_da: 0.5739\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.6660)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nimport os\n\n# Ensure deterministic CuBLAS workspace for CUDA deterministic algorithms\nif torch.cuda.is_available():\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\n\ndef _set_global_seed(seed: int = 2021) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        try:\n            torch.use_deterministic_algorithms(True)\n        except Exception:\n            # Older torch versions may not have this or certain ops may not support it\n            pass\n\n_set_global_seed(FIX_SEED)\n\n# Generator for DataLoader to keep shuffling deterministic\n_shared_generator = torch.Generator()\n_shared_generator.manual_seed(FIX_SEED)\n\ndef _worker_init_fn(worker_id: int) -> None:\n    # Ensure each worker has a deterministic seed derived from global seed\n    worker_seed = FIX_SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n# EVOLVE-BLOCK-START\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\ndef time_features(dates, freq: str = \"b\") -> np.ndarray:\n    \"\"\"Business-day calendar features (dow/dom/doy scaled to [-0.5, 0.5]).\"\"\"\n    dates = pd.to_datetime(dates)\n    dow = dates.dayofweek / 6.0 - 0.5\n    dom = (dates.day - 1) / 30.0 - 0.5\n    doy = (dates.dayofyear - 1) / 365.0 - 0.5\n    return np.vstack([dow, dom, doy])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# -----------------------------------------------------------------------------\n# Feature fusion helpers (minimal stub, real fusion not used in this script)\n# -----------------------------------------------------------------------------\nDEFAULT_FUSION_CONFIG: Dict[str, Any] = {}\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    raise NotImplementedError(\"Feature fusion is disabled; use cached train/val/test CSV splits.\")\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\n# DFTSeriesDecomp is unnecessary here because decomp_method is fixed to 'moving_avg'.\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\n# TemporalEmbedding / FixedEmbedding stubs are not needed since embed='timeF'\n# always routes through TimeFeatureEmbedding in DataEmbeddingWoPos.\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        # For this task we always use calendar time features (embed='timeF')\n        self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        # Keep only what is actually used in forward to simplify the block.\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method != 'moving_avg':\n            raise ValueError('Unsupported decomposition method')\n        self.decomposition = SeriesDecomp(configs.moving_avg)\n        if configs.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list, trend_list = [], []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list: List[torch.Tensor] = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList(\n            [PastDecomposableMixing(configs) for _ in range(configs.e_layers)]\n        )\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        # dir_adjust_scale is kept for compatibility with original configs but\n        # we do not use additional future-temporal gates, keeping the model lean.\n        self.dir_adjust_scale = getattr(configs, \"dir_adjust_scale\", 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(\n                1, configs.d_model, configs.embed, configs.freq, configs.dropout\n            )\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(\n                configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n            )\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList(\n            [\n                Normalize(\n                    configs.enc_in,\n                    affine=True,\n                    non_norm=True if configs.use_norm == 0 else False,\n                )\n                for _ in range(configs.down_sampling_layers + 1)\n            ]\n        )\n        if self.task_name in [\"long_term_forecast\", \"short_term_forecast\"]:\n            self.predict_layers = nn.ModuleList(\n                [\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ]\n            )\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(\n                    configs.d_model, configs.c_out, bias=True\n                )\n                self.out_res_layers = nn.ModuleList(\n                    [\n                        nn.Linear(\n                            configs.seq_len // (configs.down_sampling_window ** i),\n                            configs.seq_len // (configs.down_sampling_window ** i),\n                        )\n                        for i in range(configs.down_sampling_layers + 1)\n                    ]\n                )\n                self.regression_layers = nn.ModuleList(\n                    [\n                        nn.Linear(\n                            configs.seq_len // (configs.down_sampling_window ** i),\n                            configs.pred_len,\n                        )\n                        for i in range(configs.down_sampling_layers + 1)\n                    ]\n                )\n            # Learnable weights for aggregating multi-scale predictions\n            self.scale_weights = nn.Parameter(\n                torch.ones(configs.down_sampling_layers + 1)\n            )\n        else:\n            raise ValueError(\"Unsupported task name\")\n\n    def out_projection(\n        self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor\n    ) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(\n        self, x_list: List[torch.Tensor]\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == \"max\":\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == \"avg\":\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == \"conv\":\n            padding = 1 if torch.__version__ >= \"1.5.0\" else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError(\"Unknown down sampling method\")\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(\n                    x_mark_enc_mark_ori[:, :: self.configs.down_sampling_window, :]\n                )\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[\n                    :, :: self.configs.down_sampling_window, :\n                ]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        # We do not use future temporal features in this task, so decoding relies\n        # purely on past information aggregated at multiple time scales.\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list: List[torch.Tensor] = []\n        x_mark_processed: List[torch.Tensor] = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(\n                x_enc_list, x_mark_list, self.normalize_layers\n            ):\n                x = norm_layer(x, \"norm\")\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, \"norm\")\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n\n        enc_out_list: List[torch.Tensor] = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out_stack = torch.stack(dec_out_list, dim=-1)\n        if hasattr(self, \"scale_weights\"):\n            weights = torch.softmax(self.scale_weights, dim=0)\n            dec_out = (dec_out_stack * weights.view(1, 1, 1, -1)).sum(-1)\n        else:\n            dec_out = dec_out_stack.sum(-1)\n        dec_out = self.normalize_layers[0](dec_out, \"denorm\")\n        return dec_out\n\n    def future_multi_mixing(\n        self, B: int, enc_out_list: List[torch.Tensor], x_list\n    ) -> List[torch.Tensor]:\n        # The pipeline always uses channel_independence=0, so we only need the\n        # shared multi-scale regression path.\n        enc_inputs, out_res_list = x_list\n        dec_out_list: List[torch.Tensor] = []\n        for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n            dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(\n                0, 2, 1\n            )\n            dec_out = self.out_projection(dec_out, i, out_res)\n            dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in [\"long_term_forecast\", \"short_term_forecast\"]:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        raise ValueError(\"Unsupported task name for TimeMixer\")\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # project_root: Path = Path(__file__).resolve().parents[0]\n    # project_root: Path = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test\\exp_iron_4_gpu\") \n    project_root: Path = Path(r\"/home/jovyan/research/kaikai/c3i/AutoMLAgent/openevolve/iron_test/exp_iron_4_gpu\") \n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    cached_split_dir: Path | None = None\n    use_cached_splits: bool = True\n    seq_len: int = 48\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 1000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # device: str = \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n    blend_alpha: float = 0.8\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.cached_split_dir is None:\n            self.cached_split_dir = self.project_root / \"data\"\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_split_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    \"\"\"Unused helper in this standalone pipeline; cached CSV splits are loaded instead.\"\"\"\n    raise NotImplementedError(\n        \"fuse_and_align_features is unused; cached train/val/test CSV splits are loaded instead.\"\n    )\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    # unused helper; cached CSV splits are required\n    raise NotImplementedError(\"compute_split_borders is disabled; cached train/val/test splits are required.\")\n\n\ndef get_split_cache_paths(cfg: IronDailyConfig) -> Dict[str, Path]:\n    names = ['train', 'val', 'test']\n    return {name: cfg.cached_split_dir / f\"{name}_raw.csv\" for name in names}\n\n\ndef split_raw_dataframe(fused_df: pd.DataFrame, cfg: IronDailyConfig) -> Dict[str, pd.DataFrame]:\n    # unused helper; cached train/val/test splits must be provided instead\n    raise NotImplementedError(\"split_raw_dataframe is unused in this pipeline; cached splits must be provided.\")\n\n\ndef load_splits_data(\n    cfg: IronDailyConfig,\n) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path], bool]:\n    split_paths = get_split_cache_paths(cfg)\n    if cfg.use_cached_splits and all(path.exists() for path in split_paths.values()):\n        logger.info(\"Loading cached splits from %s\", cfg.cached_split_dir)\n        splits = {\n            name: pd.read_csv(path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n            for name, path in split_paths.items()\n        }\n        return splits, split_paths\n\n\ndef run_feature_engineering_on_splits(\n    raw_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Dict[str, pd.DataFrame]:\n    fe_splits: Dict[str, pd.DataFrame] = {}\n    for name, df in raw_splits.items():\n        fe_df = run_feature_engineering(df, cfg)\n        fe_splits[name] = fe_df\n    return fe_splits\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_single_split_data(\n    df: pd.DataFrame,\n    cfg: IronDailyConfig,\n    feature_cols: List[str] | None = None,\n) -> Tuple[Dict[str, np.ndarray], List[str]]:\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    if feature_cols is None:\n        feature_cols = [c for c in df.columns if c != 'date']\n    missing_cols = [c for c in feature_cols if c not in df.columns]\n    if missing_cols:\n        raise KeyError(f\"Missing expected feature columns: {missing_cols}\")\n    df = df[['date'] + feature_cols]\n    data_values = df[feature_cols].values.astype(np.float32)\n    stamp_slice = build_time_mark_array(df['date'], cfg)\n    split_entry = {\n        'data': data_values,\n        'stamp': stamp_slice.astype(np.float32),\n        'length': len(data_values),\n        'dates': df['date'].to_numpy(),\n    }\n    return split_entry, feature_cols\n\n\ndef prepare_splits_after_engineering(\n    fe_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Tuple[Dict[str, Dict[str, np.ndarray]], List[str]]:\n    split_info: Dict[str, Dict[str, np.ndarray]] = {}\n    feature_cols: List[str] | None = None\n    for name in ['train', 'val', 'test']:\n        if name not in fe_splits:\n            raise KeyError(f\"Missing split '{name}' in engineered datasets.\")\n        split_entry, feature_cols = prepare_single_split_data(fe_splits[name], cfg, feature_cols)\n        split_info[name] = split_entry\n\n    # \u6807\u51c6\u5316\u9664\u76ee\u6807\u5217\u4e4b\u5916\u7684\u7279\u5f81\uff08\u4f7f\u7528\u8bad\u7ec3\u96c6\u7edf\u8ba1\u91cf\uff09\uff0c\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    if 'train' in split_info:\n        train_data = split_info['train']['data']\n        if isinstance(train_data, np.ndarray) and train_data.ndim == 2 and train_data.shape[1] > 1:\n            num_features = train_data.shape[1]\n            feat_slice = slice(0, num_features - 1)  # \u6700\u540e\u4e00\u5217\u4e3a\u76ee\u6807y\uff0c\u4fdd\u6301\u539f\u5c3a\u5ea6\n            mean = train_data[:, feat_slice].mean(axis=0, keepdims=True)\n            std = train_data[:, feat_slice].std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            for name in ['train', 'val', 'test']:\n                data = split_info[name]['data'].astype(np.float32)\n                data[:, feat_slice] = (data[:, feat_slice] - mean) / std\n                split_info[name]['data'] = data\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=False,\n            worker_init_fn=_worker_init_fn,\n            generator=_shared_generator,\n        )\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef _collect_log_forecasts(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:\n    \"\"\"Run model on a loader and return (preds, trues, naive) in log space.\"\"\"\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    naives: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            # \u5f53\u524d\u914d\u7f6e\u4e2d\u603b\u662f\u4f7f\u7528\u591a\u5c42\u4e0b\u91c7\u6837\uff0c\u56e0\u6b64\u89e3\u7801\u5668\u8f93\u5165\u6052\u4e3a None\n            dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n\n            # Naive baseline: repeat last observed target value over the horizon\n            if cfg.c_out == 1:\n                last_val = batch_x[:, -1:, -1:]\n            else:\n                last_val = batch_x[:, -1:, 0:1]\n            naive_y = last_val.repeat(1, cfg.pred_len, 1)\n\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n            naives.append(naive_y.cpu().numpy())\n    if not preds:\n        return None, None, None\n\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    naive_arr = np.concatenate(naives, axis=0)\n    return preds_arr, trues_arr, naive_arr\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n    calibr: Tuple[float, float] | None = None,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Evaluate model; optionally apply affine calibration in log-space.\n\n    Additionally blend the model forecast with a simple naive \"last value\"\n    baseline in log space, which empirically stabilises errors on this task.\n    \"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n\n    # Blend model and naive forecasts in log space\n    alpha = getattr(cfg, \"blend_alpha\", 0.8)\n    preds_arr = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n    # Optional linear calibration in log-space: y \u2248 w * y_pred + b\n    if calibr is not None:\n        w, b = calibr\n        preds_arr = preds_arr * float(w) + float(b)\n\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(\n        np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None))\n    )\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Grid-search blend_alpha and affine log-space calibration on validation data.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # Finer grid over [0, 1] for the blend weight between model and naive\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        # Closed-form linear calibration y \u2248 w * blended + b in log space\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        # Compare calibrated vs identity mapping using (MSE + MAE) in log space\n        calibrated = blended * float(w) + float(b)\n        err = calibrated - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        score = mse + mae\n\n        err_id = blended - trues_arr\n        mse_id = float((err_id ** 2).mean())\n        mae_id = float(np.abs(err_id).mean())\n        score_id = mse_id + mae_id\n        if score_id < score:\n            score = score_id\n            w, b = 1.0, 0.0\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n\n\ndef tune_blend_alpha(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> float:\n    return float(getattr(cfg, \"blend_alpha\", 0.8))\n\n\ndef train_predict_evaluate() -> None:\n    cfg = IronDailyConfig()\n    print(\"1) \u52a0\u8f7d\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6...\")\n    raw_splits, split_paths = load_splits_data(cfg)\n    print(f\"   \u5df2\u52a0\u8f7d\u6570\u636e\uff1a{', '.join(str(p.name) for p in split_paths.values())}\")\n\n    print(\"   \u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in raw_splits.items()})\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u5bf9\u62c6\u5206\u540e\u7684\u6570\u636e\u5206\u522b\u53d8\u6362...\")\n    fe_splits = run_feature_engineering_on_splits(raw_splits, cfg)\n    print(\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in fe_splits.items()})\n\n    print(\"3) \u6570\u636e\u7a97\u53e3\u6784\u5efa\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_splits_after_engineering(fe_splits, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {name: len(loader.dataset) for name, loader in loaders.items()}\n    print(\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1a\", dataset_sizes)\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    # \u4f7f\u7528\u8f7b\u5fae\u7684\u6743\u91cd\u8870\u51cf\u63d0\u5347\u6cdb\u5316\u80fd\u529b\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=5e-4)\n    criterion = nn.MSELoss()\n    # \u7a0d\u5f31\u7684\u65b9\u5411\u6b63\u5219\uff0c\u66f4\u597d\u517c\u987eMSE/MAE\u4e0e\u65b9\u5411\u4e00\u81f4\u6027\n    lambda_dir = 0.08\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, lr={cfg.learning_rate}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}, down_layers={cfg.down_sampling_layers}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    # \u5728\u9a8c\u8bc1\u96c6\u4e0a\u540c\u65f6\u641c\u7d22\u6700\u4f18blend_alpha\u5e76\u62df\u5408\u7b80\u5355\u7ebf\u6027\u6821\u51c6\u53c2\u6570\n    calib_w, calib_b = compute_log_calibration(model, loaders[\"val\"], cfg, cfg.device_obj)\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj, calibr=(calib_w, calib_b)\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n# EVOLVE-BLOCK-END\n    return test_mse, test_mae, test_mape, test_da\n\nif __name__ == \"__main__\":\n    test_mse, test_mae, test_mape, test_da = train_predict_evaluate()\n    \n\n```\nKey features: Performs well on combined_score (0.6660), Performs well on test_mse (0.0007), Performs well on test_mae (0.0196), Performs well on test_mape (0.0197), Performs well on test_da (0.5739)\n\n### Program 2 (Score: 0.6660)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nimport os\n\n# Ensure deterministic CuBLAS workspace for CUDA deterministic algorithms\nif torch.cuda.is_available():\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\n\ndef _set_global_seed(seed: int = 2021) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        try:\n            torch.use_deterministic_algorithms(True)\n        except Exception:\n            # Older torch versions may not have this or certain ops may not support it\n            pass\n\n_set_global_seed(FIX_SEED)\n\n# Generator for DataLoader to keep shuffling deterministic\n_shared_generator = torch.Generator()\n_shared_generator.manual_seed(FIX_SEED)\n\ndef _worker_init_fn(worker_id: int) -> None:\n    # Ensure each worker has a deterministic seed derived from global seed\n    worker_seed = FIX_SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n# EVOLVE-BLOCK-START\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\ndef time_features(dates, freq: str = \"b\") -> np.ndarray:\n    \"\"\"Business-day calendar features (dow/dom/doy scaled to [-0.5, 0.5]).\"\"\"\n    dates = pd.to_datetime(dates)\n    dow = dates.dayofweek / 6.0 - 0.5\n    dom = (dates.day - 1) / 30.0 - 0.5\n    doy = (dates.dayofyear - 1) / 365.0 - 0.5\n    return np.vstack([dow, dom, doy])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# Feature fusion is disabled in this standalone script; keep a tiny placeholder.\nDEFAULT_FUSION_CONFIG: Dict[str, Any] = {}\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:  # pragma: no cover\n    raise NotImplementedError(\n        \"Feature fusion is disabled; provide cached train_raw/val_raw/test_raw CSVs instead.\"\n    )\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\n# DFTSeriesDecomp is unnecessary here because decomp_method is fixed to 'moving_avg'.\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\n# TemporalEmbedding / FixedEmbedding stubs are not needed since embed='timeF'\n# always routes through TimeFeatureEmbedding in DataEmbeddingWoPos.\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        # For this task we always use calendar time features (embed='timeF')\n        self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method != 'moving_avg':\n            raise ValueError('Unsupported decomposition method')\n        self.decomposition = SeriesDecomp(configs.moving_avg)\n        if self.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list: List[torch.Tensor] = []\n        trend_list: List[torch.Tensor] = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list: List[torch.Tensor] = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n            # Learnable weights for aggregating multi-scale predictions instead of a simple sum\n            self.scale_weights = nn.Parameter(torch.ones(configs.down_sampling_layers + 1))\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        # In this task we always set use_future_temporal_feature=0, so we skip\n        # the unused future-time gating logic and directly build multi-scale\n        # encoder inputs. This keeps the forward pass compact but is behaviour-\n        # equivalent for the current configuration.\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list: List[torch.Tensor] = []\n        x_mark_processed: List[torch.Tensor] = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list: List[torch.Tensor] = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        # Multi-scale regression + projection\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out_stack = torch.stack(dec_out_list, dim=-1)\n        # Aggregate predictions from different scales using learnable softmax weights\n        if hasattr(self, \"scale_weights\"):\n            weights = torch.softmax(self.scale_weights, dim=0)\n            dec_out = (dec_out_stack * weights.view(1, 1, 1, -1)).sum(-1)\n        else:\n            dec_out = dec_out_stack.sum(-1)\n        # Denormalise back to the original scale of encoder inputs\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        # With channel_independence fixed to 0 in this pipeline, we only need\n        # the shared multi-scale regression path, which removes unused branches\n        # and slightly reduces overhead without changing behaviour.\n        enc_inputs, out_res_list = x_list\n        dec_out_list: List[torch.Tensor] = []\n        for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n            dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n            dec_out = self.out_projection(dec_out, i, out_res)\n            dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        raise ValueError('Unsupported task name for TimeMixer')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # project_root: Path = Path(__file__).resolve().parents[0]\n    # project_root: Path = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test\\exp_iron_4_gpu\") \n    project_root: Path = Path(r\"/home/jovyan/research/kaikai/c3i/AutoMLAgent/openevolve/iron_test/exp_iron_4_gpu\") \n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    cached_split_dir: Path | None = None\n    use_cached_splits: bool = True\n    seq_len: int = 48\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 1000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # device: str = \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n    blend_alpha: float = 0.8\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.cached_split_dir is None:\n            self.cached_split_dir = self.project_root / \"data\"\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_split_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    \"\"\"Unused in this standalone pipeline; cached CSV splits are loaded instead.\"\"\"\n    raise NotImplementedError(\n        \"fuse_and_align_features is unused; cached train/val/test splits are loaded instead.\"\n    )\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    # unused helper; cached CSV splits are required\n    raise NotImplementedError(\"compute_split_borders is disabled; cached train/val/test splits are required.\")\n\n\ndef get_split_cache_paths(cfg: IronDailyConfig) -> Dict[str, Path]:\n    names = ['train', 'val', 'test']\n    return {name: cfg.cached_split_dir / f\"{name}_raw.csv\" for name in names}\n\n\ndef split_raw_dataframe(fused_df: pd.DataFrame, cfg: IronDailyConfig) -> Dict[str, pd.DataFrame]:\n    # unused helper; cached train/val/test splits must be provided instead\n    raise NotImplementedError(\"split_raw_dataframe is unused in this pipeline; cached splits must be provided.\")\n\n\ndef load_splits_data(\n    cfg: IronDailyConfig,\n) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path], bool]:\n    split_paths = get_split_cache_paths(cfg)\n    if cfg.use_cached_splits and all(path.exists() for path in split_paths.values()):\n        logger.info(\"Loading cached splits from %s\", cfg.cached_split_dir)\n        splits = {\n            name: pd.read_csv(path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n            for name, path in split_paths.items()\n        }\n        return splits, split_paths\n\n\ndef run_feature_engineering_on_splits(\n    raw_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Dict[str, pd.DataFrame]:\n    fe_splits: Dict[str, pd.DataFrame] = {}\n    for name, df in raw_splits.items():\n        fe_df = run_feature_engineering(df, cfg)\n        fe_splits[name] = fe_df\n    return fe_splits\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_single_split_data(\n    df: pd.DataFrame,\n    cfg: IronDailyConfig,\n    feature_cols: List[str] | None = None,\n) -> Tuple[Dict[str, np.ndarray], List[str]]:\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    if feature_cols is None:\n        feature_cols = [c for c in df.columns if c != 'date']\n    missing_cols = [c for c in feature_cols if c not in df.columns]\n    if missing_cols:\n        raise KeyError(f\"Missing expected feature columns: {missing_cols}\")\n    df = df[['date'] + feature_cols]\n    data_values = df[feature_cols].values.astype(np.float32)\n    stamp_slice = build_time_mark_array(df['date'], cfg)\n    split_entry = {\n        'data': data_values,\n        'stamp': stamp_slice.astype(np.float32),\n        'length': len(data_values),\n        'dates': df['date'].to_numpy(),\n    }\n    return split_entry, feature_cols\n\n\ndef prepare_splits_after_engineering(\n    fe_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Tuple[Dict[str, Dict[str, np.ndarray]], List[str]]:\n    split_info: Dict[str, Dict[str, np.ndarray]] = {}\n    feature_cols: List[str] | None = None\n    for name in ['train', 'val', 'test']:\n        if name not in fe_splits:\n            raise KeyError(f\"Missing split '{name}' in engineered datasets.\")\n        split_entry, feature_cols = prepare_single_split_data(fe_splits[name], cfg, feature_cols)\n        split_info[name] = split_entry\n\n    # \u6807\u51c6\u5316\u9664\u76ee\u6807\u5217\u4e4b\u5916\u7684\u7279\u5f81\uff08\u4f7f\u7528\u8bad\u7ec3\u96c6\u7edf\u8ba1\u91cf\uff09\uff0c\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    if 'train' in split_info:\n        train_data = split_info['train']['data']\n        if isinstance(train_data, np.ndarray) and train_data.ndim == 2 and train_data.shape[1] > 1:\n            num_features = train_data.shape[1]\n            feat_slice = slice(0, num_features - 1)  # \u6700\u540e\u4e00\u5217\u4e3a\u76ee\u6807y\uff0c\u4fdd\u6301\u539f\u5c3a\u5ea6\n            mean = train_data[:, feat_slice].mean(axis=0, keepdims=True)\n            std = train_data[:, feat_slice].std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            for name in ['train', 'val', 'test']:\n                data = split_info[name]['data'].astype(np.float32)\n                data[:, feat_slice] = (data[:, feat_slice] - mean) / std\n                split_info[name]['data'] = data\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=False,\n            worker_init_fn=_worker_init_fn,\n            generator=_shared_generator,\n        )\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef _collect_log_forecasts(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:\n    \"\"\"Helper that returns (preds, trues, naive) in log space.\"\"\"\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    naives: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            # \u5f53\u524d\u914d\u7f6e\u4e2d\u603b\u662f\u4f7f\u7528\u591a\u5c42\u4e0b\u91c7\u6837\uff0c\u56e0\u6b64\u89e3\u7801\u5668\u8f93\u5165\u6052\u4e3a None\n            dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n\n            # Naive baseline: repeat last observed target value over the horizon\n            if cfg.c_out == 1:\n                last_val = batch_x[:, -1:, -1:]\n            else:\n                last_val = batch_x[:, -1:, 0:1]\n            naive_y = last_val.repeat(1, cfg.pred_len, 1)\n\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n            naives.append(naive_y.cpu().numpy())\n    if not preds:\n        return None, None, None\n\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    naive_arr = np.concatenate(naives, axis=0)\n    return preds_arr, trues_arr, naive_arr\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n    calibr: Tuple[float, float] | None = None,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Evaluate model on a loader and compute error metrics.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n\n    # Blend model and naive forecasts in log space\n    alpha = getattr(cfg, \"blend_alpha\", 0.8)\n    preds_arr = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n    # Optional linear calibration in log-space: y \u2248 w * y_pred + b\n    if calibr is not None:\n        w, b = calibr\n        preds_arr = preds_arr * float(w) + float(b)\n\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(\n        np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None))\n    )\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Jointly tune blend_alpha and affine log-space calibration on validation data.\n\n    We search a small grid of blend alphas; for each blended forecast we fit a\n    simple linear calibration y \u2248 w*y_pred + b in closed form, then keep the\n    combination that minimises (MSE + MAE) on the validation set. To avoid\n    harming performance, we also compare against an identity (no-calibration)\n    mapping and only apply calibration when it helps.\n    \"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # \u6269\u5927 alpha \u641c\u7d22\u7f51\u683c\u5230 [0.0, 1.0] \u5e76\u7ec6\u5316\u6b65\u957f\uff0c\u63d0\u9ad8\u5728\u9a8c\u8bc1\u96c6\u4e0a\u9009\u62e9\u6700\u4f18\u6df7\u5408\u6743\u91cd\u7684\u7cbe\u5ea6\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            # \u9000\u5316\u60c5\u51b5\uff1a\u65e0\u6cd5\u7a33\u5b9a\u4f30\u8ba1\u7ebf\u6027\u6821\u51c6\u53c2\u6570\uff0c\u56de\u9000\u5230\u6052\u7b49\u6620\u5c04\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                # \u51e0\u4e4e\u65e0\u65b9\u5dee\u65f6\u53ea\u6821\u6b63\u504f\u79fb\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        # \u8bc4\u4ef7\u201c\u5b66\u4e60\u5f97\u5230\u7684\u7ebf\u6027\u6821\u51c6\u201d\u548c\u201c\u65e0\u6821\u51c6\u6052\u7b49\u6620\u5c04\u201d\u4e24\u79cd\u65b9\u6848\uff0c\u907f\u514d\u6821\u51c6\u5728\u9a8c\u8bc1\u96c6\u4e0a\u6076\u5316\u8bef\u5dee\n        calibrated = blended * float(w) + float(b)\n        err = calibrated - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        score = mse + mae\n\n        err_id = blended - trues_arr\n        mse_id = float((err_id ** 2).mean())\n        mae_id = float(np.abs(err_id).mean())\n        score_id = mse_id + mae_id\n        if score_id < score:\n            score = score_id\n            w, b = 1.0, 0.0\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n\n\n\n\n\ndef train_predict_evaluate() -> None:\n    cfg = IronDailyConfig()\n    print(\"1) \u52a0\u8f7d\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6...\")\n    raw_splits, split_paths = load_splits_data(cfg)\n    print(f\"   \u5df2\u52a0\u8f7d\u6570\u636e\uff1a{', '.join(str(p.name) for p in split_paths.values())}\")\n\n    print(\"   \u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in raw_splits.items()})\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u5bf9\u62c6\u5206\u540e\u7684\u6570\u636e\u5206\u522b\u53d8\u6362...\")\n    fe_splits = run_feature_engineering_on_splits(raw_splits, cfg)\n    print(\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in fe_splits.items()})\n\n    print(\"3) \u6570\u636e\u7a97\u53e3\u6784\u5efa\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_splits_after_engineering(fe_splits, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {name: len(loader.dataset) for name, loader in loaders.items()}\n    print(\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1a\", dataset_sizes)\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    # \u4f7f\u7528\u8f7b\u5fae\u7684\u6743\u91cd\u8870\u51cf\u63d0\u5347\u6cdb\u5316\u80fd\u529b\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=5e-4)\n    criterion = nn.MSELoss()\n    # \u7a0d\u5f31\u7684\u65b9\u5411\u6b63\u5219\uff0c\u66f4\u597d\u517c\u987eMSE/MAE\u4e0e\u65b9\u5411\u4e00\u81f4\u6027\n    lambda_dir = 0.08\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, lr={cfg.learning_rate}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}, down_layers={cfg.down_sampling_layers}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    # \u5728\u9a8c\u8bc1\u96c6\u4e0a\u8054\u5408\u641c\u7d22blend_alpha\u5e76\u62df\u5408\u7b80\u5355\u7ebf\u6027\u6821\u51c6\u53c2\u6570\n    calib_w, calib_b = compute_log_calibration(model, loaders[\"val\"], cfg, cfg.device_obj)\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj, calibr=(calib_w, calib_b)\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n# EVOLVE-BLOCK-END\n    return test_mse, test_mae, test_mape, test_da\n\nif __name__ == \"__main__\":\n    test_mse, test_mae, test_mape, test_da = train_predict_evaluate()\n    \n\n```\nKey features: Performs well on combined_score (0.6660), Performs well on test_mse (0.0007), Performs well on test_mae (0.0196), Performs well on test_mape (0.0197), Performs well on test_da (0.5739)\n\n### Program 3 (Score: 0.6660)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nimport os\n\n# Ensure deterministic CuBLAS workspace for CUDA deterministic algorithms\nif torch.cuda.is_available():\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\n\ndef _set_global_seed(seed: int = 2021) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        try:\n            torch.use_deterministic_algorithms(True)\n        except Exception:\n            # Older torch versions may not have this or certain ops may not support it\n            pass\n\n_set_global_seed(FIX_SEED)\n\n# Generator for DataLoader to keep shuffling deterministic\n_shared_generator = torch.Generator()\n_shared_generator.manual_seed(FIX_SEED)\n\ndef _worker_init_fn(worker_id: int) -> None:\n    # Ensure each worker has a deterministic seed derived from global seed\n    worker_seed = FIX_SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n# EVOLVE-BLOCK-START\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\ndef time_features(dates, freq: str = \"b\") -> np.ndarray:\n    dates = pd.to_datetime(dates)\n    dow = dates.dayofweek / 6.0 - 0.5\n    dom = (dates.day - 1) / 30.0 - 0.5\n    doy = (dates.dayofyear - 1) / 365.0 - 0.5\n    return np.vstack([dow, dom, doy])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# -----------------------------------------------------------------------------\n# Feature fusion helpers (simplified stub)\n# -----------------------------------------------------------------------------\n# The main pipeline always loads cached train_raw/val_raw/test_raw CSVs via\n# `load_splits_data`, so heavy multi-file fusion is unnecessary here. A tiny\n# placeholder config plus a stub function is enough to satisfy\n# IronDailyConfig.__post_init__ without affecting runtime behaviour.\nDEFAULT_FUSION_CONFIG: Dict[str, Any] = {}\n\n\ndef build_feature_fusion_dataset(cfg: \"IronDailyConfig\") -> pd.DataFrame:\n    raise NotImplementedError(\n        \"Feature fusion is disabled in this standalone pipeline; \"\n        \"use cached train_raw/val_raw/test_raw CSV splits instead.\"\n    )\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\n# DFTSeriesDecomp is unnecessary here because decomp_method is fixed to 'moving_avg'.\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\n# TemporalEmbedding / FixedEmbedding stubs are not needed since embed='timeF'\n# always routes through TimeFeatureEmbedding in DataEmbeddingWoPos.\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        # For this task we always use calendar time features (embed='timeF')\n        self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        # Keep only what is actually used in forward to simplify the block\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method != 'moving_avg':\n            raise ValueError('Unsupported decomposition method')\n        self.decomposition = SeriesDecomp(configs.moving_avg)\n        if configs.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list, trend_list = [], []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list: List[torch.Tensor] = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList(\n            [PastDecomposableMixing(configs) for _ in range(configs.e_layers)]\n        )\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        # dir_adjust_scale kept for compatibility; we avoid extra future-time gates\n        self.dir_adjust_scale = getattr(configs, \"dir_adjust_scale\", 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(\n                1, configs.d_model, configs.embed, configs.freq, configs.dropout\n            )\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(\n                configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n            )\n        # number of encoder blocks is len(self.pdm_blocks)\n        self.normalize_layers = nn.ModuleList(\n            [\n                Normalize(\n                    configs.enc_in,\n                    affine=True,\n                    non_norm=True if configs.use_norm == 0 else False,\n                )\n                for _ in range(configs.down_sampling_layers + 1)\n            ]\n        )\n        if self.task_name in [\"long_term_forecast\", \"short_term_forecast\"]:\n            self.predict_layers = nn.ModuleList(\n                [\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ]\n            )\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(\n                    configs.d_model, configs.c_out, bias=True\n                )\n                self.out_res_layers = nn.ModuleList(\n                    [\n                        nn.Linear(\n                            configs.seq_len // (configs.down_sampling_window ** i),\n                            configs.seq_len // (configs.down_sampling_window ** i),\n                        )\n                        for i in range(configs.down_sampling_layers + 1)\n                    ]\n                )\n                self.regression_layers = nn.ModuleList(\n                    [\n                        nn.Linear(\n                            configs.seq_len // (configs.down_sampling_window ** i),\n                            configs.pred_len,\n                        )\n                        for i in range(configs.down_sampling_layers + 1)\n                    ]\n                )\n            # Learnable weights for aggregating multi-scale predictions\n            self.scale_weights = nn.Parameter(\n                torch.ones(configs.down_sampling_layers + 1)\n            )\n        else:\n            raise ValueError(\"Unsupported task name\")\n\n    def out_projection(\n        self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor\n    ) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(\n        self, x_list: List[torch.Tensor]\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == \"max\":\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == \"avg\":\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == \"conv\":\n            padding = 1 if torch.__version__ >= \"1.5.0\" else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError(\"Unknown down sampling method\")\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(\n                    x_mark_enc_mark_ori[:, :: self.configs.down_sampling_window, :]\n                )\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[\n                    :, :: self.configs.down_sampling_window, :\n                ]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        # This task does not use future temporal features; decoding relies only on past information.\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list: List[torch.Tensor] = []\n        x_mark_processed: List[torch.Tensor] = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(\n                x_enc_list, x_mark_list, self.normalize_layers\n            ):\n                x = norm_layer(x, \"norm\")\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, \"norm\")\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n\n        enc_out_list: List[torch.Tensor] = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n\n        for block in self.pdm_blocks:\n            enc_out_list = block(enc_out_list)\n\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out_stack = torch.stack(dec_out_list, dim=-1)\n        if hasattr(self, \"scale_weights\"):\n            weights = torch.softmax(self.scale_weights, dim=0)\n            dec_out = (dec_out_stack * weights.view(1, 1, 1, -1)).sum(-1)\n        else:\n            dec_out = dec_out_stack.sum(-1)\n        dec_out = self.normalize_layers[0](dec_out, \"denorm\")\n        return dec_out\n\n    def future_multi_mixing(\n        self, B: int, enc_out_list: List[torch.Tensor], x_list\n    ) -> List[torch.Tensor]:\n        # Pipeline uses channel_independence=0, so we only need the shared regression path.\n        enc_inputs, out_res_list = x_list\n        dec_out_list: List[torch.Tensor] = []\n        for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n            dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(\n                0, 2, 1\n            )\n            dec_out = self.out_projection(dec_out, i, out_res)\n            dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in [\"long_term_forecast\", \"short_term_forecast\"]:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        raise ValueError(\"Unsupported task name for TimeMixer\")\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # project_root: Path = Path(__file__).resolve().parents[0]\n    # project_root: Path = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test\\exp_iron_4_gpu\") \n    project_root: Path = Path(r\"/home/jovyan/research/kaikai/c3i/AutoMLAgent/openevolve/iron_test/exp_iron_4_gpu\") \n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    cached_split_dir: Path | None = None\n    use_cached_splits: bool = True\n    seq_len: int = 48\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 1000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # device: str = \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n    blend_alpha: float = 0.8\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.cached_split_dir is None:\n            self.cached_split_dir = self.project_root / \"data\"\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_split_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    raise NotImplementedError(\"fuse_and_align_features is unused; rely on cached CSV splits instead.\")\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    # unused helper; cached CSV splits are required\n    raise NotImplementedError(\"compute_split_borders is disabled; cached train/val/test splits are required.\")\n\n\ndef get_split_cache_paths(cfg: IronDailyConfig) -> Dict[str, Path]:\n    names = ['train', 'val', 'test']\n    return {name: cfg.cached_split_dir / f\"{name}_raw.csv\" for name in names}\n\n\ndef split_raw_dataframe(fused_df: pd.DataFrame, cfg: IronDailyConfig) -> Dict[str, pd.DataFrame]:\n    # unused helper; cached train/val/test splits must be provided instead\n    raise NotImplementedError(\"split_raw_dataframe is unused in this pipeline; cached splits must be provided.\")\n\n\ndef load_splits_data(\n    cfg: IronDailyConfig,\n) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path], bool]:\n    split_paths = get_split_cache_paths(cfg)\n    if cfg.use_cached_splits and all(path.exists() for path in split_paths.values()):\n        logger.info(\"Loading cached splits from %s\", cfg.cached_split_dir)\n        splits = {\n            name: pd.read_csv(path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n            for name, path in split_paths.items()\n        }\n        return splits, split_paths\n\n\ndef run_feature_engineering_on_splits(\n    raw_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Dict[str, pd.DataFrame]:\n    fe_splits: Dict[str, pd.DataFrame] = {}\n    for name, df in raw_splits.items():\n        fe_df = run_feature_engineering(df, cfg)\n        fe_splits[name] = fe_df\n    return fe_splits\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_single_split_data(\n    df: pd.DataFrame,\n    cfg: IronDailyConfig,\n    feature_cols: List[str] | None = None,\n) -> Tuple[Dict[str, np.ndarray], List[str]]:\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    if feature_cols is None:\n        feature_cols = [c for c in df.columns if c != 'date']\n    missing_cols = [c for c in feature_cols if c not in df.columns]\n    if missing_cols:\n        raise KeyError(f\"Missing expected feature columns: {missing_cols}\")\n    df = df[['date'] + feature_cols]\n    data_values = df[feature_cols].values.astype(np.float32)\n    stamp_slice = build_time_mark_array(df['date'], cfg)\n    split_entry = {\n        'data': data_values,\n        'stamp': stamp_slice.astype(np.float32),\n        'length': len(data_values),\n        'dates': df['date'].to_numpy(),\n    }\n    return split_entry, feature_cols\n\n\ndef prepare_splits_after_engineering(\n    fe_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Tuple[Dict[str, Dict[str, np.ndarray]], List[str]]:\n    split_info: Dict[str, Dict[str, np.ndarray]] = {}\n    feature_cols: List[str] | None = None\n    for name in ['train', 'val', 'test']:\n        if name not in fe_splits:\n            raise KeyError(f\"Missing split '{name}' in engineered datasets.\")\n        split_entry, feature_cols = prepare_single_split_data(fe_splits[name], cfg, feature_cols)\n        split_info[name] = split_entry\n\n    # \u6807\u51c6\u5316\u9664\u76ee\u6807\u5217\u4e4b\u5916\u7684\u7279\u5f81\uff08\u4f7f\u7528\u8bad\u7ec3\u96c6\u7edf\u8ba1\u91cf\uff09\uff0c\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    if 'train' in split_info:\n        train_data = split_info['train']['data']\n        if isinstance(train_data, np.ndarray) and train_data.ndim == 2 and train_data.shape[1] > 1:\n            num_features = train_data.shape[1]\n            feat_slice = slice(0, num_features - 1)  # \u6700\u540e\u4e00\u5217\u4e3a\u76ee\u6807y\uff0c\u4fdd\u6301\u539f\u5c3a\u5ea6\n            mean = train_data[:, feat_slice].mean(axis=0, keepdims=True)\n            std = train_data[:, feat_slice].std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            for name in ['train', 'val', 'test']:\n                data = split_info[name]['data'].astype(np.float32)\n                data[:, feat_slice] = (data[:, feat_slice] - mean) / std\n                split_info[name]['data'] = data\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=False,\n            worker_init_fn=_worker_init_fn,\n            generator=_shared_generator,\n        )\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef _collect_log_forecasts(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:\n    # Run model on a loader and return (preds, trues, naive) in log space.\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    naives: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            # \u5f53\u524d\u914d\u7f6e\u4e2d\u603b\u662f\u4f7f\u7528\u591a\u5c42\u4e0b\u91c7\u6837\uff0c\u56e0\u6b64\u89e3\u7801\u5668\u8f93\u5165\u6052\u4e3a None\n            outputs = model(batch_x, batch_x_mark, None, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n\n            # Naive baseline: repeat last observed target value over the horizon\n            if cfg.c_out == 1:\n                last_val = batch_x[:, -1:, -1:]\n            else:\n                last_val = batch_x[:, -1:, 0:1]\n            naive_y = last_val.repeat(1, cfg.pred_len, 1)\n\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n            naives.append(naive_y.cpu().numpy())\n    if not preds:\n        return None, None, None\n\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    naive_arr = np.concatenate(naives, axis=0)\n    return preds_arr, trues_arr, naive_arr\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n    calibr: Tuple[float, float] | None = None,\n) -> Tuple[float, float, float, float]:\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return math.nan, math.nan, math.nan, math.nan\n\n    alpha = getattr(cfg, \"blend_alpha\", 0.8)\n    preds_arr = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n    if calibr is not None:\n        w, b = calibr\n        preds_arr = preds_arr * float(w) + float(b)\n\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value, true_value = preds_arr, trues_arr\n    value_mape = np.mean(\n        np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None))\n    )\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    # Jointly tune blend_alpha and affine log-space calibration on validation data.\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # \u5728 [0,1] \u533a\u95f4\u4e0a\u4f7f\u7528\u66f4\u7ec6\u7684 0.05 \u6b65\u957f\u641c\u7d22 alpha\uff0c\u63d0\u9ad8\u6df7\u5408\u7cfb\u6570\u9009\u62e9\u7cbe\u5ea6\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            # \u9000\u5316\u60c5\u51b5\uff1a\u65e0\u6cd5\u7a33\u5b9a\u4f30\u8ba1\u7ebf\u6027\u6821\u51c6\u53c2\u6570\uff0c\u56de\u9000\u5230\u6052\u7b49\u6620\u5c04\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                # \u51e0\u4e4e\u65e0\u65b9\u5dee\u65f6\u53ea\u6821\u6b63\u504f\u79fb\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        # \u8bc4\u4ef7\u201c\u5b66\u4e60\u5f97\u5230\u7684\u7ebf\u6027\u6821\u51c6\u201d\u548c\u201c\u65e0\u6821\u51c6\u6052\u7b49\u6620\u5c04\u201d\u4e24\u79cd\u65b9\u6848\uff0c\u907f\u514d\u6821\u51c6\u5728\u9a8c\u8bc1\u96c6\u4e0a\u6076\u5316\u8bef\u5dee\n        calibrated = blended * float(w) + float(b)\n        err = calibrated - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        score = mse + mae\n\n        err_id = blended - trues_arr\n        mse_id = float((err_id ** 2).mean())\n        mae_id = float(np.abs(err_id).mean())\n        score_id = mse_id + mae_id\n        if score_id < score:\n            score = score_id\n            w, b = 1.0, 0.0\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n\n\n# tune_blend_alpha has been inlined into compute_log_calibration and is no longer used.\n\n\ndef train_predict_evaluate() -> None:\n    cfg = IronDailyConfig()\n    print(\"1) \u52a0\u8f7d\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6...\")\n    raw_splits, split_paths = load_splits_data(cfg)\n    print(f\"   \u5df2\u52a0\u8f7d\u6570\u636e\uff1a{', '.join(str(p.name) for p in split_paths.values())}\")\n\n    print(\"   \u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in raw_splits.items()})\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u5bf9\u62c6\u5206\u540e\u7684\u6570\u636e\u5206\u522b\u53d8\u6362...\")\n    fe_splits = run_feature_engineering_on_splits(raw_splits, cfg)\n    print(\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in fe_splits.items()})\n\n    print(\"3) \u6570\u636e\u7a97\u53e3\u6784\u5efa\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_splits_after_engineering(fe_splits, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {name: len(loader.dataset) for name, loader in loaders.items()}\n    print(\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1a\", dataset_sizes)\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=5e-4)\n    criterion = nn.MSELoss()\n    lambda_dir = 0.08\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, lr={cfg.learning_rate}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}, down_layers={cfg.down_sampling_layers}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, None, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    # \u5728\u9a8c\u8bc1\u96c6\u4e0a\u641c\u7d22\u6700\u4f18 blend_alpha \u5e76\u62df\u5408\u7b80\u5355\u7ebf\u6027\u6821\u51c6\u53c2\u6570\n    calib_w, calib_b = compute_log_calibration(model, loaders[\"val\"], cfg, cfg.device_obj)\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj, calibr=(calib_w, calib_b)\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n# EVOLVE-BLOCK-END\n    return test_mse, test_mae, test_mape, test_da\n\nif __name__ == \"__main__\":\n    test_mse, test_mae, test_mape, test_da = train_predict_evaluate()\n    \n\n```\nKey features: Performs well on combined_score (0.6660), Performs well on test_mse (0.0007), Performs well on test_mae (0.0196), Performs well on test_mape (0.0197), Performs well on test_da (0.5739)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.6660)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nimport os\n\n# Ensure deterministic CuBLAS workspace for CUDA deterministic algorithms\nif torch.cuda.is_available():\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\n\ndef _set_global_seed(seed: int = 2021) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        try:\n            torch.use_deterministic_algorithms(True)\n        except Exception:\n            # Older torch versions may not have this or certain ops may not support it\n            pass\n\n_set_global_seed(FIX_SEED)\n\n# Generator for DataLoader to keep shuffling deterministic\n_shared_generator = torch.Generator()\n_shared_generator.manual_seed(FIX_SEED)\n\ndef _worker_init_fn(worker_id: int) -> None:\n    # Ensure each worker has a deterministic seed derived from global seed\n    worker_seed = FIX_SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n# EVOLVE-BLOCK-START\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\ndef time_features(dates, freq: str = \"b\") -> np.ndarray:\n    \"\"\"Business-day calendar features (dow/dom/doy scaled to [-0.5, 0.5]).\"\"\"\n    dates = pd.to_datetime(dates)\n    dow = dates.dayofweek / 6.0 - 0.5\n    dom = (dates.day - 1) / 30.0 - 0.5\n    doy = (dates.dayofyear - 1) / 365.0 - 0.5\n    return np.vstack([dow, dom, doy])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# Feature fusion is disabled in this standalone script; keep a tiny placeholder.\nDEFAULT_FUSION_CONFIG: Dict[str, Any] = {}\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:  # pragma: no cover\n    raise NotImplementedError(\n        \"Feature fusion is disabled; provide cached train_raw/val_raw/test_raw CSVs instead.\"\n    )\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\n# DFTSeriesDecomp is unnecessary here because decomp_method is fixed to 'moving_avg'.\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\n# TemporalEmbedding / FixedEmbedding stubs are not needed since embed='timeF'\n# always routes through TimeFeatureEmbedding in DataEmbeddingWoPos.\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        # For this task we always use calendar time features (embed='timeF')\n        self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method != 'moving_avg':\n            raise ValueError('Unsupported decomposition method')\n        self.decomposition = SeriesDecomp(configs.moving_avg)\n        if self.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list: List[torch.Tensor] = []\n        trend_list: List[torch.Tensor] = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list: List[torch.Tensor] = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n            # Learnable weights for aggregating multi-scale predictions instead of a simple sum\n            self.scale_weights = nn.Parameter(torch.ones(configs.down_sampling_layers + 1))\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        # In this task we always set use_future_temporal_feature=0, so we skip\n        # the unused future-time gating logic and directly build multi-scale\n        # encoder inputs. This keeps the forward pass compact but is behaviour-\n        # equivalent for the current configuration.\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list: List[torch.Tensor] = []\n        x_mark_processed: List[torch.Tensor] = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list: List[torch.Tensor] = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        # Multi-scale regression + projection\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out_stack = torch.stack(dec_out_list, dim=-1)\n        # Aggregate predictions from different scales using learnable softmax weights\n        if hasattr(self, \"scale_weights\"):\n            weights = torch.softmax(self.scale_weights, dim=0)\n            dec_out = (dec_out_stack * weights.view(1, 1, 1, -1)).sum(-1)\n        else:\n            dec_out = dec_out_stack.sum(-1)\n        # Denormalise back to the original scale of encoder inputs\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        # With channel_independence fixed to 0 in this pipeline, we only need\n        # the shared multi-scale regression path, which removes unused branches\n        # and slightly reduces overhead without changing behaviour.\n        enc_inputs, out_res_list = x_list\n        dec_out_list: List[torch.Tensor] = []\n        for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n            dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n            dec_out = self.out_projection(dec_out, i, out_res)\n            dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        raise ValueError('Unsupported task name for TimeMixer')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # project_root: Path = Path(__file__).resolve().parents[0]\n    # project_root: Path = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test\\exp_iron_4_gpu\") \n    project_root: Path = Path(r\"/home/jovyan/research/kaikai/c3i/AutoMLAgent/openevolve/iron_test/exp_iron_4_gpu\") \n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    cached_split_dir: Path | None = None\n    use_cached_splits: bool = True\n    seq_len: int = 48\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 1000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # device: str = \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n    blend_alpha: float = 0.8\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.cached_split_dir is None:\n            self.cached_split_dir = self.project_root / \"data\"\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_split_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    \"\"\"Unused helper in this standalone pipeline; cached CSV splits are loaded instead.\"\"\"\n    raise NotImplementedError(\n        \"fuse_and_align_features is unused; cached train/val/test splits are loaded instead.\"\n    )\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    # unused helper; cached CSV splits are required\n    raise NotImplementedError(\"compute_split_borders is disabled; cached train/val/test splits are required.\")\n\n\ndef get_split_cache_paths(cfg: IronDailyConfig) -> Dict[str, Path]:\n    names = ['train', 'val', 'test']\n    return {name: cfg.cached_split_dir / f\"{name}_raw.csv\" for name in names}\n\n\ndef split_raw_dataframe(fused_df: pd.DataFrame, cfg: IronDailyConfig) -> Dict[str, pd.DataFrame]:\n    # unused helper; cached train/val/test splits must be provided instead\n    raise NotImplementedError(\"split_raw_dataframe is unused in this pipeline; cached splits must be provided.\")\n\n\ndef load_splits_data(\n    cfg: IronDailyConfig,\n) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path], bool]:\n    split_paths = get_split_cache_paths(cfg)\n    if cfg.use_cached_splits and all(path.exists() for path in split_paths.values()):\n        logger.info(\"Loading cached splits from %s\", cfg.cached_split_dir)\n        splits = {\n            name: pd.read_csv(path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n            for name, path in split_paths.items()\n        }\n        return splits, split_paths\n\n\ndef run_feature_engineering_on_splits(\n    raw_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Dict[str, pd.DataFrame]:\n    fe_splits: Dict[str, pd.DataFrame] = {}\n    for name, df in raw_splits.items():\n        fe_df = run_feature_engineering(df, cfg)\n        fe_splits[name] = fe_df\n    return fe_splits\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_single_split_data(\n    df: pd.DataFrame,\n    cfg: IronDailyConfig,\n    feature_cols: List[str] | None = None,\n) -> Tuple[Dict[str, np.ndarray], List[str]]:\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    if feature_cols is None:\n        feature_cols = [c for c in df.columns if c != 'date']\n    missing_cols = [c for c in feature_cols if c not in df.columns]\n    if missing_cols:\n        raise KeyError(f\"Missing expected feature columns: {missing_cols}\")\n    df = df[['date'] + feature_cols]\n    data_values = df[feature_cols].values.astype(np.float32)\n    stamp_slice = build_time_mark_array(df['date'], cfg)\n    split_entry = {\n        'data': data_values,\n        'stamp': stamp_slice.astype(np.float32),\n        'length': len(data_values),\n        'dates': df['date'].to_numpy(),\n    }\n    return split_entry, feature_cols\n\n\ndef prepare_splits_after_engineering(\n    fe_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Tuple[Dict[str, Dict[str, np.ndarray]], List[str]]:\n    split_info: Dict[str, Dict[str, np.ndarray]] = {}\n    feature_cols: List[str] | None = None\n    for name in ['train', 'val', 'test']:\n        if name not in fe_splits:\n            raise KeyError(f\"Missing split '{name}' in engineered datasets.\")\n        split_entry, feature_cols = prepare_single_split_data(fe_splits[name], cfg, feature_cols)\n        split_info[name] = split_entry\n\n    # \u6807\u51c6\u5316\u9664\u76ee\u6807\u5217\u4e4b\u5916\u7684\u7279\u5f81\uff08\u4f7f\u7528\u8bad\u7ec3\u96c6\u7edf\u8ba1\u91cf\uff09\uff0c\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    if 'train' in split_info:\n        train_data = split_info['train']['data']\n        if isinstance(train_data, np.ndarray) and train_data.ndim == 2 and train_data.shape[1] > 1:\n            num_features = train_data.shape[1]\n            feat_slice = slice(0, num_features - 1)  # \u6700\u540e\u4e00\u5217\u4e3a\u76ee\u6807y\uff0c\u4fdd\u6301\u539f\u5c3a\u5ea6\n            mean = train_data[:, feat_slice].mean(axis=0, keepdims=True)\n            std = train_data[:, feat_slice].std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            for name in ['train', 'val', 'test']:\n                data = split_info[name]['data'].astype(np.float32)\n                data[:, feat_slice] = (data[:, feat_slice] - mean) / std\n                split_info[name]['data'] = data\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=False,\n            worker_init_fn=_worker_init_fn,\n            generator=_shared_generator,\n        )\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef _collect_log_forecasts(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:\n    \"\"\"Helper that returns (preds, trues, naive) in log space.\"\"\"\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    naives: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            # \u5f53\u524d\u914d\u7f6e\u4e2d\u603b\u662f\u4f7f\u7528\u591a\u5c42\u4e0b\u91c7\u6837\uff0c\u56e0\u6b64\u89e3\u7801\u5668\u8f93\u5165\u6052\u4e3a None\n            dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n\n            # Naive baseline: repeat last observed target value over the horizon\n            if cfg.c_out == 1:\n                last_val = batch_x[:, -1:, -1:]\n            else:\n                last_val = batch_x[:, -1:, 0:1]\n            naive_y = last_val.repeat(1, cfg.pred_len, 1)\n\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n            naives.append(naive_y.cpu().numpy())\n    if not preds:\n        return None, None, None\n\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    naive_arr = np.concatenate(naives, axis=0)\n    return preds_arr, trues_arr, naive_arr\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n    calibr: Tuple[float, float] | None = None,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Evaluate model on a loader and compute error metrics.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n\n    # Blend model and naive forecasts in log space\n    alpha = getattr(cfg, \"blend_alpha\", 0.8)\n    preds_arr = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n    # Optional linear calibration in log-space: y \u2248 w * y_pred + b\n    if calibr is not None:\n        w, b = calibr\n        preds_arr = preds_arr * float(w) + float(b)\n\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(\n        np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None))\n    )\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Tune blend_alpha and affine log calibration using a proxy close to final fitness.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    def _combo_score(pred_log: np.ndarray) -> float:\n        \"\"\"Lower is better: combines log-space errors, value-space MAPE and 1-DA.\"\"\"\n        err = pred_log - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        pred_val = np.expm1(pred_log)\n        true_val = np.expm1(trues_arr)\n        mape = float(\n            np.mean(\n                np.abs(pred_val - true_val) / np.clip(true_val, 1e-6, None)\n            )\n        )\n        da = compute_directional_accuracy(pred_val, true_val)\n        if not np.isfinite(da):\n            da = 0.5\n        # \u8ba9\u6821\u51c6\u8fc7\u7a0b\u66f4\u8d34\u8fd1\u6700\u7ec8\u8bc4\u4f30\uff1a\u540c\u65f6\u8003\u8651 MSE/MAE/MAPE \u4e0e \u65b9\u5411\u4e00\u81f4\u6027\n        return mse + mae + mape + (1.0 - da)\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # \u66f4\u7ec6\u7684 alpha \u7f51\u683c\uff0c\u63d0\u9ad8\u201c\u6a21\u578b vs naive\u201d\u6df7\u5408\u6bd4\u4f8b\u9009\u62e9\u7cbe\u5ea6\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        # \u95ed\u5f0f\u89e3\u62df\u5408 log \u7a7a\u95f4\u7ebf\u6027\u6821\u51c6 y \u2248 w * blended + b\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        # \u5728\u7efc\u5408\u6307\u6807\u4e0a\u6bd4\u8f83\u201c\u5e26\u7ebf\u6027\u6821\u51c6\u201d\u548c\u201c\u6052\u7b49\u6620\u5c04\u201d\u4e24\u79cd\u65b9\u6848\uff0c\u53ea\u4fdd\u7559\u66f4\u4f18\u8005\n        calibrated = blended * float(w) + float(b)\n        score_cal = _combo_score(calibrated)\n        score_id = _combo_score(blended)\n\n        if score_id <= score_cal:\n            score = score_id\n            w, b = 1.0, 0.0\n        else:\n            score = score_cal\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n\n\ndef tune_blend_alpha(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> float:\n    # \u4fdd\u7559\u5360\u4f4d\u4ee5\u517c\u5bb9\u53ef\u80fd\u7684\u5916\u90e8\u8c03\u7528\uff0c\u5b9e\u9645 alpha \u7531 compute_log_calibration \u51b3\u5b9a\n    return float(getattr(cfg, \"blend_alpha\", 0.8))\n\n\ndef train_predict_evaluate() -> None:\n    cfg = IronDailyConfig()\n    print(\"1) \u52a0\u8f7d\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6...\")\n    raw_splits, split_paths = load_splits_data(cfg)\n    print(f\"   \u5df2\u52a0\u8f7d\u6570\u636e\uff1a{', '.join(str(p.name) for p in split_paths.values())}\")\n\n    print(\"   \u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in raw_splits.items()})\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u5bf9\u62c6\u5206\u540e\u7684\u6570\u636e\u5206\u522b\u53d8\u6362...\")\n    fe_splits = run_feature_engineering_on_splits(raw_splits, cfg)\n    print(\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in fe_splits.items()})\n\n    print(\"3) \u6570\u636e\u7a97\u53e3\u6784\u5efa\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_splits_after_engineering(fe_splits, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {name: len(loader.dataset) for name, loader in loaders.items()}\n    print(\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1a\", dataset_sizes)\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    # \u4f7f\u7528\u8f7b\u5fae\u7684\u6743\u91cd\u8870\u51cf\u63d0\u5347\u6cdb\u5316\u80fd\u529b\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=5e-4)\n    criterion = nn.MSELoss()\n    # \u7a0d\u5f31\u7684\u65b9\u5411\u6b63\u5219\uff0c\u66f4\u597d\u517c\u987eMSE/MAE\u4e0e\u65b9\u5411\u4e00\u81f4\u6027\n    lambda_dir = 0.08\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, lr={cfg.learning_rate}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}, down_layers={cfg.down_sampling_layers}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    # \u5728\u9a8c\u8bc1\u96c6\u4e0a\u8054\u5408\u641c\u7d22blend_alpha\u5e76\u62df\u5408\u7b80\u5355\u7ebf\u6027\u6821\u51c6\u53c2\u6570\n    calib_w, calib_b = compute_log_calibration(model, loaders[\"val\"], cfg, cfg.device_obj)\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj, calibr=(calib_w, calib_b)\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n# EVOLVE-BLOCK-END\n    return test_mse, test_mae, test_mape, test_da\n\nif __name__ == \"__main__\":\n    test_mse, test_mae, test_mape, test_da = train_predict_evaluate()\n    \n\n```\nKey features: Alternative approach to combined_score, Alternative approach to test_mse\n\n### Program D2 (Score: 0.6660)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nimport os\n\n# Ensure deterministic CuBLAS workspace for CUDA deterministic algorithms\nif torch.cuda.is_available():\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\n\ndef _set_global_seed(seed: int = 2021) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        try:\n            torch.use_deterministic_algorithms(True)\n        except Exception:\n            # Older torch versions may not have this or certain ops may not support it\n            pass\n\n_set_global_seed(FIX_SEED)\n\n# Generator for DataLoader to keep shuffling deterministic\n_shared_generator = torch.Generator()\n_shared_generator.manual_seed(FIX_SEED)\n\ndef _worker_init_fn(worker_id: int) -> None:\n    # Ensure each worker has a deterministic seed derived from global seed\n    worker_seed = FIX_SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n# EVOLVE-BLOCK-START\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\ndef time_features(dates, freq: str = \"b\") -> np.ndarray:\n    \"\"\"Business-day calendar features (dow/dom/doy scaled to [-0.5, 0.5]).\"\"\"\n    dates = pd.to_datetime(dates)\n    dow = dates.dayofweek / 6.0 - 0.5\n    dom = (dates.day - 1) / 30.0 - 0.5\n    doy = (dates.dayofyear - 1) / 365.0 - 0.5\n    return np.vstack([dow, dom, doy])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# Feature fusion is disabled in this standalone script; keep a tiny placeholder.\nDEFAULT_FUSION_CONFIG: Dict[str, Any] = {}\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:  # pragma: no cover\n    raise NotImplementedError(\n        \"Feature fusion is disabled; provide cached train_raw/val_raw/test_raw CSVs instead.\"\n    )\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\n# DFTSeriesDecomp is unnecessary here because decomp_method is fixed to 'moving_avg'.\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\n# TemporalEmbedding / FixedEmbedding stubs are not needed since embed='timeF'\n# always routes through TimeFeatureEmbedding in DataEmbeddingWoPos.\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        # For this task we always use calendar time features (embed='timeF')\n        self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method != 'moving_avg':\n            raise ValueError('Unsupported decomposition method')\n        self.decomposition = SeriesDecomp(configs.moving_avg)\n        if self.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list: List[torch.Tensor] = []\n        trend_list: List[torch.Tensor] = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list: List[torch.Tensor] = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n            # Learnable weights for aggregating multi-scale predictions instead of a simple sum\n            self.scale_weights = nn.Parameter(torch.ones(configs.down_sampling_layers + 1))\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        # In this task we always set use_future_temporal_feature=0, so we skip\n        # the unused future-time gating logic and directly build multi-scale\n        # encoder inputs. This keeps the forward pass compact but is behaviour-\n        # equivalent for the current configuration.\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list: List[torch.Tensor] = []\n        x_mark_processed: List[torch.Tensor] = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list: List[torch.Tensor] = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        # Multi-scale regression + projection\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out_stack = torch.stack(dec_out_list, dim=-1)\n        # Aggregate predictions from different scales using learnable softmax weights\n        if hasattr(self, \"scale_weights\"):\n            weights = torch.softmax(self.scale_weights, dim=0)\n            dec_out = (dec_out_stack * weights.view(1, 1, 1, -1)).sum(-1)\n        else:\n            dec_out = dec_out_stack.sum(-1)\n        # Denormalise back to the original scale of encoder inputs\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        # With channel_independence fixed to 0 in this pipeline, we only need\n        # the shared multi-scale regression path, which removes unused branches\n        # and slightly reduces overhead without changing behaviour.\n        enc_inputs, out_res_list = x_list\n        dec_out_list: List[torch.Tensor] = []\n        for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n            dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n            dec_out = self.out_projection(dec_out, i, out_res)\n            dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        raise ValueError('Unsupported task name for TimeMixer')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # project_root: Path = Path(__file__).resolve().parents[0]\n    # project_root: Path = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test\\exp_iron_4_gpu\") \n    project_root: Path = Path(r\"/home/jovyan/research/kaikai/c3i/AutoMLAgent/openevolve/iron_test/exp_iron_4_gpu\") \n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    cached_split_dir: Path | None = None\n    use_cached_splits: bool = True\n    seq_len: int = 48\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 1000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # device: str = \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n    blend_alpha: float = 0.8\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.cached_split_dir is None:\n            self.cached_split_dir = self.project_root / \"data\"\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_split_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    \"\"\"Unused in this standalone pipeline; cached CSV splits are loaded instead.\"\"\"\n    raise NotImplementedError(\n        \"fuse_and_align_features is unused; cached train/val/test splits are loaded instead.\"\n    )\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    # unused helper; cached CSV splits are required\n    raise NotImplementedError(\"compute_split_borders is disabled; cached train/val/test splits are required.\")\n\n\ndef get_split_cache_paths(cfg: IronDailyConfig) -> Dict[str, Path]:\n    names = ['train', 'val', 'test']\n    return {name: cfg.cached_split_dir / f\"{name}_raw.csv\" for name in names}\n\n\ndef split_raw_dataframe(fused_df: pd.DataFrame, cfg: IronDailyConfig) -> Dict[str, pd.DataFrame]:\n    # unused helper; cached train/val/test splits must be provided instead\n    raise NotImplementedError(\"split_raw_dataframe is unused in this pipeline; cached splits must be provided.\")\n\n\ndef load_splits_data(\n    cfg: IronDailyConfig,\n) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path], bool]:\n    split_paths = get_split_cache_paths(cfg)\n    if cfg.use_cached_splits and all(path.exists() for path in split_paths.values()):\n        logger.info(\"Loading cached splits from %s\", cfg.cached_split_dir)\n        splits = {\n            name: pd.read_csv(path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n            for name, path in split_paths.items()\n        }\n        return splits, split_paths\n\n\ndef run_feature_engineering_on_splits(\n    raw_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Dict[str, pd.DataFrame]:\n    fe_splits: Dict[str, pd.DataFrame] = {}\n    for name, df in raw_splits.items():\n        fe_df = run_feature_engineering(df, cfg)\n        fe_splits[name] = fe_df\n    return fe_splits\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_single_split_data(\n    df: pd.DataFrame,\n    cfg: IronDailyConfig,\n    feature_cols: List[str] | None = None,\n) -> Tuple[Dict[str, np.ndarray], List[str]]:\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    if feature_cols is None:\n        feature_cols = [c for c in df.columns if c != 'date']\n    missing_cols = [c for c in feature_cols if c not in df.columns]\n    if missing_cols:\n        raise KeyError(f\"Missing expected feature columns: {missing_cols}\")\n    df = df[['date'] + feature_cols]\n    data_values = df[feature_cols].values.astype(np.float32)\n    stamp_slice = build_time_mark_array(df['date'], cfg)\n    split_entry = {\n        'data': data_values,\n        'stamp': stamp_slice.astype(np.float32),\n        'length': len(data_values),\n        'dates': df['date'].to_numpy(),\n    }\n    return split_entry, feature_cols\n\n\ndef prepare_splits_after_engineering(\n    fe_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Tuple[Dict[str, Dict[str, np.ndarray]], List[str]]:\n    split_info: Dict[str, Dict[str, np.ndarray]] = {}\n    feature_cols: List[str] | None = None\n    for name in ['train', 'val', 'test']:\n        if name not in fe_splits:\n            raise KeyError(f\"Missing split '{name}' in engineered datasets.\")\n        split_entry, feature_cols = prepare_single_split_data(fe_splits[name], cfg, feature_cols)\n        split_info[name] = split_entry\n\n    # \u6807\u51c6\u5316\u9664\u76ee\u6807\u5217\u4e4b\u5916\u7684\u7279\u5f81\uff08\u4f7f\u7528\u8bad\u7ec3\u96c6\u7edf\u8ba1\u91cf\uff09\uff0c\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    if 'train' in split_info:\n        train_data = split_info['train']['data']\n        if isinstance(train_data, np.ndarray) and train_data.ndim == 2 and train_data.shape[1] > 1:\n            num_features = train_data.shape[1]\n            feat_slice = slice(0, num_features - 1)  # \u6700\u540e\u4e00\u5217\u4e3a\u76ee\u6807y\uff0c\u4fdd\u6301\u539f\u5c3a\u5ea6\n            mean = train_data[:, feat_slice].mean(axis=0, keepdims=True)\n            std = train_data[:, feat_slice].std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            for name in ['train', 'val', 'test']:\n                data = split_info[name]['data'].astype(np.float32)\n                data[:, feat_slice] = (data[:, feat_slice] - mean) / std\n                split_info[name]['data'] = data\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=False,\n            worker_init_fn=_worker_init_fn,\n            generator=_shared_generator,\n        )\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef _collect_log_forecasts(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:\n    \"\"\"Helper that returns (preds, trues, naive) in log space.\"\"\"\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    naives: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            # \u5f53\u524d\u914d\u7f6e\u4e2d\u603b\u662f\u4f7f\u7528\u591a\u5c42\u4e0b\u91c7\u6837\uff0c\u56e0\u6b64\u89e3\u7801\u5668\u8f93\u5165\u6052\u4e3a None\n            dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n\n            # Naive baseline: repeat last observed target value over the horizon\n            if cfg.c_out == 1:\n                last_val = batch_x[:, -1:, -1:]\n            else:\n                last_val = batch_x[:, -1:, 0:1]\n            naive_y = last_val.repeat(1, cfg.pred_len, 1)\n\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n            naives.append(naive_y.cpu().numpy())\n    if not preds:\n        return None, None, None\n\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    naive_arr = np.concatenate(naives, axis=0)\n    return preds_arr, trues_arr, naive_arr\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n    calibr: Tuple[float, float] | None = None,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Evaluate model on a loader and compute error metrics.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n\n    # Blend model and naive forecasts in log space\n    alpha = getattr(cfg, \"blend_alpha\", 0.8)\n    preds_arr = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n    # Optional linear calibration in log-space: y \u2248 w * y_pred + b\n    if calibr is not None:\n        w, b = calibr\n        preds_arr = preds_arr * float(w) + float(b)\n\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(\n        np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None))\n    )\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Tune blend_alpha and affine log calibration using a proxy close to final fitness.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    def _combo_score(pred_log: np.ndarray) -> float:\n        \"\"\"Lower is better: combines log/linear errors and directional accuracy.\"\"\"\n        err = pred_log - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        pred_val = np.expm1(pred_log)\n        true_val = np.expm1(trues_arr)\n        mape = float(\n            np.mean(\n                np.abs(pred_val - true_val) / np.clip(true_val, 1e-6, None)\n            )\n        )\n        da = compute_directional_accuracy(pred_val, true_val)\n        if not np.isfinite(da):\n            da = 0.5\n        return mse + mae + mape + (1.0 - da)\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # \u5728 [0,1] \u533a\u95f4\u4e0a\u4f7f\u7528 0.05 \u6b65\u957f\u641c\u7d22 alpha\uff0c\u5e76\u540c\u65f6\u6bd4\u8f83\u201c\u5e26\u7ebf\u6027\u6821\u51c6\u201d\u548c\u201c\u6052\u7b49\u6620\u5c04\u201d\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        # \u95ed\u5f0f\u89e3\u62df\u5408 log \u7a7a\u95f4\u7ebf\u6027\u6821\u51c6 y \u2248 w * blended + b\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        calibrated = blended * float(w) + float(b)\n        score_cal = _combo_score(calibrated)\n        score_id = _combo_score(blended)\n\n        if score_id <= score_cal:\n            score = score_id\n            w, b = 1.0, 0.0\n        else:\n            score = score_cal\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n\n\ndef tune_blend_alpha(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> float:\n    # Deprecated: blend_alpha is now tuned inside compute_log_calibration.\n    return float(getattr(cfg, \"blend_alpha\", 0.8))\n\n\ndef train_predict_evaluate() -> None:\n    cfg = IronDailyConfig()\n    print(\"1) \u52a0\u8f7d\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6...\")\n    raw_splits, split_paths = load_splits_data(cfg)\n    print(f\"   \u5df2\u52a0\u8f7d\u6570\u636e\uff1a{', '.join(str(p.name) for p in split_paths.values())}\")\n\n    print(\"   \u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in raw_splits.items()})\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u5bf9\u62c6\u5206\u540e\u7684\u6570\u636e\u5206\u522b\u53d8\u6362...\")\n    fe_splits = run_feature_engineering_on_splits(raw_splits, cfg)\n    print(\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in fe_splits.items()})\n\n    print(\"3) \u6570\u636e\u7a97\u53e3\u6784\u5efa\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_splits_after_engineering(fe_splits, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {name: len(loader.dataset) for name, loader in loaders.items()}\n    print(\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1a\", dataset_sizes)\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    # \u4f7f\u7528\u8f7b\u5fae\u7684\u6743\u91cd\u8870\u51cf\u63d0\u5347\u6cdb\u5316\u80fd\u529b\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=5e-4)\n    criterion = nn.MSELoss()\n    # \u7a0d\u5f31\u7684\u65b9\u5411\u6b63\u5219\uff0c\u66f4\u597d\u517c\u987eMSE/MAE\u4e0e\u65b9\u5411\u4e00\u81f4\u6027\n    lambda_dir = 0.08\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, lr={cfg.learning_rate}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}, down_layers={cfg.down_sampling_layers}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, val_mae, val_mape, val_da = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        da_term = val_da if math.isfinite(val_da) else 0.5\n        val_score = val_mse + val_mae + val_mape + (1.0 - da_term)\n        print(\n            f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, \"\n            f\"val_mse={val_mse:.4f}, val_score={val_score:.4f}\"\n        )\n        if val_score < best_val:\n            best_val = val_score\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation score %.6f at epoch %d\", val_score, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    # \u5728\u9a8c\u8bc1\u96c6\u4e0a\u8054\u5408\u641c\u7d22blend_alpha\u5e76\u62df\u5408\u7b80\u5355\u7ebf\u6027\u6821\u51c6\u53c2\u6570\n    calib_w, calib_b = compute_log_calibration(model, loaders[\"val\"], cfg, cfg.device_obj)\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj, calibr=(calib_w, calib_b)\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n# EVOLVE-BLOCK-END\n    return test_mse, test_mae, test_mape, test_da\n\nif __name__ == \"__main__\":\n    test_mse, test_mae, test_mape, test_da = train_predict_evaluate()\n    \n\n```\nKey features: Alternative approach to combined_score, Alternative approach to test_mse\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.6660, Type: Alternative)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nimport os\n\n# Ensure deterministic CuBLAS workspace for CUDA deterministic algorithms\nif torch.cuda.is_available():\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\n\ndef _set_global_seed(seed: int = 2021) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        try:\n            torch.use_deterministic_algorithms(True)\n        except Exception:\n            # Older torch versions may not have this or certain ops may not support it\n            pass\n\n_set_global_seed(FIX_SEED)\n\n# Generator for DataLoader to keep shuffling deterministic\n_shared_generator = torch.Generator()\n_shared_generator.manual_seed(FIX_SEED)\n\ndef _worker_init_fn(worker_id: int) -> None:\n    # Ensure each worker has a deterministic seed derived from global seed\n    worker_seed = FIX_SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n# EVOLVE-BLOCK-START\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\ndef time_features(dates, freq: str = \"b\") -> np.ndarray:\n    \"\"\"Business-day calendar features (dow/dom/doy scaled to [-0.5, 0.5]).\"\"\"\n    dates = pd.to_datetime(dates)\n    dow = dates.dayofweek / 6.0 - 0.5\n    dom = (dates.day - 1) / 30.0 - 0.5\n    doy = (dates.dayofyear - 1) / 365.0 - 0.5\n    return np.vstack([dow, dom, doy])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# Feature fusion is disabled in this standalone script; keep a tiny placeholder.\nDEFAULT_FUSION_CONFIG: Dict[str, Any] = {}\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:  # pragma: no cover\n    raise NotImplementedError(\n        \"Feature fusion is disabled; provide cached train_raw/val_raw/test_raw CSVs instead.\"\n    )\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\n# DFTSeriesDecomp is unnecessary here because decomp_method is fixed to 'moving_avg'.\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\n# TemporalEmbedding / FixedEmbedding stubs are not needed since embed='timeF'\n# always routes through TimeFeatureEmbedding in DataEmbeddingWoPos.\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        # For this task we always use calendar time features (embed='timeF')\n        self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method != 'moving_avg':\n            raise ValueError('Unsupported decomposition method')\n        self.decomposition = SeriesDecomp(configs.moving_avg)\n        if self.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list: List[torch.Tensor] = []\n        trend_list: List[torch.Tensor] = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list: List[torch.Tensor] = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n            # Learnable weights for aggregating multi-scale predictions instead of a simple sum\n            self.scale_weights = nn.Parameter(torch.ones(configs.down_sampling_layers + 1))\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        # In this task we always set use_future_temporal_feature=0, so we skip\n        # the unused future-time gating logic and directly build multi-scale\n        # encoder inputs. This keeps the forward pass compact but is behaviour-\n        # equivalent for the current configuration.\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list: List[torch.Tensor] = []\n        x_mark_processed: List[torch.Tensor] = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list: List[torch.Tensor] = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        # Multi-scale regression + projection\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out_stack = torch.stack(dec_out_list, dim=-1)\n        # Aggregate predictions from different scales using learnable softmax weights\n        if hasattr(self, \"scale_weights\"):\n            weights = torch.softmax(self.scale_weights, dim=0)\n            dec_out = (dec_out_stack * weights.view(1, 1, 1, -1)).sum(-1)\n        else:\n            dec_out = dec_out_stack.sum(-1)\n        # Denormalise back to the original scale of encoder inputs\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        # With channel_independence fixed to 0 in this pipeline, we only need\n        # the shared multi-scale regression path, which removes unused branches\n        # and slightly reduces overhead without changing behaviour.\n        enc_inputs, out_res_list = x_list\n        dec_out_list: List[torch.Tensor] = []\n        for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n            dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n            dec_out = self.out_projection(dec_out, i, out_res)\n            dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        raise ValueError('Unsupported task name for TimeMixer')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # project_root: Path = Path(__file__).resolve().parents[0]\n    # project_root: Path = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test\\exp_iron_4_gpu\") \n    project_root: Path = Path(r\"/home/jovyan/research/kaikai/c3i/AutoMLAgent/openevolve/iron_test/exp_iron_4_gpu\") \n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    cached_split_dir: Path | None = None\n    use_cached_splits: bool = True\n    seq_len: int = 48\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 1000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # device: str = \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n    blend_alpha: float = 0.8\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.cached_split_dir is None:\n            self.cached_split_dir = self.project_root / \"data\"\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_split_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    \"\"\"Unused helper in this standalone pipeline; cached CSV splits are loaded instead.\"\"\"\n    raise NotImplementedError(\n        \"fuse_and_align_features is unused; cached train/val/test splits are loaded instead.\"\n    )\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    # unused helper; cached CSV splits are required\n    raise NotImplementedError(\"compute_split_borders is disabled; cached train/val/test splits are required.\")\n\n\ndef get_split_cache_paths(cfg: IronDailyConfig) -> Dict[str, Path]:\n    names = ['train', 'val', 'test']\n    return {name: cfg.cached_split_dir / f\"{name}_raw.csv\" for name in names}\n\n\ndef split_raw_dataframe(fused_df: pd.DataFrame, cfg: IronDailyConfig) -> Dict[str, pd.DataFrame]:\n    # unused helper; cached train/val/test splits must be provided instead\n    raise NotImplementedError(\"split_raw_dataframe is unused in this pipeline; cached splits must be provided.\")\n\n\ndef load_splits_data(\n    cfg: IronDailyConfig,\n) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path], bool]:\n    split_paths = get_split_cache_paths(cfg)\n    if cfg.use_cached_splits and all(path.exists() for path in split_paths.values()):\n        logger.info(\"Loading cached splits from %s\", cfg.cached_split_dir)\n        splits = {\n            name: pd.read_csv(path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n            for name, path in split_paths.items()\n        }\n        return splits, split_paths\n\n\ndef run_feature_engineering_on_splits(\n    raw_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Dict[str, pd.DataFrame]:\n    fe_splits: Dict[str, pd.DataFrame] = {}\n    for name, df in raw_splits.items():\n        fe_df = run_feature_engineering(df, cfg)\n        fe_splits[name] = fe_df\n    return fe_splits\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_single_split_data(\n    df: pd.DataFrame,\n    cfg: IronDailyConfig,\n    feature_cols: List[str] | None = None,\n) -> Tuple[Dict[str, np.ndarray], List[str]]:\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    if feature_cols is None:\n        feature_cols = [c for c in df.columns if c != 'date']\n    missing_cols = [c for c in feature_cols if c not in df.columns]\n    if missing_cols:\n        raise KeyError(f\"Missing expected feature columns: {missing_cols}\")\n    df = df[['date'] + feature_cols]\n    data_values = df[feature_cols].values.astype(np.float32)\n    stamp_slice = build_time_mark_array(df['date'], cfg)\n    split_entry = {\n        'data': data_values,\n        'stamp': stamp_slice.astype(np.float32),\n        'length': len(data_values),\n        'dates': df['date'].to_numpy(),\n    }\n    return split_entry, feature_cols\n\n\ndef prepare_splits_after_engineering(\n    fe_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Tuple[Dict[str, Dict[str, np.ndarray]], List[str]]:\n    split_info: Dict[str, Dict[str, np.ndarray]] = {}\n    feature_cols: List[str] | None = None\n    for name in ['train', 'val', 'test']:\n        if name not in fe_splits:\n            raise KeyError(f\"Missing split '{name}' in engineered datasets.\")\n        split_entry, feature_cols = prepare_single_split_data(fe_splits[name], cfg, feature_cols)\n        split_info[name] = split_entry\n\n    # \u6807\u51c6\u5316\u9664\u76ee\u6807\u5217\u4e4b\u5916\u7684\u7279\u5f81\uff08\u4f7f\u7528\u8bad\u7ec3\u96c6\u7edf\u8ba1\u91cf\uff09\uff0c\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    if 'train' in split_info:\n        train_data = split_info['train']['data']\n        if isinstance(train_data, np.ndarray) and train_data.ndim == 2 and train_data.shape[1] > 1:\n            num_features = train_data.shape[1]\n            feat_slice = slice(0, num_features - 1)  # \u6700\u540e\u4e00\u5217\u4e3a\u76ee\u6807y\uff0c\u4fdd\u6301\u539f\u5c3a\u5ea6\n            mean = train_data[:, feat_slice].mean(axis=0, keepdims=True)\n            std = train_data[:, feat_slice].std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            for name in ['train', 'val', 'test']:\n                data = split_info[name]['data'].astype(np.float32)\n                data[:, feat_slice] = (data[:, feat_slice] - mean) / std\n                split_info[name]['data'] = data\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=False,\n            worker_init_fn=_worker_init_fn,\n            generator=_shared_generator,\n        )\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef _collect_log_forecasts(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:\n    \"\"\"Helper that returns (preds, trues, naive) in log space.\"\"\"\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    naives: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            # \u5f53\u524d\u914d\u7f6e\u4e2d\u603b\u662f\u4f7f\u7528\u591a\u5c42\u4e0b\u91c7\u6837\uff0c\u56e0\u6b64\u89e3\u7801\u5668\u8f93\u5165\u6052\u4e3a None\n            dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n\n            # Naive baseline: repeat last observed target value over the horizon\n            if cfg.c_out == 1:\n                last_val = batch_x[:, -1:, -1:]\n            else:\n                last_val = batch_x[:, -1:, 0:1]\n            naive_y = last_val.repeat(1, cfg.pred_len, 1)\n\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n            naives.append(naive_y.cpu().numpy())\n    if not preds:\n        return None, None, None\n\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    naive_arr = np.concatenate(naives, axis=0)\n    return preds_arr, trues_arr, naive_arr\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n    calibr: Tuple[float, float] | None = None,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Evaluate model on a loader and compute error metrics.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n\n    # Blend model and naive forecasts in log space\n    alpha = getattr(cfg, \"blend_alpha\", 0.8)\n    preds_arr = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n    # Optional linear calibration in log-space: y \u2248 w * y_pred + b\n    if calibr is not None:\n        w, b = calibr\n        preds_arr = preds_arr * float(w) + float(b)\n\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(\n        np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None))\n    )\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Tune blend_alpha and affine log calibration using a proxy close to final fitness.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    def _combo_score(pred_log: np.ndarray) -> float:\n        \"\"\"Lower is better: combines log-space errors, value-space MAPE and 1-DA.\"\"\"\n        err = pred_log - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        pred_val = np.expm1(pred_log)\n        true_val = np.expm1(trues_arr)\n        mape = float(\n            np.mean(\n                np.abs(pred_val - true_val) / np.clip(true_val, 1e-6, None)\n            )\n        )\n        da = compute_directional_accuracy(pred_val, true_val)\n        if not np.isfinite(da):\n            da = 0.5\n        # \u8ba9\u6821\u51c6\u8fc7\u7a0b\u66f4\u8d34\u8fd1\u6700\u7ec8\u8bc4\u4f30\uff1a\u540c\u65f6\u8003\u8651 MSE/MAE/MAPE \u4e0e \u65b9\u5411\u4e00\u81f4\u6027\n        return mse + mae + mape + (1.0 - da)\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # \u66f4\u7ec6\u7684 alpha \u7f51\u683c\uff0c\u63d0\u9ad8\u201c\u6a21\u578b vs naive\u201d\u6df7\u5408\u6bd4\u4f8b\u9009\u62e9\u7cbe\u5ea6\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        # \u95ed\u5f0f\u89e3\u62df\u5408 log \u7a7a\u95f4\u7ebf\u6027\u6821\u51c6 y \u2248 w * blended + b\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        # \u5728\u7efc\u5408\u6307\u6807\u4e0a\u6bd4\u8f83\u201c\u5e26\u7ebf\u6027\u6821\u51c6\u201d\u548c\u201c\u6052\u7b49\u6620\u5c04\u201d\u4e24\u79cd\u65b9\u6848\uff0c\u53ea\u4fdd\u7559\u66f4\u4f18\u8005\n        calibrated = blended * float(w) + float(b)\n        score_cal = _combo_score(calibrated)\n        score_id = _combo_score(blended)\n\n        if score_id <= score_cal:\n            score = score_id\n            w, b = 1.0, 0.0\n        else:\n            score = score_cal\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n\n\ndef tune_blend_alpha(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> float:\n    # \u4fdd\u7559\u5360\u4f4d\u4ee5\u517c\u5bb9\u53ef\u80fd\u7684\u5916\u90e8\u8c03\u7528\uff0c\u5b9e\u9645 alpha \u7531 compute_log_calibration \u51b3\u5b9a\n    return float(getattr(cfg, \"blend_alpha\", 0.8))\n\n\ndef train_predict_evaluate() -> None:\n    cfg = IronDailyConfig()\n    print(\"1) \u52a0\u8f7d\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6...\")\n    raw_splits, split_paths = load_splits_data(cfg)\n    print(f\"   \u5df2\u52a0\u8f7d\u6570\u636e\uff1a{', '.join(str(p.name) for p in split_paths.values())}\")\n\n    print(\"   \u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in raw_splits.items()})\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u5bf9\u62c6\u5206\u540e\u7684\u6570\u636e\u5206\u522b\u53d8\u6362...\")\n    fe_splits = run_feature_engineering_on_splits(raw_splits, cfg)\n    print(\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in fe_splits.items()})\n\n    print(\"3) \u6570\u636e\u7a97\u53e3\u6784\u5efa\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_splits_after_engineering(fe_splits, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {name: len(loader.dataset) for name, loader in loaders.items()}\n    print(\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1a\", dataset_sizes)\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    # \u4f7f\u7528\u8f7b\u5fae\u7684\u6743\u91cd\u8870\u51cf\u63d0\u5347\u6cdb\u5316\u80fd\u529b\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=5e-4)\n    criterion = nn.MSELoss()\n    # \u7a0d\u5f31\u7684\u65b9\u5411\u6b63\u5219\uff0c\u66f4\u597d\u517c\u987eMSE/MAE\u4e0e\u65b9\u5411\u4e00\u81f4\u6027\n    lambda_dir = 0.08\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, lr={cfg.learning_rate}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}, down_layers={cfg.down_sampling_layers}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    # \u5728\u9a8c\u8bc1\u96c6\u4e0a\u8054\u5408\u641c\u7d22blend_alpha\u5e76\u62df\u5408\u7b80\u5355\u7ebf\u6027\u6821\u51c6\u53c2\u6570\n    calib_w, calib_b = compute_log_calibration(model, loaders[\"val\"], cfg, cfg.device_obj)\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj, calibr=(calib_w, calib_b)\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n# EVOLVE-BLOCK-END\n    return test_mse, test_mae, test_mape, test_da\n\nif __name__ == \"__main__\":\n    test_mse, test_mae, test_mape, test_da = train_predict_evaluate()\n    \n\n```\nUnique approach: Modification: Change 1: Replace 61 lines with 74 lines\nChange 2: Replace 8 lines with 8 lines, Alternative test_mse approach, Alternative test_mae approach\n\n### Inspiration 2 (Score: 0.6660, Type: Alternative)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nimport os\n\n# Ensure deterministic CuBLAS workspace for CUDA deterministic algorithms\nif torch.cuda.is_available():\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\n\ndef _set_global_seed(seed: int = 2021) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        try:\n            torch.use_deterministic_algorithms(True)\n        except Exception:\n            # Older torch versions may not have this or certain ops may not support it\n            pass\n\n_set_global_seed(FIX_SEED)\n\n# Generator for DataLoader to keep shuffling deterministic\n_shared_generator = torch.Generator()\n_shared_generator.manual_seed(FIX_SEED)\n\ndef _worker_init_fn(worker_id: int) -> None:\n    # Ensure each worker has a deterministic seed derived from global seed\n    worker_seed = FIX_SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n# EVOLVE-BLOCK-START\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\ndef time_features(dates, freq: str = \"b\") -> np.ndarray:\n    \"\"\"Business-day calendar features (dow/dom/doy scaled to [-0.5, 0.5]).\"\"\"\n    dates = pd.to_datetime(dates)\n    dow = dates.dayofweek / 6.0 - 0.5\n    dom = (dates.day - 1) / 30.0 - 0.5\n    doy = (dates.dayofyear - 1) / 365.0 - 0.5\n    return np.vstack([dow, dom, doy])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# -----------------------------------------------------------------------------\n# Feature fusion helpers (minimal stub, real fusion not used in this script)\n# -----------------------------------------------------------------------------\nDEFAULT_FUSION_CONFIG: Dict[str, Any] = {}\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    raise NotImplementedError(\"Feature fusion is disabled; use cached train/val/test CSV splits.\")\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\n# DFTSeriesDecomp is unnecessary here because decomp_method is fixed to 'moving_avg'.\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\n# TemporalEmbedding / FixedEmbedding stubs are not needed since embed='timeF'\n# always routes through TimeFeatureEmbedding in DataEmbeddingWoPos.\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        # For this task we always use calendar time features (embed='timeF')\n        self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        # Keep only what is actually used in forward to simplify the block.\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method != 'moving_avg':\n            raise ValueError('Unsupported decomposition method')\n        self.decomposition = SeriesDecomp(configs.moving_avg)\n        if configs.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list, trend_list = [], []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list: List[torch.Tensor] = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList(\n            [PastDecomposableMixing(configs) for _ in range(configs.e_layers)]\n        )\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        # dir_adjust_scale is kept for compatibility with original configs but\n        # we do not use additional future-temporal gates, keeping the model lean.\n        self.dir_adjust_scale = getattr(configs, \"dir_adjust_scale\", 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(\n                1, configs.d_model, configs.embed, configs.freq, configs.dropout\n            )\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(\n                configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout\n            )\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList(\n            [\n                Normalize(\n                    configs.enc_in,\n                    affine=True,\n                    non_norm=True if configs.use_norm == 0 else False,\n                )\n                for _ in range(configs.down_sampling_layers + 1)\n            ]\n        )\n        if self.task_name in [\"long_term_forecast\", \"short_term_forecast\"]:\n            self.predict_layers = nn.ModuleList(\n                [\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ]\n            )\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(\n                    configs.d_model, configs.c_out, bias=True\n                )\n                self.out_res_layers = nn.ModuleList(\n                    [\n                        nn.Linear(\n                            configs.seq_len // (configs.down_sampling_window ** i),\n                            configs.seq_len // (configs.down_sampling_window ** i),\n                        )\n                        for i in range(configs.down_sampling_layers + 1)\n                    ]\n                )\n                self.regression_layers = nn.ModuleList(\n                    [\n                        nn.Linear(\n                            configs.seq_len // (configs.down_sampling_window ** i),\n                            configs.pred_len,\n                        )\n                        for i in range(configs.down_sampling_layers + 1)\n                    ]\n                )\n            # Learnable weights for aggregating multi-scale predictions\n            self.scale_weights = nn.Parameter(\n                torch.ones(configs.down_sampling_layers + 1)\n            )\n        else:\n            raise ValueError(\"Unsupported task name\")\n\n    def out_projection(\n        self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor\n    ) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(\n        self, x_list: List[torch.Tensor]\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == \"max\":\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == \"avg\":\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == \"conv\":\n            padding = 1 if torch.__version__ >= \"1.5.0\" else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError(\"Unknown down sampling method\")\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(\n                    x_mark_enc_mark_ori[:, :: self.configs.down_sampling_window, :]\n                )\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[\n                    :, :: self.configs.down_sampling_window, :\n                ]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        # We do not use future temporal features in this task, so decoding relies\n        # purely on past information aggregated at multiple time scales.\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list: List[torch.Tensor] = []\n        x_mark_processed: List[torch.Tensor] = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(\n                x_enc_list, x_mark_list, self.normalize_layers\n            ):\n                x = norm_layer(x, \"norm\")\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, \"norm\")\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n\n        enc_out_list: List[torch.Tensor] = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out_stack = torch.stack(dec_out_list, dim=-1)\n        if hasattr(self, \"scale_weights\"):\n            weights = torch.softmax(self.scale_weights, dim=0)\n            dec_out = (dec_out_stack * weights.view(1, 1, 1, -1)).sum(-1)\n        else:\n            dec_out = dec_out_stack.sum(-1)\n        dec_out = self.normalize_layers[0](dec_out, \"denorm\")\n        return dec_out\n\n    def future_multi_mixing(\n        self, B: int, enc_out_list: List[torch.Tensor], x_list\n    ) -> List[torch.Tensor]:\n        # The pipeline always uses channel_independence=0, so we only need the\n        # shared multi-scale regression path.\n        enc_inputs, out_res_list = x_list\n        dec_out_list: List[torch.Tensor] = []\n        for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n            dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(\n                0, 2, 1\n            )\n            dec_out = self.out_projection(dec_out, i, out_res)\n            dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in [\"long_term_forecast\", \"short_term_forecast\"]:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        raise ValueError(\"Unsupported task name for TimeMixer\")\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # project_root: Path = Path(__file__).resolve().parents[0]\n    # project_root: Path = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test\\exp_iron_4_gpu\") \n    project_root: Path = Path(r\"/home/jovyan/research/kaikai/c3i/AutoMLAgent/openevolve/iron_test/exp_iron_4_gpu\") \n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    cached_split_dir: Path | None = None\n    use_cached_splits: bool = True\n    seq_len: int = 48\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 1000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # device: str = \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n    blend_alpha: float = 0.8\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.cached_split_dir is None:\n            self.cached_split_dir = self.project_root / \"data\"\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_split_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    \"\"\"Unused helper in this standalone pipeline; cached CSV splits are loaded instead.\"\"\"\n    raise NotImplementedError(\n        \"fuse_and_align_features is unused; cached train/val/test CSV splits are loaded instead.\"\n    )\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    # unused helper; cached CSV splits are required\n    raise NotImplementedError(\"compute_split_borders is disabled; cached train/val/test splits are required.\")\n\n\ndef get_split_cache_paths(cfg: IronDailyConfig) -> Dict[str, Path]:\n    names = ['train', 'val', 'test']\n    return {name: cfg.cached_split_dir / f\"{name}_raw.csv\" for name in names}\n\n\ndef split_raw_dataframe(fused_df: pd.DataFrame, cfg: IronDailyConfig) -> Dict[str, pd.DataFrame]:\n    # unused helper; cached train/val/test splits must be provided instead\n    raise NotImplementedError(\"split_raw_dataframe is unused in this pipeline; cached splits must be provided.\")\n\n\ndef load_splits_data(\n    cfg: IronDailyConfig,\n) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path], bool]:\n    split_paths = get_split_cache_paths(cfg)\n    if cfg.use_cached_splits and all(path.exists() for path in split_paths.values()):\n        logger.info(\"Loading cached splits from %s\", cfg.cached_split_dir)\n        splits = {\n            name: pd.read_csv(path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n            for name, path in split_paths.items()\n        }\n        return splits, split_paths\n\n\ndef run_feature_engineering_on_splits(\n    raw_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Dict[str, pd.DataFrame]:\n    fe_splits: Dict[str, pd.DataFrame] = {}\n    for name, df in raw_splits.items():\n        fe_df = run_feature_engineering(df, cfg)\n        fe_splits[name] = fe_df\n    return fe_splits\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_single_split_data(\n    df: pd.DataFrame,\n    cfg: IronDailyConfig,\n    feature_cols: List[str] | None = None,\n) -> Tuple[Dict[str, np.ndarray], List[str]]:\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    if feature_cols is None:\n        feature_cols = [c for c in df.columns if c != 'date']\n    missing_cols = [c for c in feature_cols if c not in df.columns]\n    if missing_cols:\n        raise KeyError(f\"Missing expected feature columns: {missing_cols}\")\n    df = df[['date'] + feature_cols]\n    data_values = df[feature_cols].values.astype(np.float32)\n    stamp_slice = build_time_mark_array(df['date'], cfg)\n    split_entry = {\n        'data': data_values,\n        'stamp': stamp_slice.astype(np.float32),\n        'length': len(data_values),\n        'dates': df['date'].to_numpy(),\n    }\n    return split_entry, feature_cols\n\n\ndef prepare_splits_after_engineering(\n    fe_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Tuple[Dict[str, Dict[str, np.ndarray]], List[str]]:\n    split_info: Dict[str, Dict[str, np.ndarray]] = {}\n    feature_cols: List[str] | None = None\n    for name in ['train', 'val', 'test']:\n        if name not in fe_splits:\n            raise KeyError(f\"Missing split '{name}' in engineered datasets.\")\n        split_entry, feature_cols = prepare_single_split_data(fe_splits[name], cfg, feature_cols)\n        split_info[name] = split_entry\n\n    # \u6807\u51c6\u5316\u9664\u76ee\u6807\u5217\u4e4b\u5916\u7684\u7279\u5f81\uff08\u4f7f\u7528\u8bad\u7ec3\u96c6\u7edf\u8ba1\u91cf\uff09\uff0c\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    if 'train' in split_info:\n        train_data = split_info['train']['data']\n        if isinstance(train_data, np.ndarray) and train_data.ndim == 2 and train_data.shape[1] > 1:\n            num_features = train_data.shape[1]\n            feat_slice = slice(0, num_features - 1)  # \u6700\u540e\u4e00\u5217\u4e3a\u76ee\u6807y\uff0c\u4fdd\u6301\u539f\u5c3a\u5ea6\n            mean = train_data[:, feat_slice].mean(axis=0, keepdims=True)\n            std = train_data[:, feat_slice].std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            for name in ['train', 'val', 'test']:\n                data = split_info[name]['data'].astype(np.float32)\n                data[:, feat_slice] = (data[:, feat_slice] - mean) / std\n                split_info[name]['data'] = data\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=False,\n            worker_init_fn=_worker_init_fn,\n            generator=_shared_generator,\n        )\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef _collect_log_forecasts(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:\n    \"\"\"Run model on a loader and return (preds, trues, naive) in log space.\"\"\"\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    naives: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            # \u5f53\u524d\u914d\u7f6e\u4e2d\u603b\u662f\u4f7f\u7528\u591a\u5c42\u4e0b\u91c7\u6837\uff0c\u56e0\u6b64\u89e3\u7801\u5668\u8f93\u5165\u6052\u4e3a None\n            dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n\n            # Naive baseline: repeat last observed target value over the horizon\n            if cfg.c_out == 1:\n                last_val = batch_x[:, -1:, -1:]\n            else:\n                last_val = batch_x[:, -1:, 0:1]\n            naive_y = last_val.repeat(1, cfg.pred_len, 1)\n\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n            naives.append(naive_y.cpu().numpy())\n    if not preds:\n        return None, None, None\n\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    naive_arr = np.concatenate(naives, axis=0)\n    return preds_arr, trues_arr, naive_arr\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n    calibr: Tuple[float, float] | None = None,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Evaluate model; optionally apply affine calibration in log-space.\n\n    Additionally blend the model forecast with a simple naive \"last value\"\n    baseline in log space, which empirically stabilises errors on this task.\n    \"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n\n    # Blend model and naive forecasts in log space\n    alpha = getattr(cfg, \"blend_alpha\", 0.8)\n    preds_arr = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n    # Optional linear calibration in log-space: y \u2248 w * y_pred + b\n    if calibr is not None:\n        w, b = calibr\n        preds_arr = preds_arr * float(w) + float(b)\n\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(\n        np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None))\n    )\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Grid-search blend_alpha and affine log-space calibration on validation data.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # Finer grid over [0, 1] for the blend weight between model and naive\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        # Closed-form linear calibration y \u2248 w * blended + b in log space\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        # Compare calibrated vs identity mapping using (MSE + MAE) in log space\n        calibrated = blended * float(w) + float(b)\n        err = calibrated - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        score = mse + mae\n\n        err_id = blended - trues_arr\n        mse_id = float((err_id ** 2).mean())\n        mae_id = float(np.abs(err_id).mean())\n        score_id = mse_id + mae_id\n        if score_id < score:\n            score = score_id\n            w, b = 1.0, 0.0\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n\n\ndef tune_blend_alpha(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> float:\n    return float(getattr(cfg, \"blend_alpha\", 0.8))\n\n\ndef train_predict_evaluate() -> None:\n    cfg = IronDailyConfig()\n    print(\"1) \u52a0\u8f7d\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6...\")\n    raw_splits, split_paths = load_splits_data(cfg)\n    print(f\"   \u5df2\u52a0\u8f7d\u6570\u636e\uff1a{', '.join(str(p.name) for p in split_paths.values())}\")\n\n    print(\"   \u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in raw_splits.items()})\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u5bf9\u62c6\u5206\u540e\u7684\u6570\u636e\u5206\u522b\u53d8\u6362...\")\n    fe_splits = run_feature_engineering_on_splits(raw_splits, cfg)\n    print(\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in fe_splits.items()})\n\n    print(\"3) \u6570\u636e\u7a97\u53e3\u6784\u5efa\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_splits_after_engineering(fe_splits, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {name: len(loader.dataset) for name, loader in loaders.items()}\n    print(\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1a\", dataset_sizes)\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    # \u4f7f\u7528\u8f7b\u5fae\u7684\u6743\u91cd\u8870\u51cf\u63d0\u5347\u6cdb\u5316\u80fd\u529b\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=5e-4)\n    criterion = nn.MSELoss()\n    # \u7a0d\u5f31\u7684\u65b9\u5411\u6b63\u5219\uff0c\u66f4\u597d\u517c\u987eMSE/MAE\u4e0e\u65b9\u5411\u4e00\u81f4\u6027\n    lambda_dir = 0.08\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, lr={cfg.learning_rate}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}, down_layers={cfg.down_sampling_layers}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    # \u5728\u9a8c\u8bc1\u96c6\u4e0a\u540c\u65f6\u641c\u7d22\u6700\u4f18blend_alpha\u5e76\u62df\u5408\u7b80\u5355\u7ebf\u6027\u6821\u51c6\u53c2\u6570\n    calib_w, calib_b = compute_log_calibration(model, loaders[\"val\"], cfg, cfg.device_obj)\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj, calibr=(calib_w, calib_b)\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n# EVOLVE-BLOCK-END\n    return test_mse, test_mae, test_mape, test_da\n\nif __name__ == \"__main__\":\n    test_mse, test_mae, test_mape, test_da = train_predict_evaluate()\n    \n\n```\nUnique approach: Modification: Change 1: Replace 76 lines with 60 lines, Alternative test_mse approach, Alternative test_mae approach\n\n### Inspiration 3 (Score: 0.6660, Type: Alternative)\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nimport os\n\n# Ensure deterministic CuBLAS workspace for CUDA deterministic algorithms\nif torch.cuda.is_available():\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\n\ndef _set_global_seed(seed: int = 2021) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        try:\n            torch.use_deterministic_algorithms(True)\n        except Exception:\n            # Older torch versions may not have this or certain ops may not support it\n            pass\n\n_set_global_seed(FIX_SEED)\n\n# Generator for DataLoader to keep shuffling deterministic\n_shared_generator = torch.Generator()\n_shared_generator.manual_seed(FIX_SEED)\n\ndef _worker_init_fn(worker_id: int) -> None:\n    # Ensure each worker has a deterministic seed derived from global seed\n    worker_seed = FIX_SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n# EVOLVE-BLOCK-START\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\ndef time_features(dates, freq: str = \"b\") -> np.ndarray:\n    \"\"\"Business-day calendar features (dow/dom/doy scaled to [-0.5, 0.5]).\"\"\"\n    dates = pd.to_datetime(dates)\n    dow = dates.dayofweek / 6.0 - 0.5\n    dom = (dates.day - 1) / 30.0 - 0.5\n    doy = (dates.dayofyear - 1) / 365.0 - 0.5\n    return np.vstack([dow, dom, doy])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# Feature fusion is disabled in this standalone script; keep a tiny placeholder.\nDEFAULT_FUSION_CONFIG: Dict[str, Any] = {}\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:  # pragma: no cover\n    raise NotImplementedError(\n        \"Feature fusion is disabled; provide cached train_raw/val_raw/test_raw CSVs instead.\"\n    )\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\n# DFTSeriesDecomp is unnecessary here because decomp_method is fixed to 'moving_avg'.\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\n# TemporalEmbedding / FixedEmbedding stubs are not needed since embed='timeF'\n# always routes through TimeFeatureEmbedding in DataEmbeddingWoPos.\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        # For this task we always use calendar time features (embed='timeF')\n        self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method != 'moving_avg':\n            raise ValueError('Unsupported decomposition method')\n        self.decomposition = SeriesDecomp(configs.moving_avg)\n        if self.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list: List[torch.Tensor] = []\n        trend_list: List[torch.Tensor] = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list: List[torch.Tensor] = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n            # Learnable weights for aggregating multi-scale predictions instead of a simple sum\n            self.scale_weights = nn.Parameter(torch.ones(configs.down_sampling_layers + 1))\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        # In this task we always set use_future_temporal_feature=0, so we skip\n        # the unused future-time gating logic and directly build multi-scale\n        # encoder inputs. This keeps the forward pass compact but is behaviour-\n        # equivalent for the current configuration.\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list: List[torch.Tensor] = []\n        x_mark_processed: List[torch.Tensor] = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list: List[torch.Tensor] = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        # Multi-scale regression + projection\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out_stack = torch.stack(dec_out_list, dim=-1)\n        # Aggregate predictions from different scales using learnable softmax weights\n        if hasattr(self, \"scale_weights\"):\n            weights = torch.softmax(self.scale_weights, dim=0)\n            dec_out = (dec_out_stack * weights.view(1, 1, 1, -1)).sum(-1)\n        else:\n            dec_out = dec_out_stack.sum(-1)\n        # Denormalise back to the original scale of encoder inputs\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        # With channel_independence fixed to 0 in this pipeline, we only need\n        # the shared multi-scale regression path, which removes unused branches\n        # and slightly reduces overhead without changing behaviour.\n        enc_inputs, out_res_list = x_list\n        dec_out_list: List[torch.Tensor] = []\n        for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n            dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n            dec_out = self.out_projection(dec_out, i, out_res)\n            dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        raise ValueError('Unsupported task name for TimeMixer')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # project_root: Path = Path(__file__).resolve().parents[0]\n    # project_root: Path = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test\\exp_iron_4_gpu\") \n    project_root: Path = Path(r\"/home/jovyan/research/kaikai/c3i/AutoMLAgent/openevolve/iron_test/exp_iron_4_gpu\") \n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    cached_split_dir: Path | None = None\n    use_cached_splits: bool = True\n    seq_len: int = 48\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 1000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # device: str = \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n    blend_alpha: float = 0.8\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.cached_split_dir is None:\n            self.cached_split_dir = self.project_root / \"data\"\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_split_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    \"\"\"Unused in this standalone pipeline; cached CSV splits are loaded instead.\"\"\"\n    raise NotImplementedError(\n        \"fuse_and_align_features is unused; cached train/val/test splits are loaded instead.\"\n    )\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    # unused helper; cached CSV splits are required\n    raise NotImplementedError(\"compute_split_borders is disabled; cached train/val/test splits are required.\")\n\n\ndef get_split_cache_paths(cfg: IronDailyConfig) -> Dict[str, Path]:\n    names = ['train', 'val', 'test']\n    return {name: cfg.cached_split_dir / f\"{name}_raw.csv\" for name in names}\n\n\ndef split_raw_dataframe(fused_df: pd.DataFrame, cfg: IronDailyConfig) -> Dict[str, pd.DataFrame]:\n    # unused helper; cached train/val/test splits must be provided instead\n    raise NotImplementedError(\"split_raw_dataframe is unused in this pipeline; cached splits must be provided.\")\n\n\ndef load_splits_data(\n    cfg: IronDailyConfig,\n) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path], bool]:\n    split_paths = get_split_cache_paths(cfg)\n    if cfg.use_cached_splits and all(path.exists() for path in split_paths.values()):\n        logger.info(\"Loading cached splits from %s\", cfg.cached_split_dir)\n        splits = {\n            name: pd.read_csv(path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n            for name, path in split_paths.items()\n        }\n        return splits, split_paths\n\n\ndef run_feature_engineering_on_splits(\n    raw_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Dict[str, pd.DataFrame]:\n    fe_splits: Dict[str, pd.DataFrame] = {}\n    for name, df in raw_splits.items():\n        fe_df = run_feature_engineering(df, cfg)\n        fe_splits[name] = fe_df\n    return fe_splits\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_single_split_data(\n    df: pd.DataFrame,\n    cfg: IronDailyConfig,\n    feature_cols: List[str] | None = None,\n) -> Tuple[Dict[str, np.ndarray], List[str]]:\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    if feature_cols is None:\n        feature_cols = [c for c in df.columns if c != 'date']\n    missing_cols = [c for c in feature_cols if c not in df.columns]\n    if missing_cols:\n        raise KeyError(f\"Missing expected feature columns: {missing_cols}\")\n    df = df[['date'] + feature_cols]\n    data_values = df[feature_cols].values.astype(np.float32)\n    stamp_slice = build_time_mark_array(df['date'], cfg)\n    split_entry = {\n        'data': data_values,\n        'stamp': stamp_slice.astype(np.float32),\n        'length': len(data_values),\n        'dates': df['date'].to_numpy(),\n    }\n    return split_entry, feature_cols\n\n\ndef prepare_splits_after_engineering(\n    fe_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Tuple[Dict[str, Dict[str, np.ndarray]], List[str]]:\n    split_info: Dict[str, Dict[str, np.ndarray]] = {}\n    feature_cols: List[str] | None = None\n    for name in ['train', 'val', 'test']:\n        if name not in fe_splits:\n            raise KeyError(f\"Missing split '{name}' in engineered datasets.\")\n        split_entry, feature_cols = prepare_single_split_data(fe_splits[name], cfg, feature_cols)\n        split_info[name] = split_entry\n\n    # \u6807\u51c6\u5316\u9664\u76ee\u6807\u5217\u4e4b\u5916\u7684\u7279\u5f81\uff08\u4f7f\u7528\u8bad\u7ec3\u96c6\u7edf\u8ba1\u91cf\uff09\uff0c\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    if 'train' in split_info:\n        train_data = split_info['train']['data']\n        if isinstance(train_data, np.ndarray) and train_data.ndim == 2 and train_data.shape[1] > 1:\n            num_features = train_data.shape[1]\n            feat_slice = slice(0, num_features - 1)  # \u6700\u540e\u4e00\u5217\u4e3a\u76ee\u6807y\uff0c\u4fdd\u6301\u539f\u5c3a\u5ea6\n            mean = train_data[:, feat_slice].mean(axis=0, keepdims=True)\n            std = train_data[:, feat_slice].std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            for name in ['train', 'val', 'test']:\n                data = split_info[name]['data'].astype(np.float32)\n                data[:, feat_slice] = (data[:, feat_slice] - mean) / std\n                split_info[name]['data'] = data\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=False,\n            worker_init_fn=_worker_init_fn,\n            generator=_shared_generator,\n        )\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef _collect_log_forecasts(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:\n    \"\"\"Helper that returns (preds, trues, naive) in log space.\"\"\"\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    naives: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            # \u5f53\u524d\u914d\u7f6e\u4e2d\u603b\u662f\u4f7f\u7528\u591a\u5c42\u4e0b\u91c7\u6837\uff0c\u56e0\u6b64\u89e3\u7801\u5668\u8f93\u5165\u6052\u4e3a None\n            dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n\n            # Naive baseline: repeat last observed target value over the horizon\n            if cfg.c_out == 1:\n                last_val = batch_x[:, -1:, -1:]\n            else:\n                last_val = batch_x[:, -1:, 0:1]\n            naive_y = last_val.repeat(1, cfg.pred_len, 1)\n\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n            naives.append(naive_y.cpu().numpy())\n    if not preds:\n        return None, None, None\n\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    naive_arr = np.concatenate(naives, axis=0)\n    return preds_arr, trues_arr, naive_arr\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n    calibr: Tuple[float, float] | None = None,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Evaluate model on a loader and compute error metrics.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n\n    # Blend model and naive forecasts in log space\n    alpha = getattr(cfg, \"blend_alpha\", 0.8)\n    preds_arr = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n    # Optional linear calibration in log-space: y \u2248 w * y_pred + b\n    if calibr is not None:\n        w, b = calibr\n        preds_arr = preds_arr * float(w) + float(b)\n\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(\n        np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None))\n    )\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Jointly tune blend_alpha and affine log-space calibration on validation data.\n\n    We search a small grid of blend alphas; for each blended forecast we fit a\n    simple linear calibration y \u2248 w*y_pred + b in closed form, then keep the\n    combination that minimises (MSE + MAE) on the validation set. To avoid\n    harming performance, we also compare against an identity (no-calibration)\n    mapping and only apply calibration when it helps.\n    \"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # \u6269\u5927 alpha \u641c\u7d22\u7f51\u683c\u5230 [0.0, 1.0] \u5e76\u7ec6\u5316\u6b65\u957f\uff0c\u63d0\u9ad8\u5728\u9a8c\u8bc1\u96c6\u4e0a\u9009\u62e9\u6700\u4f18\u6df7\u5408\u6743\u91cd\u7684\u7cbe\u5ea6\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            # \u9000\u5316\u60c5\u51b5\uff1a\u65e0\u6cd5\u7a33\u5b9a\u4f30\u8ba1\u7ebf\u6027\u6821\u51c6\u53c2\u6570\uff0c\u56de\u9000\u5230\u6052\u7b49\u6620\u5c04\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                # \u51e0\u4e4e\u65e0\u65b9\u5dee\u65f6\u53ea\u6821\u6b63\u504f\u79fb\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        # \u8bc4\u4ef7\u201c\u5b66\u4e60\u5f97\u5230\u7684\u7ebf\u6027\u6821\u51c6\u201d\u548c\u201c\u65e0\u6821\u51c6\u6052\u7b49\u6620\u5c04\u201d\u4e24\u79cd\u65b9\u6848\uff0c\u907f\u514d\u6821\u51c6\u5728\u9a8c\u8bc1\u96c6\u4e0a\u6076\u5316\u8bef\u5dee\n        calibrated = blended * float(w) + float(b)\n        err = calibrated - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        score = mse + mae\n\n        err_id = blended - trues_arr\n        mse_id = float((err_id ** 2).mean())\n        mae_id = float(np.abs(err_id).mean())\n        score_id = mse_id + mae_id\n        if score_id < score:\n            score = score_id\n            w, b = 1.0, 0.0\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n\n\ndef tune_blend_alpha(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> float:\n    # Deprecated: blend_alpha is now tuned inside compute_log_calibration.\n    return float(getattr(cfg, \"blend_alpha\", 0.8))\n\n\ndef train_predict_evaluate() -> None:\n    cfg = IronDailyConfig()\n    print(\"1) \u52a0\u8f7d\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6...\")\n    raw_splits, split_paths = load_splits_data(cfg)\n    print(f\"   \u5df2\u52a0\u8f7d\u6570\u636e\uff1a{', '.join(str(p.name) for p in split_paths.values())}\")\n\n    print(\"   \u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in raw_splits.items()})\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u5bf9\u62c6\u5206\u540e\u7684\u6570\u636e\u5206\u522b\u53d8\u6362...\")\n    fe_splits = run_feature_engineering_on_splits(raw_splits, cfg)\n    print(\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in fe_splits.items()})\n\n    print(\"3) \u6570\u636e\u7a97\u53e3\u6784\u5efa\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_splits_after_engineering(fe_splits, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {name: len(loader.dataset) for name, loader in loaders.items()}\n    print(\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1a\", dataset_sizes)\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    # \u4f7f\u7528\u8f7b\u5fae\u7684\u6743\u91cd\u8870\u51cf\u63d0\u5347\u6cdb\u5316\u80fd\u529b\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=5e-4)\n    criterion = nn.MSELoss()\n    # \u7a0d\u5f31\u7684\u65b9\u5411\u6b63\u5219\uff0c\u66f4\u597d\u517c\u987eMSE/MAE\u4e0e\u65b9\u5411\u4e00\u81f4\u6027\n    lambda_dir = 0.08\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, lr={cfg.learning_rate}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}, down_layers={cfg.down_sampling_layers}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    # \u5728\u9a8c\u8bc1\u96c6\u4e0a\u8054\u5408\u641c\u7d22blend_alpha\u5e76\u62df\u5408\u7b80\u5355\u7ebf\u6027\u6821\u51c6\u53c2\u6570\n    calib_w, calib_b = compute_log_calibration(model, loaders[\"val\"], cfg, cfg.device_obj)\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj, calibr=(calib_w, calib_b)\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n# EVOLVE-BLOCK-END\n    return test_mse, test_mae, test_mape, test_da\n\nif __name__ == \"__main__\":\n    test_mse, test_mae, test_mape, test_da = train_predict_evaluate()\n    \n\n```\nUnique approach: Modification: Change 1: Replace 5 lines with 5 lines\nChange 2: Replace 2 lines with 2 lines, Alternative test_mse approach, Alternative test_mae approach\n\n# Current Program\n```python\n\"\"\"\nStandalone pipeline for the iron_future_01_daily task.\n\nThis script aligns raw series data, applies the task-specific feature engineering\nsteps, builds sliding-window datasets, and trains/evaluates the TimeMixer model\nend-to-end without relying on external modules from the project.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom types import SimpleNamespace\nfrom pandas.tseries import offsets\nfrom pandas.tseries.frequencies import to_offset\nimport os\n\n# Ensure deterministic CuBLAS workspace for CUDA deterministic algorithms\nif torch.cuda.is_available():\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"iron_daily_pipeline\")\nFIX_SEED = 2021\n\ndef _set_global_seed(seed: int = 2021) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        try:\n            torch.use_deterministic_algorithms(True)\n        except Exception:\n            # Older torch versions may not have this or certain ops may not support it\n            pass\n\n_set_global_seed(FIX_SEED)\n\n# Generator for DataLoader to keep shuffling deterministic\n_shared_generator = torch.Generator()\n_shared_generator.manual_seed(FIX_SEED)\n\ndef _worker_init_fn(worker_id: int) -> None:\n    # Ensure each worker has a deterministic seed derived from global seed\n    worker_seed = FIX_SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    torch.manual_seed(worker_seed)\n\n# EVOLVE-BLOCK-START\n\n# -----------------------------------------------------------------------------\n# Feature engineering helpers (inlined from data_provider.feature_engineer)\n# -----------------------------------------------------------------------------\n\ndef time_features(dates, freq: str = \"b\") -> np.ndarray:\n    \"\"\"Business-day calendar features (dow/dom/doy scaled to [-0.5, 0.5]).\"\"\"\n    dates = pd.to_datetime(dates)\n    dow = dates.dayofweek / 6.0 - 0.5\n    dom = (dates.day - 1) / 30.0 - 0.5\n    doy = (dates.dayofyear - 1) / 365.0 - 0.5\n    return np.vstack([dow, dom, doy])\n\ndef add_age_since_release(df: pd.DataFrame, monthly_cols: List[str], date_col: str) -> pd.DataFrame:\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.sort_values(date_col)\n    for col in monthly_cols:\n        age_col = f\"{col}_age_since_release\"\n        last_release_date = None\n        ages = []\n        for idx, (val, prev_val, cur_date) in enumerate(zip(df[col], df[col].shift(1), df[date_col])):\n            if pd.isna(val):\n                ages.append(np.nan)\n                continue\n            if idx == 0 or val != prev_val:\n                last_release_date = cur_date\n                ages.append(0)\n            else:\n                ages.append((cur_date - last_release_date).days if last_release_date else np.nan)\n        df[age_col] = ages\n    return df\n\n\ndef add_pct_change(df: pd.DataFrame, cols: List[str], periods: List[int] | None = None) -> pd.DataFrame:\n    df = df.copy()\n    if periods is None:\n        periods = [15, 30]\n    for col in cols:\n        for p in periods:\n            df[f\"{col}_pctchg_{p}\"] = df[col].pct_change(p)\n    return df\n\n\ndef add_rolling_features_nomedian(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n    df = df.copy()\n    for col in cols:\n        for w in windows:\n            shifted = df[col].shift(1)\n            df[f\"{col}_rollmean_{w}\"] = shifted.rolling(w).mean()\n            df[f\"{col}_rollstd_{w}\"] = shifted.rolling(w).std()\n            df[f\"{col}_roll_slope{w}\"] = shifted.rolling(w).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n            )\n    return df\n\n\ndef add_price_features(df: pd.DataFrame, price_cols: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    if not isinstance(price_cols, list):\n        price_cols = [price_cols]\n    for price_col in price_cols:\n        for p in [1, 3, 7]:\n            df[f\"{price_col}_ret_{p}d\"] = df[price_col].pct_change(p)\n        for w in [5, 10]:\n            ma = df[price_col].rolling(w).mean()\n            df[f\"{price_col}_ma_{w}d\"] = ma\n            df[f\"{price_col}_price_minus_ma_{w}d\"] = df[price_col] - ma\n        for v in [7, 21]:\n            df[f\"{price_col}_vol_{v}d\"] = df[price_col].pct_change().rolling(v).std()\n    return df\n\n\ndef add_macd_features(df: pd.DataFrame, price_col: str = \"y\", fast: int = 8, slow: int = 21, signal: int = 5) -> pd.DataFrame:\n    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()\n    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()\n    df['MACD_DIF'] = ema_fast - ema_slow\n    df['MACD_DEA'] = df['MACD_DIF'].ewm(span=signal, adjust=False).mean()\n    df['MACD_BAR'] = df['MACD_DIF'] - df['MACD_DEA']\n    df['MACD_cross'] = (df['MACD_DIF'] > df['MACD_DEA']).astype(int)\n    df['MACD_cross_above'] = ((df['MACD_DIF'] > df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) <= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_cross_below'] = ((df['MACD_DIF'] < df['MACD_DEA']) &\n                              (df['MACD_DIF'].shift(1) >= df['MACD_DEA'].shift(1))).astype(int)\n    df['MACD_strength'] = df['MACD_BAR'] / df[price_col].rolling(20).mean()\n    return df\n\n\ndef add_commodity_optimized_indicators(df: pd.DataFrame, price_col: str = 'y') -> pd.DataFrame:\n    df = df.copy()\n    df = add_macd_features(df, price_col=price_col, fast=8, slow=21, signal=5)\n    return df\n\n\ndef add_supply_demand_composite_features(\n    df: pd.DataFrame,\n    port_inventory: str,\n    supply_side: str,\n    demand_side: str,\n    production_activity: str,\n    macro_cost: str,\n) -> pd.DataFrame:\n    df = df.copy()\n    production_intensity = df[production_activity] * df[demand_side] / 100.0\n    df['production_inventory_ratio'] = production_intensity / df[port_inventory].replace(0, np.nan)\n    df['inventory_cover_days'] = df[port_inventory] / df[demand_side].replace(0, np.nan)\n    df['inventory_cover_days_roll5'] = df['inventory_cover_days'].rolling(5).mean()\n    df['supply_demand_gap'] = df[supply_side] - df[demand_side]\n    df['supply_demand_ratio'] = df[supply_side] / df[demand_side].replace(0, np.nan)\n    inventory_trend = df[port_inventory].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['inventory_structure_health'] = inventory_trend - consumption_trend\n    pmi_trend = df[macro_cost].rolling(3).mean()\n    consumption_trend = df[demand_side].rolling(10).apply(\n        lambda x: np.polyfit(range(len(x)), x, 1)[0], raw=False\n    )\n    df['macro_demand_transmission'] = pmi_trend * consumption_trend\n    return df\n\n\n# Feature fusion is disabled in this standalone script; keep a tiny placeholder.\nDEFAULT_FUSION_CONFIG: Dict[str, Any] = {}\n\n\ndef build_feature_fusion_dataset(cfg: 'IronDailyConfig') -> pd.DataFrame:  # pragma: no cover\n    raise NotImplementedError(\n        \"Feature fusion is disabled; provide cached train_raw/val_raw/test_raw CSVs instead.\"\n    )\n\n\n# -----------------------------------------------------------------------------\n# TimeMixer implementation (inlined from models/TimeMixer.py)\n# -----------------------------------------------------------------------------\n\n\nclass MovingAvg(nn.Module):\n    def __init__(self, kernel_size: int, stride: int = 1):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        return x.permute(0, 2, 1)\n\n\nclass SeriesDecomp(nn.Module):\n    def __init__(self, kernel_size: int):\n        super().__init__()\n        self.moving_avg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean\n        return res, moving_mean\n\n\n# DFTSeriesDecomp is unnecessary here because decomp_method is fixed to 'moving_avg'.\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.token_conv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=padding,\n            padding_mode='circular',\n            bias=False,\n        )\n        nn.init.kaiming_normal_(self.token_conv.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.token_conv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\n\n# TemporalEmbedding / FixedEmbedding stubs are not needed since embed='timeF'\n# always routes through TimeFeatureEmbedding in DataEmbeddingWoPos.\n\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model: int, freq: str = 'h'):\n        super().__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6, 'ms': 7, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        self.embed = nn.Linear(freq_map[freq], d_model, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.embed(x)\n\n\nclass DataEmbeddingWoPos(nn.Module):\n    def __init__(self, c_in: int, d_model: int, embed_type: str, freq: str, dropout: float):\n        super().__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        # For this task we always use calendar time features (embed='timeF')\n        self.temporal_embedding = TimeFeatureEmbedding(d_model=d_model, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor | None, x_mark: torch.Tensor | None) -> torch.Tensor:\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n\n\nclass Normalize(nn.Module):\n    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True, non_norm: bool = False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.non_norm = non_norm\n        if self.affine:\n            self.affine_weight = nn.Parameter(torch.ones(num_features))\n            self.affine_bias = nn.Parameter(torch.zeros(num_features))\n        self.mean = None\n        self.stdev = None\n\n    def forward(self, x: torch.Tensor, mode: str) -> torch.Tensor:\n        if mode == 'norm':\n            if not self.non_norm:\n                dims = tuple(range(1, x.ndim - 1))\n                self.mean = torch.mean(x, dim=dims, keepdim=True).detach()\n                self.stdev = torch.sqrt(torch.var(x, dim=dims, keepdim=True, unbiased=False) + self.eps).detach()\n                x = (x - self.mean) / self.stdev\n                if self.affine:\n                    x = x * self.affine_weight + self.affine_bias\n            return x\n        if mode == 'denorm':\n            if not self.non_norm and self.mean is not None and self.stdev is not None:\n                if self.affine:\n                    x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n                x = x * self.stdev + self.mean\n            return x\n        raise NotImplementedError\n\n\nclass MultiScaleSeasonMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.down_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                ),\n            )\n            for i in range(configs.down_sampling_layers)\n        ])\n\n    def forward(self, season_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.up_sampling_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** (i + 1)),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n                nn.GELU(),\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                ),\n            )\n            for i in reversed(range(configs.down_sampling_layers))\n        ])\n\n    def forward(self, trend_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.channel_independence = configs.channel_independence\n        if configs.decomp_method != 'moving_avg':\n            raise ValueError('Unsupported decomposition method')\n        self.decomposition = SeriesDecomp(configs.moving_avg)\n        if self.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(configs.d_model, configs.d_ff),\n                nn.GELU(),\n                nn.Linear(configs.d_ff, configs.d_model),\n            )\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(configs.d_model, configs.d_ff),\n            nn.GELU(),\n            nn.Linear(configs.d_ff, configs.d_model),\n        )\n\n    def forward(self, x_list: List[torch.Tensor]) -> List[torch.Tensor]:\n        length_list = [x.size(1) for x in x_list]\n        season_list: List[torch.Tensor] = []\n        trend_list: List[torch.Tensor] = []\n        for x in x_list:\n            season, trend = self.decomposition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n        out_list: List[torch.Tensor] = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list, length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n\n\nclass TimeMixer(nn.Module):\n    def __init__(self, configs):\n        super().__init__()\n        self.configs = configs\n        self.task_name = configs.task_name\n        self.seq_len = configs.seq_len\n        self.label_len = configs.label_len\n        self.pred_len = configs.pred_len\n        self.down_sampling_window = configs.down_sampling_window\n        self.channel_independence = configs.channel_independence\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs) for _ in range(configs.e_layers)])\n        self.preprocess = SeriesDecomp(configs.moving_avg)\n        self.enc_in = configs.enc_in\n        self.use_future_temporal_feature = configs.use_future_temporal_feature\n        self.future_gate = nn.Linear(2 * configs.d_model, configs.d_model) if self.use_future_temporal_feature else None\n        self.dir_adjust_scale = getattr(configs, 'dir_adjust_scale', 20)\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbeddingWoPos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        else:\n            self.enc_embedding = DataEmbeddingWoPos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n        self.layer = configs.e_layers\n        self.normalize_layers = nn.ModuleList([\n            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)\n            for _ in range(configs.down_sampling_layers + 1)\n        ])\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            self.predict_layers = nn.ModuleList([\n                nn.Linear(\n                    configs.seq_len // (configs.down_sampling_window ** i),\n                    configs.pred_len,\n                )\n                for i in range(configs.down_sampling_layers + 1)\n            ])\n            if self.channel_independence == 1:\n                self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)\n            else:\n                self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)\n                self.out_res_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n                self.regression_layers = nn.ModuleList([\n                    nn.Linear(\n                        configs.seq_len // (configs.down_sampling_window ** i),\n                        configs.pred_len,\n                    )\n                    for i in range(configs.down_sampling_layers + 1)\n                ])\n            # Learnable weights for aggregating multi-scale predictions instead of a simple sum\n            self.scale_weights = nn.Parameter(torch.ones(configs.down_sampling_layers + 1))\n        else:\n            raise ValueError('Unsupported task name')\n\n    def out_projection(self, dec_out: torch.Tensor, i: int, out_res: torch.Tensor) -> torch.Tensor:\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        return dec_out + out_res\n\n    def pre_enc(self, x_list: List[torch.Tensor]) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.channel_independence == 1:\n            return x_list, None\n        out1_list, out2_list = [], []\n        for x in x_list:\n            x_1, x_2 = self.preprocess(x)\n            out1_list.append(x_1)\n            out2_list.append(x_2)\n        return out1_list, out2_list\n\n    def __multi_scale_process_inputs(\n        self, x_enc: torch.Tensor, x_mark_enc: torch.Tensor | None\n    ) -> Tuple[List[torch.Tensor], List[torch.Tensor] | None]:\n        if self.configs.down_sampling_method == 'max':\n            down_pool = nn.MaxPool1d(self.configs.down_sampling_window, return_indices=False)\n        elif self.configs.down_sampling_method == 'avg':\n            down_pool = nn.AvgPool1d(self.configs.down_sampling_window)\n        elif self.configs.down_sampling_method == 'conv':\n            padding = 1 if torch.__version__ >= '1.5.0' else 2\n            down_pool = nn.Conv1d(\n                in_channels=self.configs.enc_in,\n                out_channels=self.configs.enc_in,\n                kernel_size=3,\n                padding=padding,\n                stride=self.configs.down_sampling_window,\n            )\n        else:\n            raise ValueError('Unknown down sampling method')\n\n        x_enc = x_enc.permute(0, 2, 1)\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list: List[torch.Tensor] = []\n        x_mark_sampling_list: List[torch.Tensor] | None = None\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        if x_mark_enc is not None:\n            x_mark_sampling_list = [x_mark_enc]\n\n        for _ in range(self.configs.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None and x_mark_sampling_list is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n        return x_enc, x_mark_enc\n\n    def forecast(\n        self,\n        x_enc: torch.Tensor,\n        x_mark_enc: torch.Tensor | None,\n        x_dec: torch.Tensor | None,\n        x_mark_dec: torch.Tensor | None,\n    ) -> torch.Tensor:\n        # In this task we always set use_future_temporal_feature=0, so we skip\n        # the unused future-time gating logic and directly build multi-scale\n        # encoder inputs. This keeps the forward pass compact but is behaviour-\n        # equivalent for the current configuration.\n        x_enc_list, x_mark_list = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n        x_list: List[torch.Tensor] = []\n        x_mark_processed: List[torch.Tensor] = []\n        if x_mark_list is not None:\n            for x, x_mark, norm_layer in zip(x_enc_list, x_mark_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_processed.append(x_mark)\n        else:\n            for x, norm_layer in zip(x_enc_list, self.normalize_layers):\n                x = norm_layer(x, 'norm')\n                if self.channel_independence == 1:\n                    B, T, N = x.size()\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n        enc_out_list: List[torch.Tensor] = []\n        processed = self.pre_enc(x_list)\n        if self.channel_independence == 1:\n            processed_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(processed_list, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in processed_list:\n                    enc_out_list.append(self.enc_embedding(x, None))\n        else:\n            enc_inputs, out_res_list = processed\n            if x_mark_list is not None:\n                for x, x_mark in zip(enc_inputs, x_mark_processed):\n                    enc_out_list.append(self.enc_embedding(x, x_mark))\n            else:\n                for x in enc_inputs:\n                    enc_out_list.append(self.enc_embedding(x, None))\n            x_list = (enc_inputs, out_res_list)\n        for i in range(self.layer):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n        # Multi-scale regression + projection\n        dec_out_list = self.future_multi_mixing(x_enc.size(0), enc_out_list, x_list)\n        dec_out_stack = torch.stack(dec_out_list, dim=-1)\n        # Aggregate predictions from different scales using learnable softmax weights\n        if hasattr(self, \"scale_weights\"):\n            weights = torch.softmax(self.scale_weights, dim=0)\n            dec_out = (dec_out_stack * weights.view(1, 1, 1, -1)).sum(-1)\n        else:\n            dec_out = dec_out_stack.sum(-1)\n        # Denormalise back to the original scale of encoder inputs\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n\n    def future_multi_mixing(self, B: int, enc_out_list: List[torch.Tensor], x_list):\n        # With channel_independence fixed to 0 in this pipeline, we only need\n        # the shared multi-scale regression path, which removes unused branches\n        # and slightly reduces overhead without changing behaviour.\n        enc_inputs, out_res_list = x_list\n        dec_out_list: List[torch.Tensor] = []\n        for i, (enc_out, out_res) in enumerate(zip(enc_out_list, out_res_list)):\n            dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(0, 2, 1)\n            dec_out = self.out_projection(dec_out, i, out_res)\n            dec_out_list.append(dec_out)\n        return dec_out_list\n\n    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n        if self.task_name in ['long_term_forecast', 'short_term_forecast']:\n            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n        raise ValueError('Unsupported task name for TimeMixer')\n\n\n# -----------------------------------------------------------------------------\n# Pipeline configuration and training logic\n# -----------------------------------------------------------------------------\n\n\n@dataclass\nclass IronDailyConfig:\n    # project_root: Path = Path(__file__).resolve().parents[0]\n    # project_root: Path = Path(r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\AutoMLAgent\\openevolve\\iron_test\\exp_iron_4_gpu\") \n    project_root: Path = Path(r\"/home/jovyan/research/kaikai/c3i/AutoMLAgent/openevolve/iron_test/exp_iron_4_gpu\") \n    checkpoint_dir: Path | None = None\n    raw_data_override: str | None = None\n    fusion_config: Dict[str, Any] | None = None\n    cached_split_dir: Path | None = None\n    use_cached_splits: bool = True\n    seq_len: int = 48\n    label_len: int = 0\n    pred_len: int = 12\n    freq: str = \"b\"\n    target_col: str = \"y\"\n    batch_size: int = 16\n    learning_rate: float = 1e-2\n    train_epochs: int = 10\n    patience: int = 1000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # device: str = \"cpu\"\n    e_layers: int = 4\n    d_layers: int = 2\n    d_model: int = 16\n    d_ff: int = 32\n    dropout: float = 0.1\n    down_sampling_layers: int = 4\n    down_sampling_window: int = 2\n    factor: int = 1\n    channel_independence: int = 0\n    c_out: int = 1\n    use_future_temporal_feature: int = 0\n    moving_avg: int = 25\n    decomp_method: str = \"moving_avg\"\n    top_k: int = 5\n    embed: str = \"timeF\"\n    use_norm: int = 1\n    dir_adjust_scale: float = 20.0\n    split_ratio: Dict[str, float] | None = None\n    blend_alpha: float = 0.8\n\n    def __post_init__(self) -> None:\n        if self.checkpoint_dir is None:\n            self.checkpoint_dir = self.project_root / \"checkpoints\"\n        if self.fusion_config is None:\n            self.fusion_config = copy.deepcopy(DEFAULT_FUSION_CONFIG)\n        if self.cached_split_dir is None:\n            self.cached_split_dir = self.project_root / \"data\"\n        if self.split_ratio is None:\n            self.split_ratio = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_split_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def device_obj(self) -> torch.device:\n        return torch.device(self.device)\n\n\ndef fuse_and_align_features(cfg: 'IronDailyConfig') -> pd.DataFrame:\n    \"\"\"Unused in this standalone pipeline; cached CSV splits are loaded instead.\"\"\"\n    raise NotImplementedError(\n        \"fuse_and_align_features is unused; cached train/val/test splits are loaded instead.\"\n    )\n\n\ndef run_feature_engineering(df: pd.DataFrame, cfg: IronDailyConfig) -> pd.DataFrame:\n    df = df.copy()\n    df[\"y\"] = np.log1p(df[\"value\"])\n    cols = list(df.columns)\n    cols.remove(cfg.target_col)\n    remove_list = [\"value\", \"contract_id\", \"date\"] + [f\"value_lag_{i + 1}\" for i in range(4, 10)]\n    cols = [c for c in cols if c not in remove_list]\n    df = df[[\"date\"] + cols + [cfg.target_col]]\n    df = add_age_since_release(df, monthly_cols=[\"GM0000033031\"], date_col=\"date\")\n    df = add_pct_change(df, cols=[\"ID00186575\", \"ID00186100\"])\n    df = add_rolling_features_nomedian(df, cols=[\"ID01002312\"], windows=[3, 5, 15])\n    df = add_price_features(df, price_cols=[\"ID00183109\"])\n    df = add_commodity_optimized_indicators(df, price_col=\"y\")\n    df = add_supply_demand_composite_features(\n        df,\n        port_inventory=\"ID01002312\",\n        supply_side=\"ID00186575\",\n        demand_side=\"ID00186100\",\n        production_activity=\"ID00183109\",\n        macro_cost=\"CM0000013263\",\n    )\n    df = df.dropna().reset_index(drop=True)\n    return df\n\n\ndef compute_split_borders(total_len: int, cfg: IronDailyConfig) -> Tuple[List[int], List[int]]:\n    # unused helper; cached CSV splits are required\n    raise NotImplementedError(\"compute_split_borders is disabled; cached train/val/test splits are required.\")\n\n\ndef get_split_cache_paths(cfg: IronDailyConfig) -> Dict[str, Path]:\n    names = ['train', 'val', 'test']\n    return {name: cfg.cached_split_dir / f\"{name}_raw.csv\" for name in names}\n\n\ndef split_raw_dataframe(fused_df: pd.DataFrame, cfg: IronDailyConfig) -> Dict[str, pd.DataFrame]:\n    # unused helper; cached train/val/test splits must be provided instead\n    raise NotImplementedError(\"split_raw_dataframe is unused in this pipeline; cached splits must be provided.\")\n\n\ndef load_splits_data(\n    cfg: IronDailyConfig,\n) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path], bool]:\n    split_paths = get_split_cache_paths(cfg)\n    if cfg.use_cached_splits and all(path.exists() for path in split_paths.values()):\n        logger.info(\"Loading cached splits from %s\", cfg.cached_split_dir)\n        splits = {\n            name: pd.read_csv(path, parse_dates=['date']).sort_values('date').reset_index(drop=True)\n            for name, path in split_paths.items()\n        }\n        return splits, split_paths\n\n\ndef run_feature_engineering_on_splits(\n    raw_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Dict[str, pd.DataFrame]:\n    fe_splits: Dict[str, pd.DataFrame] = {}\n    for name, df in raw_splits.items():\n        fe_df = run_feature_engineering(df, cfg)\n        fe_splits[name] = fe_df\n    return fe_splits\n\n\ndef build_time_mark_array(dates: pd.Series, cfg: IronDailyConfig) -> np.ndarray:\n    if cfg.embed == 'timeF':\n        date_array = pd.to_datetime(dates.values)\n        data_stamp = time_features(date_array, freq=cfg.freq)\n        return data_stamp.transpose(1, 0)\n    df_stamp = pd.DataFrame({'date': pd.to_datetime(dates)})\n    df_stamp['month'] = df_stamp['date'].dt.month\n    df_stamp['day'] = df_stamp['date'].dt.day\n    df_stamp['weekday'] = df_stamp['date'].dt.weekday\n    df_stamp['hour'] = df_stamp['date'].dt.hour\n    return df_stamp[['month', 'day', 'weekday', 'hour']].values\n\n\ndef prepare_single_split_data(\n    df: pd.DataFrame,\n    cfg: IronDailyConfig,\n    feature_cols: List[str] | None = None,\n) -> Tuple[Dict[str, np.ndarray], List[str]]:\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date').reset_index(drop=True)\n    df = df.assign(**{cfg.target_col: df.pop(cfg.target_col)})\n    if feature_cols is None:\n        feature_cols = [c for c in df.columns if c != 'date']\n    missing_cols = [c for c in feature_cols if c not in df.columns]\n    if missing_cols:\n        raise KeyError(f\"Missing expected feature columns: {missing_cols}\")\n    df = df[['date'] + feature_cols]\n    data_values = df[feature_cols].values.astype(np.float32)\n    stamp_slice = build_time_mark_array(df['date'], cfg)\n    split_entry = {\n        'data': data_values,\n        'stamp': stamp_slice.astype(np.float32),\n        'length': len(data_values),\n        'dates': df['date'].to_numpy(),\n    }\n    return split_entry, feature_cols\n\n\ndef prepare_splits_after_engineering(\n    fe_splits: Dict[str, pd.DataFrame], cfg: IronDailyConfig\n) -> Tuple[Dict[str, Dict[str, np.ndarray]], List[str]]:\n    split_info: Dict[str, Dict[str, np.ndarray]] = {}\n    feature_cols: List[str] | None = None\n    for name in ['train', 'val', 'test']:\n        if name not in fe_splits:\n            raise KeyError(f\"Missing split '{name}' in engineered datasets.\")\n        split_entry, feature_cols = prepare_single_split_data(fe_splits[name], cfg, feature_cols)\n        split_info[name] = split_entry\n\n    # \u6807\u51c6\u5316\u9664\u76ee\u6807\u5217\u4e4b\u5916\u7684\u7279\u5f81\uff08\u4f7f\u7528\u8bad\u7ec3\u96c6\u7edf\u8ba1\u91cf\uff09\uff0c\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\n    if 'train' in split_info:\n        train_data = split_info['train']['data']\n        if isinstance(train_data, np.ndarray) and train_data.ndim == 2 and train_data.shape[1] > 1:\n            num_features = train_data.shape[1]\n            feat_slice = slice(0, num_features - 1)  # \u6700\u540e\u4e00\u5217\u4e3a\u76ee\u6807y\uff0c\u4fdd\u6301\u539f\u5c3a\u5ea6\n            mean = train_data[:, feat_slice].mean(axis=0, keepdims=True)\n            std = train_data[:, feat_slice].std(axis=0, keepdims=True)\n            std[std == 0] = 1.0\n            for name in ['train', 'val', 'test']:\n                data = split_info[name]['data'].astype(np.float32)\n                data[:, feat_slice] = (data[:, feat_slice] - mean) / std\n                split_info[name]['data'] = data\n    return split_info, feature_cols\n\n\nclass CustomStyleDataset(Dataset):\n    def __init__(self, data: np.ndarray, stamp: np.ndarray, seq_len: int, label_len: int,\n                 pred_len: int, set_type: int, stride_test: int, dates: np.ndarray):\n        self.data_x = torch.from_numpy(data)\n        self.data_y = torch.from_numpy(data)\n        self.data_stamp = torch.from_numpy(stamp)\n        self.seq_len = seq_len\n        self.label_len = label_len\n        self.pred_len = pred_len\n        self.set_type = set_type\n        self.stride_test = stride_test\n        self.dates = dates\n\n    def __len__(self) -> int:\n        total_windows = len(self.data_x) - self.seq_len - self.pred_len + 1\n        if total_windows <= 0:\n            return 0\n        if self.set_type == 2:\n            return max(total_windows // self.stride_test, 0)\n        return total_windows\n\n    def _calc_indices(self, idx: int):\n        stride = self.stride_test if self.set_type == 2 else 1\n        max_s_begin = len(self.data_x) - self.seq_len - self.pred_len\n        s_begin = max_s_begin - idx * stride\n        s_end = s_begin + self.seq_len\n        r_begin = s_end - self.label_len\n        r_end = r_begin + self.label_len + self.pred_len\n        return s_begin, s_end, r_begin, r_end\n\n    def __getitem__(self, idx: int):\n        s_begin, s_end, r_begin, r_end = self._calc_indices(idx)\n        seq_x = self.data_x[s_begin:s_end]\n        seq_y = self.data_y[r_begin:r_end]\n        seq_x_mark = self.data_stamp[s_begin:s_end]\n        seq_y_mark = self.data_stamp[r_begin:r_end]\n        return seq_x.float(), seq_y.float(), seq_x_mark.float(), seq_y_mark.float()\n\n    def window_bounds(self, idx: int):\n        s_begin, s_end, _, _ = self._calc_indices(idx)\n        start_date = pd.Timestamp(self.dates[s_begin])\n        end_date = pd.Timestamp(self.dates[s_end - 1])\n        return start_date, end_date\n\n\ndef make_dataloaders_from_splits(\n    split_info: Dict[str, Dict[str, np.ndarray]], cfg: IronDailyConfig\n) -> Dict[str, DataLoader]:\n    loaders: Dict[str, DataLoader] = {}\n    freq = cfg.freq.lower()\n    stride_test = 2 if freq.startswith('m') else 12\n    set_types = {'train': 0, 'val': 1, 'test': 2}\n    for split_name, set_type in set_types.items():\n        entry = split_info[split_name]\n        dataset = CustomStyleDataset(\n            entry['data'],\n            entry['stamp'],\n            cfg.seq_len,\n            cfg.label_len,\n            cfg.pred_len,\n            set_type,\n            stride_test,\n            entry['dates'],\n        )\n        batch_size = cfg.batch_size if split_name != 'test' else 1\n        shuffle = split_name == 'train'\n        loaders[split_name] = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=False,\n            worker_init_fn=_worker_init_fn,\n            generator=_shared_generator,\n        )\n    return loaders\n\n\ndef build_model(cfg: IronDailyConfig, enc_in: int) -> TimeMixer:\n    model_args = {\n        \"task_name\": \"long_term_forecast\",\n        \"seq_len\": cfg.seq_len,\n        \"label_len\": cfg.label_len,\n        \"pred_len\": cfg.pred_len,\n        \"down_sampling_window\": cfg.down_sampling_window,\n        \"down_sampling_layers\": cfg.down_sampling_layers,\n        \"channel_independence\": cfg.channel_independence,\n        \"e_layers\": cfg.e_layers,\n        \"d_layers\": cfg.d_layers,\n        \"moving_avg\": cfg.moving_avg,\n        \"use_future_temporal_feature\": cfg.use_future_temporal_feature,\n        \"d_model\": cfg.d_model,\n        \"d_ff\": cfg.d_ff,\n        \"dropout\": cfg.dropout,\n        \"embed\": cfg.embed,\n        \"freq\": cfg.freq,\n        \"enc_in\": enc_in,\n        \"dec_in\": enc_in,\n        \"c_out\": cfg.c_out,\n        \"factor\": cfg.factor,\n        \"use_norm\": cfg.use_norm,\n        \"decomp_method\": cfg.decomp_method,\n        \"top_k\": cfg.top_k,\n        \"dir_adjust_scale\": cfg.dir_adjust_scale,\n        \"down_sampling_method\": \"avg\",\n    }\n    model_cfg = SimpleNamespace(**model_args)\n    return TimeMixer(model_cfg)\n\n\ndef extract_target(pred: torch.Tensor, batch_y: torch.Tensor, cfg: IronDailyConfig) -> Tuple[torch.Tensor, torch.Tensor]:\n    f_dim = -1 if cfg.c_out == 1 else 0\n    pred_y = pred[:, -cfg.pred_len :, f_dim:]\n    true_y = batch_y[:, -cfg.pred_len :, f_dim:]\n    return pred_y, true_y\n\n\ndef compute_directional_accuracy(pred_value: np.ndarray, true_value: np.ndarray) -> float:\n    if pred_value.shape[1] < 2:\n        return float(\"nan\")\n    pred_diff = np.diff(pred_value, axis=1)\n    true_diff = np.diff(true_value, axis=1)\n    agreement = np.sign(pred_diff) == np.sign(true_diff)\n    return float(np.mean(agreement))\n\n\ndef _collect_log_forecasts(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:\n    \"\"\"Helper that returns (preds, trues, naive) in log space.\"\"\"\n    model.eval()\n    preds: List[np.ndarray] = []\n    trues: List[np.ndarray] = []\n    naives: List[np.ndarray] = []\n    with torch.no_grad():\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loader:\n            batch_x = batch_x.to(device)\n            batch_y = batch_y.to(device)\n            batch_x_mark = batch_x_mark.to(device)\n            batch_y_mark = batch_y_mark.to(device)\n            # \u5f53\u524d\u914d\u7f6e\u4e2d\u603b\u662f\u4f7f\u7528\u591a\u5c42\u4e0b\u91c7\u6837\uff0c\u56e0\u6b64\u89e3\u7801\u5668\u8f93\u5165\u6052\u4e3a None\n            dec_inp = None\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n\n            # Naive baseline: repeat last observed target value over the horizon\n            if cfg.c_out == 1:\n                last_val = batch_x[:, -1:, -1:]\n            else:\n                last_val = batch_x[:, -1:, 0:1]\n            naive_y = last_val.repeat(1, cfg.pred_len, 1)\n\n            preds.append(pred_y.cpu().numpy())\n            trues.append(true_y.cpu().numpy())\n            naives.append(naive_y.cpu().numpy())\n    if not preds:\n        return None, None, None\n\n    preds_arr = np.concatenate(preds, axis=0)\n    trues_arr = np.concatenate(trues, axis=0)\n    naive_arr = np.concatenate(naives, axis=0)\n    return preds_arr, trues_arr, naive_arr\n\n\ndef evaluate(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n    apply_log_transform: bool = True,\n    calibr: Tuple[float, float] | None = None,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Evaluate model on a loader and compute error metrics.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n\n    # Blend model and naive forecasts in log space\n    alpha = getattr(cfg, \"blend_alpha\", 0.8)\n    preds_arr = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n    # Optional linear calibration in log-space: y \u2248 w * y_pred + b\n    if calibr is not None:\n        w, b = calibr\n        preds_arr = preds_arr * float(w) + float(b)\n\n    scaled_mse = np.mean((preds_arr - trues_arr) ** 2)\n    scaled_mae = np.mean(np.abs(preds_arr - trues_arr))\n    if apply_log_transform:\n        pred_value = np.expm1(preds_arr)\n        true_value = np.expm1(trues_arr)\n    else:\n        pred_value = preds_arr\n        true_value = trues_arr\n    value_mape = np.mean(\n        np.abs((pred_value - true_value) / np.clip(true_value, 1e-6, None))\n    )\n    da_score = compute_directional_accuracy(pred_value, true_value)\n    return scaled_mse, scaled_mae, value_mape, da_score\n\n\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Tune blend_alpha and affine log calibration using a proxy close to final fitness.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    def _combo_score(pred_log: np.ndarray) -> float:\n        \"\"\"Lower is better: combines log/linear errors and directional accuracy.\"\"\"\n        err = pred_log - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        pred_val = np.expm1(pred_log)\n        true_val = np.expm1(trues_arr)\n        mape = float(\n            np.mean(\n                np.abs(pred_val - true_val) / np.clip(true_val, 1e-6, None)\n            )\n        )\n        da = compute_directional_accuracy(pred_val, true_val)\n        if not np.isfinite(da):\n            da = 0.5\n        return mse + mae + mape + (1.0 - da)\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # \u5728 [0,1] \u533a\u95f4\u4e0a\u4f7f\u7528 0.05 \u6b65\u957f\u641c\u7d22 alpha\uff0c\u5e76\u540c\u65f6\u6bd4\u8f83\u201c\u5e26\u7ebf\u6027\u6821\u51c6\u201d\u548c\u201c\u6052\u7b49\u6620\u5c04\u201d\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        # \u95ed\u5f0f\u89e3\u62df\u5408 log \u7a7a\u95f4\u7ebf\u6027\u6821\u51c6 y \u2248 w * blended + b\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        calibrated = blended * float(w) + float(b)\n        score_cal = _combo_score(calibrated)\n        score_id = _combo_score(blended)\n\n        if score_id <= score_cal:\n            score = score_id\n            w, b = 1.0, 0.0\n        else:\n            score = score_cal\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n\n\ndef tune_blend_alpha(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> float:\n    # Deprecated: blend_alpha is now tuned inside compute_log_calibration.\n    return float(getattr(cfg, \"blend_alpha\", 0.8))\n\n\ndef train_predict_evaluate() -> None:\n    cfg = IronDailyConfig()\n    print(\"1) \u52a0\u8f7d\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6...\")\n    raw_splits, split_paths = load_splits_data(cfg)\n    print(f\"   \u5df2\u52a0\u8f7d\u6570\u636e\uff1a{', '.join(str(p.name) for p in split_paths.values())}\")\n\n    print(\"   \u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in raw_splits.items()})\n\n    print(\"2) \u7279\u5f81\u5de5\u7a0b\uff1a\u5bf9\u62c6\u5206\u540e\u7684\u6570\u636e\u5206\u522b\u53d8\u6362...\")\n    fe_splits = run_feature_engineering_on_splits(raw_splits, cfg)\n    print(\"   \u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\uff0c\u6837\u672c\u91cf\uff1a\", {k: len(v) for k, v in fe_splits.items()})\n\n    print(\"3) \u6570\u636e\u7a97\u53e3\u6784\u5efa\u4e0e\u6807\u51c6\u5316...\")\n    split_info, feature_cols = prepare_splits_after_engineering(fe_splits, cfg)\n    enc_in = len(feature_cols)\n    print(f\"   \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6 enc_in={enc_in}\")\n    loaders = make_dataloaders_from_splits(split_info, cfg)\n    dataset_sizes = {name: len(loader.dataset) for name, loader in loaders.items()}\n    print(\"   \u6570\u636e\u7a97\u53e3\u6570\u91cf\uff1a\", dataset_sizes)\n\n    print(\"4) \u6a21\u578b\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3...\")\n    model = build_model(cfg, enc_in).to(cfg.device_obj)\n    # \u4f7f\u7528\u8f7b\u5fae\u7684\u6743\u91cd\u8870\u51cf\u63d0\u5347\u6cdb\u5316\u80fd\u529b\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=5e-4)\n    criterion = nn.MSELoss()\n    # \u7a0d\u5f31\u7684\u65b9\u5411\u6b63\u5219\uff0c\u66f4\u597d\u517c\u987eMSE/MAE\u4e0e\u65b9\u5411\u4e00\u81f4\u6027\n    lambda_dir = 0.08\n    print(\n        f\"   \u8bad\u7ec3\u53c2\u6570\uff1aepochs={cfg.train_epochs}, lr={cfg.learning_rate}, \"\n        f\"d_model={cfg.d_model}, d_ff={cfg.d_ff}, down_layers={cfg.down_sampling_layers}\"\n    )\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, val_mae, val_mape, val_da = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        da_term = val_da if math.isfinite(val_da) else 0.5\n        val_score = val_mse + val_mae + val_mape + (1.0 - da_term)\n        print(\n            f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, \"\n            f\"val_mse={val_mse:.4f}, val_score={val_score:.4f}\"\n        )\n        if val_score < best_val:\n            best_val = val_score\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation score %.6f at epoch %d\", val_score, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    torch.save(model.state_dict(), cfg.checkpoint_dir / \"best_model.pt\")\n\n    # \u5728\u9a8c\u8bc1\u96c6\u4e0a\u8054\u5408\u641c\u7d22blend_alpha\u5e76\u62df\u5408\u7b80\u5355\u7ebf\u6027\u6821\u51c6\u53c2\u6570\n    calib_w, calib_b = compute_log_calibration(model, loaders[\"val\"], cfg, cfg.device_obj)\n\n    print(\"5) \u6d4b\u8bd5\u96c6\u8bc4\u4f30...\")\n    test_mse, test_mae, test_mape, test_da = evaluate(\n        model, loaders[\"test\"], cfg, cfg.device_obj, calibr=(calib_w, calib_b)\n    )\n    print(\n        f\"   Test metrics -> scaled_MSE: {test_mse:.4f}, scaled_MAE: {test_mae:.4f}, \"\n        f\"value_MAPE: {test_mape:.4f}, DA: {test_da:.4f}\"\n    )\n# EVOLVE-BLOCK-END\n    return test_mse, test_mae, test_mape, test_da\n\nif __name__ == \"__main__\":\n    test_mse, test_mae, test_mape, test_da = train_predict_evaluate()\n    \n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Tune blend_alpha and affine log calibration using a proxy close to final fitness.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    def _combo_score(pred_log: np.ndarray) -> float:\n        \"\"\"Lower is better: combines log/linear errors and directional accuracy.\"\"\"\n        err = pred_log - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        pred_val = np.expm1(pred_log)\n        true_val = np.expm1(trues_arr)\n        mape = float(\n            np.mean(\n                np.abs(pred_val - true_val) / np.clip(true_val, 1e-6, None)\n            )\n        )\n        da = compute_directional_accuracy(pred_val, true_val)\n        if not np.isfinite(da):\n            da = 0.5\n        return mse + mae + mape + (1.0 - da)\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # \u5728 [0,1] \u533a\u95f4\u4e0a\u4f7f\u7528 0.05 \u6b65\u957f\u641c\u7d22 alpha\uff0c\u5e76\u540c\u65f6\u6bd4\u8f83\u201c\u5e26\u7ebf\u6027\u6821\u51c6\u201d\u548c\u201c\u6052\u7b49\u6620\u5c04\u201d\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        # \u95ed\u5f0f\u89e3\u62df\u5408 log \u7a7a\u95f4\u7ebf\u6027\u6821\u51c6 y \u2248 w * blended + b\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        # \u5728\u7efc\u5408\u6307\u6807\u4e0a\u6bd4\u8f83\u201c\u5e26\u7ebf\u6027\u6821\u51c6\u201d\u548c\u201c\u6052\u7b49\u6620\u5c04\u201d\u4e24\u79cd\u65b9\u6848\uff0c\u53ea\u4fdd\u7559\u66f4\u4f18\u8005\n        calibrated = blended * float(w) + float(b)\n        score_cal = _combo_score(calibrated)\n        score_id = _combo_score(blended)\n\n        if score_id <= score_cal:\n            score = score_id\n            w, b = 1.0, 0.0\n        else:\n            score = score_cal\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n=======\ndef compute_log_calibration(\n    model: TimeMixer,\n    loader: DataLoader,\n    cfg: IronDailyConfig,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Grid\u2011search blend_alpha and simple affine log\u2011space calibration on validation data.\"\"\"\n    preds_arr, trues_arr, naive_arr = _collect_log_forecasts(model, loader, cfg, device)\n    if preds_arr is None:\n        return 1.0, 0.0\n\n    best_score = float(\"inf\")\n    best_alpha = float(getattr(cfg, \"blend_alpha\", 0.8))\n    best_w, best_b = 1.0, 0.0\n\n    # Search alpha in [0, 1] with step 0.05; for each alpha fit y \u2248 w * blended + b in closed form.\n    candidate_alphas = [i / 20.0 for i in range(0, 21)]\n    for alpha in candidate_alphas:\n        blended = alpha * preds_arr + (1.0 - alpha) * naive_arr\n\n        p = blended.reshape(-1)\n        t = trues_arr.reshape(-1)\n        mask = np.isfinite(p) & np.isfinite(t)\n        if mask.sum() < 2:\n            w, b = 1.0, 0.0\n        else:\n            p_m = float(p[mask].mean())\n            t_m = float(t[mask].mean())\n            var_p = float(((p[mask] - p_m) ** 2).mean())\n            if var_p <= 1e-12:\n                w = 1.0\n                b = t_m - p_m\n            else:\n                cov_pt = float(((p[mask] - p_m) * (t[mask] - t_m)).mean())\n                w = cov_pt / var_p\n                b = t_m - w * p_m\n\n        calibrated = blended * float(w) + float(b)\n        err = calibrated - trues_arr\n        mse = float((err ** 2).mean())\n        mae = float(np.abs(err).mean())\n        score = mse + mae\n\n        # Compare against identity mapping (no calibration) and keep the better of the two.\n        err_id = blended - trues_arr\n        mse_id = float((err_id ** 2).mean())\n        mae_id = float(np.abs(err_id).mean())\n        score_id = mse_id + mae_id\n        if score_id < score:\n            score = score_id\n            w, b = 1.0, 0.0\n\n        if score < best_score:\n            best_score = score\n            best_alpha = float(alpha)\n            best_w, best_b = float(w), float(b)\n\n    cfg.blend_alpha = best_alpha\n    return best_w, best_b\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        val_mse, val_mae, val_mape, val_da = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        da_term = val_da if math.isfinite(val_da) else 0.5\n        val_score = val_mse + val_mae + val_mape + (1.0 - da_term)\n        print(\n            f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, \"\n            f\"val_mse={val_mse:.4f}, val_score={val_score:.4f}\"\n        )\n        if val_score < best_val:\n            best_val = val_score\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation score %.6f at epoch %d\", val_score, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n=======\n    best_val = math.inf\n    best_state = None\n    patience_counter = 0\n    for epoch in range(cfg.train_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch_x, batch_y, batch_x_mark, batch_y_mark in loaders[\"train\"]:\n            batch_x = batch_x.to(cfg.device_obj)\n            batch_y = batch_y.to(cfg.device_obj)\n            batch_x_mark = batch_x_mark.to(cfg.device_obj)\n            batch_y_mark = batch_y_mark.to(cfg.device_obj)\n            # \u7b80\u5316\uff1a\u5f53\u524d\u6a21\u578b\u59cb\u7ec8\u91c7\u7528\u591a\u5c3a\u5ea6\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u4e0d\u9700\u8981\u663e\u5f0f decoder \u8f93\u5165\n            dec_inp = None\n            optimizer.zero_grad()\n            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n            pred_y, true_y = extract_target(outputs, batch_y, cfg)\n            mse_loss = criterion(pred_y, true_y)\n            # \u65b9\u5411\u635f\u5931\uff1a\u9f13\u52b1\u9884\u6d4b\u4ef7\u683c\u53d8\u5316\u65b9\u5411\u4e0e\u771f\u5b9e\u65b9\u5411\u4e00\u81f4\uff0c\u4ee5\u63d0\u9ad8DA\n            delta_pred = pred_y[:, 1:, :] - pred_y[:, :-1, :]\n            delta_true = true_y[:, 1:, :] - true_y[:, :-1, :]\n            dir_target = torch.sign(delta_true)\n            dir_loss = F.relu(-delta_pred * dir_target).mean()\n            loss = mse_loss + lambda_dir * dir_loss\n            loss.backward()\n            # \u68af\u5ea6\u88c1\u526a\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u5076\u53d1\u68af\u5ea6\u7206\u70b8\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            running_loss += loss.item()\n        avg_loss = running_loss / max(len(loaders[\"train\"]), 1)\n        # \u65e9\u505c\u76f4\u63a5\u4ee5\u9a8c\u8bc1\u96c6 MSE \u4e3a\u51c6\uff0c\u4fdd\u6301\u76ee\u6807\u7b80\u5355\u7a33\u5b9a\n        val_mse, _, _, _ = evaluate(model, loaders[\"val\"], cfg, cfg.device_obj)\n        print(f\"   Epoch {epoch + 1:02d}: train_loss={avg_loss:.4f}, val_mse={val_mse:.4f}\")\n        if val_mse < best_val:\n            best_val = val_mse\n            best_state = model.state_dict()\n            patience_counter = 0\n            logger.info(\"New best validation MSE %.6f at epoch %d\", val_mse, epoch + 1)\n        else:\n            patience_counter += 1\n            if patience_counter >= cfg.patience:\n                print(\"   \u65e9\u505c\uff1a\u9a8c\u8bc1\u96c6\u672a\u63d0\u5347\u3002\")\n                logger.info(\"Early stopping triggered at epoch %d\", epoch + 1)\n                break\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}
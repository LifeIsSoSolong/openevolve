{"iteration": 1, "timestamp": 1765777028.4644978, "parent_id": "afb2152a-f28a-4335-9a72-afa182f455f8", "child_id": "d780756d-a4a5-4cb3-abd8-cb0834e1ea5c", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.3761059571153266, "mape": 0.330863067693911, "rmse": 1218.7938495919004}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.07699578260862855, "mape": 0.22532158615306624, "rmse": 620.7588077630065}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 55.39417767524719, "changes": "Change 1: Replace 70 lines with 111 lines"}}
{"iteration": 2, "timestamp": 1765777073.7739737, "parent_id": "afb2152a-f28a-4335-9a72-afa182f455f8", "child_id": "c576ed26-a84f-4a98-8bc6-a43214a6bab7", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.37627932915514933, "mape": 0.33025112033000975, "rmse": 1217.1805431125765}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.07682241056880579, "mape": 0.224709638789165, "rmse": 619.1455012836826}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 45.33967876434326, "changes": "Change 1: Replace 70 lines with 132 lines"}}
{"iteration": 3, "timestamp": 1765777135.2997143, "parent_id": "d780756d-a4a5-4cb3-abd8-cb0834e1ea5c", "child_id": "9e02d25e-6394-4066-8248-8558cf25e3f1", "parent_metrics": {"combined_score": 0.3761059571153266, "mape": 0.330863067693911, "rmse": 1218.7938495919004}, "child_metrics": {"combined_score": 0.44915303127626865, "mape": 0.11569531989021994, "rmse": 497.99833633900636}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.3761\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.3761\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Change 1: Replace 70 lines with 111 lines\n- Metrics: combined_score: 0.3761, mape: 0.3309, rmse: 1218.7938\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.3761)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.3761), Performs well on mape (0.3309), Performs well on rmse (1218.7938)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.07304707416094208, "mape": -0.21516774780369105, "rmse": -720.7955132528941}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 61.44235610961914, "changes": "Change 1: Replace 111 lines with 98 lines"}}
{"iteration": 4, "timestamp": 1765777201.5753677, "parent_id": "afb2152a-f28a-4335-9a72-afa182f455f8", "child_id": "939939dd-d79f-42fd-b757-75a168222b7f", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.4250264999265601, "mape": 0.17833084328049664, "rmse": 715.8909632091413}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.3761 \u2192 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 111 lines\n- Metrics: combined_score: 0.3761, mape: 0.3309, rmse: 1218.7938\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 70 lines with 132 lines\n- Metrics: combined_score: 0.3763, mape: 0.3303, rmse: 1217.1805\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.3763)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.3763), Performs well on mape (0.3303), Performs well on rmse (1217.1805)\n\n### Program 3 (Score: 0.3761)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.3761), Performs well on mape (0.3309), Performs well on rmse (1218.7938)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.028075239797395013, "mape": 0.07278936173965188, "rmse": 117.85592138024742}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 66.3141188621521, "changes": "Change 1: Replace 70 lines with 106 lines"}}
{"iteration": 5, "timestamp": 1765777249.8228896, "parent_id": "afb2152a-f28a-4335-9a72-afa182f455f8", "child_id": "797261e2-09b9-4c00-9d69-dc3b5d91c11b", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.3761251625770389, "mape": 0.33079539545445713, "rmse": 1218.493510349091}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef _smooth_te(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k: float = 20.0, name: str = \"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    gm = tr[y].mean()\n    g[name] = (g[\"count\"] * g[\"mean\"] + k * gm) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [name]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [name]], on=keys, how=\"left\")\n    tr[name] = tr[name].fillna(gm)\n    te[name] = te[name].fillna(gm)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n\n    # smoothed target encodings (strong for region/season yield)\n    train, test = _smooth_te(train, test, [\"state\"], name=\"te_state\", k=30.0)\n    train, test = _smooth_te(train, test, [\"state\", \"month\"], name=\"te_state_month\", k=40.0)\n\n    # year relative\n    base_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = (train[\"year\"] - base_year).astype(int)\n    test[\"year_rel\"] = (test[\"year\"] - base_year).astype(int)\n    train[\"state_x_year\"] = train[\"state_enc\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"state_x_year\"] = test[\"state_enc\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # last-year validation for early stopping (if possible)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = None if y_va is None else np.log1p(y_va.clip(lower=0.0))\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=[\"state_enc\"],\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    else:\n        model.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=[\"state_enc\"])\n        pred_log = model.predict(test[features])\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.3763 \u2192 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 132 lines\n- Metrics: combined_score: 0.3763, mape: 0.3303, rmse: 1217.1805\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n### Program 3 (Score: 0.3763)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.3763), Performs well on mape (0.3303), Performs well on rmse (1217.1805)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3761)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4492, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 111 lines with 98 lines, Alternative mape approach, Excellent rmse (497.998)\n\n### Inspiration 2 (Score: 0.3761, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 111 lines, Excellent rmse (1218.794), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.3763, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 132 lines, Excellent rmse (1217.181), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef _smooth_te(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k: float = 20.0, name: str = \"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    gm = tr[y].mean()\n    g[name] = (g[\"count\"] * g[\"mean\"] + k * gm) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [name]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [name]], on=keys, how=\"left\")\n    tr[name] = tr[name].fillna(gm)\n    te[name] = te[name].fillna(gm)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n\n    # smoothed target encodings (strong for region/season yield)\n    train, test = _smooth_te(train, test, [\"state\"], name=\"te_state\", k=30.0)\n    train, test = _smooth_te(train, test, [\"state\", \"month\"], name=\"te_state_month\", k=40.0)\n\n    # year relative\n    base_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = (train[\"year\"] - base_year).astype(int)\n    test[\"year_rel\"] = (test[\"year\"] - base_year).astype(int)\n    train[\"state_x_year\"] = train[\"state_enc\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"state_x_year\"] = test[\"state_enc\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # last-year validation for early stopping (if possible)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = None if y_va is None else np.log1p(y_va.clip(lower=0.0))\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=[\"state_enc\"],\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    else:\n        model.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=[\"state_enc\"])\n        pred_log = model.predict(test[features])\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.07697657714691625, "mape": 0.22525391391361238, "rmse": 620.458468520197}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 48.03203535079956, "changes": "Change 1: Replace 70 lines with 126 lines"}}
{"iteration": 6, "timestamp": 1765777341.794694, "parent_id": "afb2152a-f28a-4335-9a72-afa182f455f8", "child_id": "b26f97cd-7515-4d6d-ab19-e1d7e7099dc6", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.4212240393211606, "mape": 0.18889613824809248, "rmse": 749.9130542241556}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4250 \u2192 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 106 lines\n- Metrics: combined_score: 0.4250, mape: 0.1783, rmse: 715.8910\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n### Program 3 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4250), Performs well on mape (0.1783), Performs well on rmse (715.8910)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3761)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.3763)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.3761, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 111 lines, Excellent rmse (1218.794), NumPy-based implementation\n\n### Inspiration 2 (Score: 0.4250, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 106 lines, Alternative mape approach, Excellent rmse (715.891)\n\n### Inspiration 3 (Score: 0.3763, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 132 lines, Excellent rmse (1217.181), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.03187770040279453, "mape": 0.08335465670724772, "rmse": 151.87801239526175}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 92.17318654060364, "changes": "Change 1: Replace 70 lines with 158 lines"}}
{"iteration": 7, "timestamp": 1765777393.9803786, "parent_id": "c576ed26-a84f-4a98-8bc6-a43214a6bab7", "child_id": "fee3bde7-0bdb-45f3-9c43-eb24fbf9d66b", "parent_metrics": {"combined_score": 0.37627932915514933, "mape": 0.33025112033000975, "rmse": 1217.1805431125765}, "child_metrics": {"combined_score": 0.4483966460904388, "mape": 0.11758291006577076, "rmse": 497.73008801518114}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.3763\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4250 \u2192 0.3763. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 106 lines\n- Metrics: combined_score: 0.4250, mape: 0.1783, rmse: 715.8910\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n### Program 3 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4250), Performs well on mape (0.1783), Performs well on rmse (715.8910)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3761)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef _smooth_te(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k: float = 20.0, name: str = \"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    gm = tr[y].mean()\n    g[name] = (g[\"count\"] * g[\"mean\"] + k * gm) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [name]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [name]], on=keys, how=\"left\")\n    tr[name] = tr[name].fillna(gm)\n    te[name] = te[name].fillna(gm)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n\n    # smoothed target encodings (strong for region/season yield)\n    train, test = _smooth_te(train, test, [\"state\"], name=\"te_state\", k=30.0)\n    train, test = _smooth_te(train, test, [\"state\", \"month\"], name=\"te_state_month\", k=40.0)\n\n    # year relative\n    base_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = (train[\"year\"] - base_year).astype(int)\n    test[\"year_rel\"] = (test[\"year\"] - base_year).astype(int)\n    train[\"state_x_year\"] = train[\"state_enc\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"state_x_year\"] = test[\"state_enc\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # last-year validation for early stopping (if possible)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = None if y_va is None else np.log1p(y_va.clip(lower=0.0))\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=[\"state_enc\"],\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    else:\n        model.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=[\"state_enc\"])\n        pred_log = model.predict(test[features])\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.3763)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 2 (Score: 0.4492, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 111 lines with 98 lines, Alternative mape approach, Excellent rmse (497.998)\n\n### Inspiration 3 (Score: 0.3761, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 111 lines, Excellent rmse (1218.794), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.07211731693528944, "mape": -0.212668210264239, "rmse": -719.4504550973954}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 52.20827674865723, "changes": "Change 1: Replace 132 lines with 100 lines"}}
{"iteration": 8, "timestamp": 1765777506.1307495, "parent_id": "9e02d25e-6394-4066-8248-8558cf25e3f1", "child_id": "9c8d89bf-59db-498f-a153-347bfaabfe00", "parent_metrics": {"combined_score": 0.44915303127626865, "mape": 0.11569531989021994, "rmse": 497.99833633900636}, "child_metrics": {"combined_score": 0.4211351081403701, "mape": 0.18914337310412893, "rmse": 751.6004515362335}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    unk = len(mapping)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(unk).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_loo_te(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    prefix: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leave-one-out smoothed target encoding (train) + smoothed mean encoding (test).\"\"\"\n    tr = train.copy()\n    te = test.copy()\n    global_mean = float(tr[target].mean())\n\n    stats = tr.groupby(keys)[target].agg([\"sum\", \"count\"]).reset_index()\n    stats.columns = keys + [\"_sum\", \"_cnt\"]\n\n    tr = tr.merge(stats, on=keys, how=\"left\")\n    cnt_loo = (tr[\"_cnt\"] - 1).clip(lower=0)\n    sum_loo = tr[\"_sum\"] - tr[target].astype(float)\n    mean_loo = np.where(cnt_loo.to_numpy() > 0, (sum_loo / cnt_loo).to_numpy(), np.nan)\n    te_tr = (cnt_loo.to_numpy() * np.nan_to_num(mean_loo, nan=global_mean) + smooth * global_mean) / (cnt_loo.to_numpy() + smooth)\n    tr[prefix] = np.where(np.isfinite(te_tr), te_tr, global_mean)\n\n    te = te.merge(stats, on=keys, how=\"left\")\n    mean_full = (te[\"_sum\"] / te[\"_cnt\"]).to_numpy()\n    cnt_full = te[\"_cnt\"].fillna(0.0).to_numpy()\n    te_te = (cnt_full * np.nan_to_num(mean_full, nan=global_mean) + smooth * global_mean) / (cnt_full + smooth)\n    te[prefix] = np.where(np.isfinite(te_te), te_te, global_mean)\n\n    tr = tr.drop(columns=[\"_sum\", \"_cnt\"])\n    te = te.drop(columns=[\"_sum\", \"_cnt\"])\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (LOO) target encodings\n    train, test = add_loo_te(train, test, [\"state_enc\"], smooth=40.0, prefix=\"te_state\")\n    train, test = add_loo_te(train, test, [\"month\"], smooth=25.0, prefix=\"te_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"month\"], smooth=60.0, prefix=\"te_state_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + robust NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=15000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.82,\n        subsample_freq=1,\n        colsample_bytree=0.82,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit_model(y_train, y_valid, seed: int):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n        else:\n            m.fit(train[features], y_train, categorical_feature=cat_feats)\n        return m, getattr(m, \"best_iteration_\", None)\n\n    # raw + log blend tends to improve both RMSE and MAPE\n    m_raw, it_raw = fit_model(y_tr, y_va, seed=42)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_model(y_tr_log, y_va_log, seed=7)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4492\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4250 \u2192 0.4492\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 106 lines\n- Metrics: combined_score: 0.4250, mape: 0.1783, rmse: 715.8910\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n### Program 3 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4250), Performs well on mape (0.1783), Performs well on rmse (715.8910)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3763)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4212)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 2 (Score: 0.4212, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 158 lines, Alternative mape approach, Excellent rmse (749.913)\n\n### Inspiration 3 (Score: 0.3761, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_sq\"] = df[\"year\"].astype(float) ** 2\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef _smooth_target_enc(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k=20.0, prefix=\"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    global_mean = tr[y].mean()\n    g[prefix] = (g[\"count\"] * g[\"mean\"] + k * global_mean) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [prefix]], on=keys, how=\"left\")\n    tr[prefix] = tr[prefix].fillna(global_mean)\n    te[prefix] = te[prefix].fillna(global_mean)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n    train, test = _smooth_target_enc(train, test, [\"state\"], prefix=\"te_state\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"month\"], prefix=\"te_state_month\")\n    train, test = _smooth_target_enc(train, test, [\"state\", \"months_since_crop_start\"], prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != target]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        if c in train.columns:\n            train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n            test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    y = np.log1p(train[target].astype(float).clip(lower=0.0))\n\n    max_year = train[\"year\"].max()\n    val_mask = train[\"year\"].eq(max_year) if train[\"year\"].nunique() > 1 else pd.Series([False] * len(train))\n    X_tr, y_tr = train.loc[~val_mask, features], y.loc[~val_mask]\n    X_va, y_va = train.loc[val_mask, features], y.loc[val_mask]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    if len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(250, verbose=False)],\n        )\n    else:\n        model.fit(train[features], y, categorical_feature=cat_feats)\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log).clip(min=0.0)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 111 lines, Excellent rmse (1218.794), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    unk = len(mapping)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(unk).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_loo_te(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    prefix: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leave-one-out smoothed target encoding (train) + smoothed mean encoding (test).\"\"\"\n    tr = train.copy()\n    te = test.copy()\n    global_mean = float(tr[target].mean())\n\n    stats = tr.groupby(keys)[target].agg([\"sum\", \"count\"]).reset_index()\n    stats.columns = keys + [\"_sum\", \"_cnt\"]\n\n    tr = tr.merge(stats, on=keys, how=\"left\")\n    cnt_loo = (tr[\"_cnt\"] - 1).clip(lower=0)\n    sum_loo = tr[\"_sum\"] - tr[target].astype(float)\n    mean_loo = np.where(cnt_loo.to_numpy() > 0, (sum_loo / cnt_loo).to_numpy(), np.nan)\n    te_tr = (cnt_loo.to_numpy() * np.nan_to_num(mean_loo, nan=global_mean) + smooth * global_mean) / (cnt_loo.to_numpy() + smooth)\n    tr[prefix] = np.where(np.isfinite(te_tr), te_tr, global_mean)\n\n    te = te.merge(stats, on=keys, how=\"left\")\n    mean_full = (te[\"_sum\"] / te[\"_cnt\"]).to_numpy()\n    cnt_full = te[\"_cnt\"].fillna(0.0).to_numpy()\n    te_te = (cnt_full * np.nan_to_num(mean_full, nan=global_mean) + smooth * global_mean) / (cnt_full + smooth)\n    te[prefix] = np.where(np.isfinite(te_te), te_te, global_mean)\n\n    tr = tr.drop(columns=[\"_sum\", \"_cnt\"])\n    te = te.drop(columns=[\"_sum\", \"_cnt\"])\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (LOO) target encodings\n    train, test = add_loo_te(train, test, [\"state_enc\"], smooth=40.0, prefix=\"te_state\")\n    train, test = add_loo_te(train, test, [\"month\"], smooth=25.0, prefix=\"te_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"month\"], smooth=60.0, prefix=\"te_state_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + robust NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=15000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.82,\n        subsample_freq=1,\n        colsample_bytree=0.82,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit_model(y_train, y_valid, seed: int):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n        else:\n            m.fit(train[features], y_train, categorical_feature=cat_feats)\n        return m, getattr(m, \"best_iteration_\", None)\n\n    # raw + log blend tends to improve both RMSE and MAPE\n    m_raw, it_raw = fit_model(y_tr, y_va, seed=42)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_model(y_tr_log, y_va_log, seed=7)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.02801792313589857, "mape": 0.07344805321390899, "rmse": 253.6021151972272}, "island_id": 0, "generation": 3, "metadata": {"iteration_time": 112.09643316268921, "changes": "Change 1: Replace 98 lines with 153 lines"}}
{"iteration": 9, "timestamp": 1765777594.670399, "parent_id": "afb2152a-f28a-4335-9a72-afa182f455f8", "child_id": "9695c3c7-0c5b-4a18-87ab-edc57a50b1d5", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.4226227073767802, "mape": 0.18498975892829803, "rmse": 736.3189752257789}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_group_stats(train: pd.DataFrame, test: pd.DataFrame, target: str = \"yield\", smooth: float = 30.0):\n    gm = float(train[target].mean())\n    # state mean\n    s = train.groupby(\"state\")[target].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[target].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n    train, test = add_group_stats(train, test, target=\"yield\", smooth=35.0)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.9,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    m_log, it_log = fit_lgb(7, np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.62 * p_raw + 0.38 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4484 \u2192 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 132 lines with 100 lines\n- Metrics: combined_score: 0.4484, mape: 0.1176, rmse: 497.7301\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n### Program 3 (Score: 0.4484)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4484), Performs well on mape (0.1176), Performs well on rmse (497.7301)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4212)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4212, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 158 lines, Alternative mape approach, Excellent rmse (749.913)\n\n### Inspiration 2 (Score: 0.4484, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 132 lines with 100 lines, Alternative mape approach, Excellent rmse (497.730)\n\n### Inspiration 3 (Score: 0.4250, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 106 lines, Alternative mape approach, Excellent rmse (715.891)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_group_stats(train: pd.DataFrame, test: pd.DataFrame, target: str = \"yield\", smooth: float = 30.0):\n    gm = float(train[target].mean())\n    # state mean\n    s = train.groupby(\"state\")[target].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[target].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n    train, test = add_group_stats(train, test, target=\"yield\", smooth=35.0)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.9,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    m_log, it_log = fit_lgb(7, np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.62 * p_raw + 0.38 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.030479032347174917, "mape": 0.07944827738745326, "rmse": 138.283933396885}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 88.32474613189697, "changes": "Change 1: Replace 70 lines with 128 lines"}}
{"iteration": 10, "timestamp": 1765777668.627027, "parent_id": "9c8d89bf-59db-498f-a153-347bfaabfe00", "child_id": "a86338d1-1027-4b0a-952c-0fd2438dfdde", "parent_metrics": {"combined_score": 0.4211351081403701, "mape": 0.18914337310412893, "rmse": 751.6004515362335}, "child_metrics": {"combined_score": 0.37608784844127463, "mape": 0.33093218902826127, "rmse": 1214.6332797903074}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    unk = len(mapping)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(unk).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_loo_te(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    prefix: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leave-one-out smoothed target encoding (train) + smoothed mean encoding (test).\"\"\"\n    tr = train.copy()\n    te = test.copy()\n    global_mean = float(tr[target].mean())\n\n    stats = tr.groupby(keys)[target].agg([\"sum\", \"count\"]).reset_index()\n    stats.columns = keys + [\"_sum\", \"_cnt\"]\n\n    tr = tr.merge(stats, on=keys, how=\"left\")\n    cnt_loo = (tr[\"_cnt\"] - 1).clip(lower=0)\n    sum_loo = tr[\"_sum\"] - tr[target].astype(float)\n    mean_loo = np.where(cnt_loo.to_numpy() > 0, (sum_loo / cnt_loo).to_numpy(), np.nan)\n    te_tr = (cnt_loo.to_numpy() * np.nan_to_num(mean_loo, nan=global_mean) + smooth * global_mean) / (cnt_loo.to_numpy() + smooth)\n    tr[prefix] = np.where(np.isfinite(te_tr), te_tr, global_mean)\n\n    te = te.merge(stats, on=keys, how=\"left\")\n    mean_full = (te[\"_sum\"] / te[\"_cnt\"]).to_numpy()\n    cnt_full = te[\"_cnt\"].fillna(0.0).to_numpy()\n    te_te = (cnt_full * np.nan_to_num(mean_full, nan=global_mean) + smooth * global_mean) / (cnt_full + smooth)\n    te[prefix] = np.where(np.isfinite(te_te), te_te, global_mean)\n\n    tr = tr.drop(columns=[\"_sum\", \"_cnt\"])\n    te = te.drop(columns=[\"_sum\", \"_cnt\"])\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (LOO) target encodings\n    train, test = add_loo_te(train, test, [\"state_enc\"], smooth=40.0, prefix=\"te_state\")\n    train, test = add_loo_te(train, test, [\"month\"], smooth=25.0, prefix=\"te_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"month\"], smooth=60.0, prefix=\"te_state_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + robust NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=15000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.82,\n        subsample_freq=1,\n        colsample_bytree=0.82,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit_model(y_train, y_valid, seed: int):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n        else:\n            m.fit(train[features], y_train, categorical_feature=cat_feats)\n        return m, getattr(m, \"best_iteration_\", None)\n\n    # raw + log blend tends to improve both RMSE and MAPE\n    m_raw, it_raw = fit_model(y_tr, y_va, seed=42)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_model(y_tr_log, y_va_log, seed=7)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smooth_mean(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Smoothed target mean encoding built on full train (no CV).\"\"\"\n    tr, te = train.copy(), test.copy()\n    g = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    g[out_col] = (g[\"mean\"] * g[\"count\"] + smooth * tr[target].mean()) / (g[\"count\"] + smooth)\n    g = g[keys + [out_col]]\n    tr = tr.merge(g, on=keys, how=\"left\")\n    te = te.merge(g, on=keys, how=\"left\")\n    fillv = float(tr[target].mean())\n    tr[out_col] = tr[out_col].fillna(fillv)\n    te[out_col] = te[out_col].fillna(fillv)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # simple (non-LOO) smoothed means: often improves MAPE without adding noise\n    train, test = add_smooth_mean(train, test, [\"state_enc\"], smooth=40.0, out_col=\"te_state\")\n    train, test = add_smooth_mean(train, test, [\"month\"], smooth=25.0, out_col=\"te_month\")\n    train, test = add_smooth_mean(train, test, [\"state_enc\", \"month\"], smooth=60.0, out_col=\"te_state_month\")\n    train, test = add_smooth_mean(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, out_col=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last year if present (time-aware)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.08:\n            use_val = False\n    if use_val:\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    # train in log-space to optimize relative error (MAPE-friendly)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if use_val and y_va is not None else None\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=25000,\n        learning_rate=0.015,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.80,\n        subsample_freq=1,\n        colsample_bytree=0.80,\n        reg_lambda=1.0,\n        reg_alpha=0.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and X_va is not None and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(500, verbose=False)],\n        )\n        best_it = getattr(model, \"best_iteration_\", None)\n    else:\n        model.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        best_it = None\n\n    pred = np.expm1(model.predict(test[features], num_iteration=best_it))\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4211\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4484 \u2192 0.4211. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 132 lines with 100 lines\n- Metrics: combined_score: 0.4484, mape: 0.1176, rmse: 497.7301\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n### Program 3 (Score: 0.4484)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4484), Performs well on mape (0.1176), Performs well on rmse (497.7301)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4212)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 2 (Score: 0.4212, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 158 lines, Alternative mape approach, Excellent rmse (749.913)\n\n### Inspiration 3 (Score: 0.4484, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 132 lines with 100 lines, Alternative mape approach, Excellent rmse (497.730)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    unk = len(mapping)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(unk).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_loo_te(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    prefix: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leave-one-out smoothed target encoding (train) + smoothed mean encoding (test).\"\"\"\n    tr = train.copy()\n    te = test.copy()\n    global_mean = float(tr[target].mean())\n\n    stats = tr.groupby(keys)[target].agg([\"sum\", \"count\"]).reset_index()\n    stats.columns = keys + [\"_sum\", \"_cnt\"]\n\n    tr = tr.merge(stats, on=keys, how=\"left\")\n    cnt_loo = (tr[\"_cnt\"] - 1).clip(lower=0)\n    sum_loo = tr[\"_sum\"] - tr[target].astype(float)\n    mean_loo = np.where(cnt_loo.to_numpy() > 0, (sum_loo / cnt_loo).to_numpy(), np.nan)\n    te_tr = (cnt_loo.to_numpy() * np.nan_to_num(mean_loo, nan=global_mean) + smooth * global_mean) / (cnt_loo.to_numpy() + smooth)\n    tr[prefix] = np.where(np.isfinite(te_tr), te_tr, global_mean)\n\n    te = te.merge(stats, on=keys, how=\"left\")\n    mean_full = (te[\"_sum\"] / te[\"_cnt\"]).to_numpy()\n    cnt_full = te[\"_cnt\"].fillna(0.0).to_numpy()\n    te_te = (cnt_full * np.nan_to_num(mean_full, nan=global_mean) + smooth * global_mean) / (cnt_full + smooth)\n    te[prefix] = np.where(np.isfinite(te_te), te_te, global_mean)\n\n    tr = tr.drop(columns=[\"_sum\", \"_cnt\"])\n    te = te.drop(columns=[\"_sum\", \"_cnt\"])\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (LOO) target encodings\n    train, test = add_loo_te(train, test, [\"state_enc\"], smooth=40.0, prefix=\"te_state\")\n    train, test = add_loo_te(train, test, [\"month\"], smooth=25.0, prefix=\"te_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"month\"], smooth=60.0, prefix=\"te_state_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + robust NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=15000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.82,\n        subsample_freq=1,\n        colsample_bytree=0.82,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit_model(y_train, y_valid, seed: int):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n        else:\n            m.fit(train[features], y_train, categorical_feature=cat_feats)\n        return m, getattr(m, \"best_iteration_\", None)\n\n    # raw + log blend tends to improve both RMSE and MAPE\n    m_raw, it_raw = fit_model(y_tr, y_va, seed=42)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_model(y_tr_log, y_va_log, seed=7)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    unk = len(mapping)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(unk).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_loo_te(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    prefix: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leave-one-out smoothed target encoding (train) + smoothed mean encoding (test).\"\"\"\n    tr = train.copy()\n    te = test.copy()\n    global_mean = float(tr[target].mean())\n\n    stats = tr.groupby(keys)[target].agg([\"sum\", \"count\"]).reset_index()\n    stats.columns = keys + [\"_sum\", \"_cnt\"]\n\n    tr = tr.merge(stats, on=keys, how=\"left\")\n    cnt_loo = (tr[\"_cnt\"] - 1).clip(lower=0)\n    sum_loo = tr[\"_sum\"] - tr[target].astype(float)\n    mean_loo = np.where(cnt_loo.to_numpy() > 0, (sum_loo / cnt_loo).to_numpy(), np.nan)\n    te_tr = (cnt_loo.to_numpy() * np.nan_to_num(mean_loo, nan=global_mean) + smooth * global_mean) / (cnt_loo.to_numpy() + smooth)\n    tr[prefix] = np.where(np.isfinite(te_tr), te_tr, global_mean)\n\n    te = te.merge(stats, on=keys, how=\"left\")\n    mean_full = (te[\"_sum\"] / te[\"_cnt\"]).to_numpy()\n    cnt_full = te[\"_cnt\"].fillna(0.0).to_numpy()\n    te_te = (cnt_full * np.nan_to_num(mean_full, nan=global_mean) + smooth * global_mean) / (cnt_full + smooth)\n    te[prefix] = np.where(np.isfinite(te_te), te_te, global_mean)\n\n    tr = tr.drop(columns=[\"_sum\", \"_cnt\"])\n    te = te.drop(columns=[\"_sum\", \"_cnt\"])\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (LOO) target encodings\n    train, test = add_loo_te(train, test, [\"state_enc\"], smooth=40.0, prefix=\"te_state\")\n    train, test = add_loo_te(train, test, [\"month\"], smooth=25.0, prefix=\"te_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"month\"], smooth=60.0, prefix=\"te_state_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + robust NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=15000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.82,\n        subsample_freq=1,\n        colsample_bytree=0.82,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit_model(y_train, y_valid, seed: int):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n        else:\n            m.fit(train[features], y_train, categorical_feature=cat_feats)\n        return m, getattr(m, \"best_iteration_\", None)\n\n    # raw + log blend tends to improve both RMSE and MAPE\n    m_raw, it_raw = fit_model(y_tr, y_va, seed=42)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_model(y_tr_log, y_va_log, seed=7)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smooth_mean(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Smoothed target mean encoding built on full train (no CV).\"\"\"\n    tr, te = train.copy(), test.copy()\n    g = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    g[out_col] = (g[\"mean\"] * g[\"count\"] + smooth * tr[target].mean()) / (g[\"count\"] + smooth)\n    g = g[keys + [out_col]]\n    tr = tr.merge(g, on=keys, how=\"left\")\n    te = te.merge(g, on=keys, how=\"left\")\n    fillv = float(tr[target].mean())\n    tr[out_col] = tr[out_col].fillna(fillv)\n    te[out_col] = te[out_col].fillna(fillv)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # simple (non-LOO) smoothed means: often improves MAPE without adding noise\n    train, test = add_smooth_mean(train, test, [\"state_enc\"], smooth=40.0, out_col=\"te_state\")\n    train, test = add_smooth_mean(train, test, [\"month\"], smooth=25.0, out_col=\"te_month\")\n    train, test = add_smooth_mean(train, test, [\"state_enc\", \"month\"], smooth=60.0, out_col=\"te_state_month\")\n    train, test = add_smooth_mean(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, out_col=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last year if present (time-aware)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.08:\n            use_val = False\n    if use_val:\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    # train in log-space to optimize relative error (MAPE-friendly)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if use_val and y_va is not None else None\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=25000,\n        learning_rate=0.015,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.80,\n        subsample_freq=1,\n        colsample_bytree=0.80,\n        reg_lambda=1.0,\n        reg_alpha=0.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and X_va is not None and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(500, verbose=False)],\n        )\n        best_it = getattr(model, \"best_iteration_\", None)\n    else:\n        model.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        best_it = None\n\n    pred = np.expm1(model.predict(test[features], num_iteration=best_it))\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.045047259699095454, "mape": 0.14178881592413234, "rmse": 463.0328282540738}, "island_id": 0, "generation": 4, "metadata": {"iteration_time": 74.22273397445679, "changes": "Change 1: Replace 153 lines with 133 lines"}}
{"iteration": 11, "timestamp": 1765777779.7926373, "parent_id": "9e02d25e-6394-4066-8248-8558cf25e3f1", "child_id": "9c245639-f20c-4b4b-9500-97c61456413a", "parent_metrics": {"combined_score": 0.44915303127626865, "mape": 0.11569531989021994, "rmse": 497.99833633900636}, "child_metrics": {"combined_score": 0.42093282109938507, "mape": 0.18971198345515472, "rmse": 753.1072322699193}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, GroupKFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef oof_target_encode(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    smooth: float = 25.0,\n    n_splits: int = 5,\n    seed: int = 42,\n):\n    tr = train_df[[*cols, target]].copy()\n    te = test_df[[*cols]].copy()\n    prior = float(tr[target].mean())\n\n    n_years = int(train_df[\"year\"].nunique()) if \"year\" in train_df.columns else 0\n    if n_years >= 2:\n        splitter = GroupKFold(n_splits=min(n_splits, n_years))\n        splits = splitter.split(tr, groups=train_df[\"year\"].values)\n    else:\n        splitter = KFold(n_splits=min(n_splits, len(tr)), shuffle=True, random_state=seed)\n        splits = splitter.split(tr)\n\n    oof = pd.Series(index=train_df.index, dtype=float)\n    for tr_idx, va_idx in splits:\n        fold = tr.iloc[tr_idx]\n        agg = fold.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"enc\"] = (agg[\"sum\"] + smooth * prior) / (agg[\"count\"] + smooth)\n        va = tr.iloc[va_idx][cols].merge(agg[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n        oof.iloc[va_idx] = va.values\n\n    agg_full = tr.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"enc\"] = (agg_full[\"sum\"] + smooth * prior) / (agg_full[\"count\"] + smooth)\n    te_enc = te.merge(agg_full[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n    return oof.astype(float), te_enc.astype(float)\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (OOF) target encodings\n    te_s_tr, te_s_te = oof_target_encode(train, test, [\"state\"], \"yield\", smooth=35.0, n_splits=5, seed=42)\n    train[\"te_state\"] = te_s_tr\n    test[\"te_state\"] = te_s_te\n    te_sm_tr, te_sm_te = oof_target_encode(train, test, [\"state\", \"month\"], \"yield\", smooth=60.0, n_splits=5, seed=7)\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric casting + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=15000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=18,\n            subsample=0.86,\n            subsample_freq=1,\n            colsample_bytree=0.86,\n            reg_alpha=0.05,\n            reg_lambda=1.0,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_lgb(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4492\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4484 \u2192 0.4492\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 132 lines with 100 lines\n- Metrics: combined_score: 0.4484, mape: 0.1176, rmse: 497.7301\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n### Program 3 (Score: 0.4484)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4484), Performs well on mape (0.1176), Performs well on rmse (497.7301)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4226)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_group_stats(train: pd.DataFrame, test: pd.DataFrame, target: str = \"yield\", smooth: float = 30.0):\n    gm = float(train[target].mean())\n    # state mean\n    s = train.groupby(\"state\")[target].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[target].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n    train, test = add_group_stats(train, test, target=\"yield\", smooth=35.0)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.9,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    m_log, it_log = fit_lgb(7, np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.62 * p_raw + 0.38 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 2 (Score: 0.4484, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 132 lines with 100 lines, Alternative mape approach, Excellent rmse (497.730)\n\n### Inspiration 3 (Score: 0.3763, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 132 lines, Excellent rmse (1217.181), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, GroupKFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef oof_target_encode(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    smooth: float = 25.0,\n    n_splits: int = 5,\n    seed: int = 42,\n):\n    tr = train_df[[*cols, target]].copy()\n    te = test_df[[*cols]].copy()\n    prior = float(tr[target].mean())\n\n    n_years = int(train_df[\"year\"].nunique()) if \"year\" in train_df.columns else 0\n    if n_years >= 2:\n        splitter = GroupKFold(n_splits=min(n_splits, n_years))\n        splits = splitter.split(tr, groups=train_df[\"year\"].values)\n    else:\n        splitter = KFold(n_splits=min(n_splits, len(tr)), shuffle=True, random_state=seed)\n        splits = splitter.split(tr)\n\n    oof = pd.Series(index=train_df.index, dtype=float)\n    for tr_idx, va_idx in splits:\n        fold = tr.iloc[tr_idx]\n        agg = fold.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"enc\"] = (agg[\"sum\"] + smooth * prior) / (agg[\"count\"] + smooth)\n        va = tr.iloc[va_idx][cols].merge(agg[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n        oof.iloc[va_idx] = va.values\n\n    agg_full = tr.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"enc\"] = (agg_full[\"sum\"] + smooth * prior) / (agg_full[\"count\"] + smooth)\n    te_enc = te.merge(agg_full[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n    return oof.astype(float), te_enc.astype(float)\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (OOF) target encodings\n    te_s_tr, te_s_te = oof_target_encode(train, test, [\"state\"], \"yield\", smooth=35.0, n_splits=5, seed=42)\n    train[\"te_state\"] = te_s_tr\n    test[\"te_state\"] = te_s_te\n    te_sm_tr, te_sm_te = oof_target_encode(train, test, [\"state\", \"month\"], \"yield\", smooth=60.0, n_splits=5, seed=7)\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric casting + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=15000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=18,\n            subsample=0.86,\n            subsample_freq=1,\n            colsample_bytree=0.86,\n            reg_alpha=0.05,\n            reg_lambda=1.0,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_lgb(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.028220210176883587, "mape": 0.07401666356493478, "rmse": 255.10889593091298}, "island_id": 0, "generation": 3, "metadata": {"iteration_time": 111.08153891563416, "changes": "Change 1: Replace 98 lines with 156 lines"}}
{"iteration": 12, "timestamp": 1765777845.0203497, "parent_id": "9695c3c7-0c5b-4a18-87ab-edc57a50b1d5", "child_id": "7cdeffc3-ff53-4f5b-976b-1df16d3571da", "parent_metrics": {"combined_score": 0.4226227073767802, "mape": 0.18498975892829803, "rmse": 736.3189752257789}, "child_metrics": {"combined_score": 0.4205860818323881, "mape": 0.19068537114890574, "rmse": 756.7275227547476}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_group_stats(train: pd.DataFrame, test: pd.DataFrame, target: str = \"yield\", smooth: float = 30.0):\n    gm = float(train[target].mean())\n    # state mean\n    s = train.groupby(\"state\")[target].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[target].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n    train, test = add_group_stats(train, test, target=\"yield\", smooth=35.0)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.9,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    m_log, it_log = fit_lgb(7, np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.62 * p_raw + 0.38 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        st = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(st)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"phase\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"m_sin\"], df[\"m_cos\"] = np.sin(ang), np.cos(ang)\n    df[\"st_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"st_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"phase\"].astype(float)\n    df[\"st_x_sin\"] = df[\"state_enc\"].astype(float) * df[\"m_sin\"].astype(float)\n    df[\"st_x_cos\"] = df[\"state_enc\"].astype(float) * df[\"m_cos\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, mp = encode_state(train)\n    test, _ = encode_state(test, mapping=mp)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    target = \"yield\"\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in numk]\n\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats], test[feats] = train[feats].fillna(med), test[feats].fillna(med)\n\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.10:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, feats], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, feats], train.loc[val_mask, target].astype(float)\n\n    cat = [\"state_enc\"] if \"state_enc\" in feats else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=12000,\n        learning_rate=0.02,\n        num_leaves=63,\n        min_child_samples=35,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit(ytr, yva, seed):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        m.fit(\n            X_tr, ytr,\n            eval_set=[(X_va, yva)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat,\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        return m, getattr(m, \"best_iteration_\", None)\n\n    m1, it1 = fit(y_tr, y_va, 42)\n    m2, it2 = fit(np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)), 7)\n\n    p1 = m1.predict(test[feats], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[feats], num_iteration=it2))\n    pred = np.clip(0.6 * p1 + 0.4 * p2, 0.0, None)\n\n    out = test.copy()\n    out[\"yield\"] = pred\n    out = out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4226\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4484 \u2192 0.4226. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 132 lines with 100 lines\n- Metrics: combined_score: 0.4484, mape: 0.1176, rmse: 497.7301\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n### Program 3 (Score: 0.4484)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4484), Performs well on mape (0.1176), Performs well on rmse (497.7301)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4226)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_group_stats(train: pd.DataFrame, test: pd.DataFrame, target: str = \"yield\", smooth: float = 30.0):\n    gm = float(train[target].mean())\n    # state mean\n    s = train.groupby(\"state\")[target].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[target].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n    train, test = add_group_stats(train, test, target=\"yield\", smooth=35.0)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.9,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    m_log, it_log = fit_lgb(7, np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.62 * p_raw + 0.38 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 2 (Score: 0.3761, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smooth_mean(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Smoothed target mean encoding built on full train (no CV).\"\"\"\n    tr, te = train.copy(), test.copy()\n    g = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    g[out_col] = (g[\"mean\"] * g[\"count\"] + smooth * tr[target].mean()) / (g[\"count\"] + smooth)\n    g = g[keys + [out_col]]\n    tr = tr.merge(g, on=keys, how=\"left\")\n    te = te.merge(g, on=keys, how=\"left\")\n    fillv = float(tr[target].mean())\n    tr[out_col] = tr[out_col].fillna(fillv)\n    te[out_col] = te[out_col].fillna(fillv)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # simple (non-LOO) smoothed means: often improves MAPE without adding noise\n    train, test = add_smooth_mean(train, test, [\"state_enc\"], smooth=40.0, out_col=\"te_state\")\n    train, test = add_smooth_mean(train, test, [\"month\"], smooth=25.0, out_col=\"te_month\")\n    train, test = add_smooth_mean(train, test, [\"state_enc\", \"month\"], smooth=60.0, out_col=\"te_state_month\")\n    train, test = add_smooth_mean(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, out_col=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last year if present (time-aware)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.08:\n            use_val = False\n    if use_val:\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    # train in log-space to optimize relative error (MAPE-friendly)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if use_val and y_va is not None else None\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=25000,\n        learning_rate=0.015,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.80,\n        subsample_freq=1,\n        colsample_bytree=0.80,\n        reg_lambda=1.0,\n        reg_alpha=0.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and X_va is not None and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_feats,\n            callbacks=[lgb.early_stopping(500, verbose=False)],\n        )\n        best_it = getattr(model, \"best_iteration_\", None)\n    else:\n        model.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        best_it = None\n\n    pred = np.expm1(model.predict(test[features], num_iteration=best_it))\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 153 lines with 133 lines, Excellent rmse (1214.633), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.4250, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 106 lines, Alternative mape approach, Excellent rmse (715.891)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_group_stats(train: pd.DataFrame, test: pd.DataFrame, target: str = \"yield\", smooth: float = 30.0):\n    gm = float(train[target].mean())\n    # state mean\n    s = train.groupby(\"state\")[target].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[target].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n    train, test = add_group_stats(train, test, target=\"yield\", smooth=35.0)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.9,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    m_log, it_log = fit_lgb(7, np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.62 * p_raw + 0.38 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_group_stats(train: pd.DataFrame, test: pd.DataFrame, target: str = \"yield\", smooth: float = 30.0):\n    gm = float(train[target].mean())\n    # state mean\n    s = train.groupby(\"state\")[target].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[target].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n    train, test = add_group_stats(train, test, target=\"yield\", smooth=35.0)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.9,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    m_log, it_log = fit_lgb(7, np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.62 * p_raw + 0.38 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        st = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(st)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"phase\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"m_sin\"], df[\"m_cos\"] = np.sin(ang), np.cos(ang)\n    df[\"st_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"st_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"phase\"].astype(float)\n    df[\"st_x_sin\"] = df[\"state_enc\"].astype(float) * df[\"m_sin\"].astype(float)\n    df[\"st_x_cos\"] = df[\"state_enc\"].astype(float) * df[\"m_cos\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, mp = encode_state(train)\n    test, _ = encode_state(test, mapping=mp)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    target = \"yield\"\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in numk]\n\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats], test[feats] = train[feats].fillna(med), test[feats].fillna(med)\n\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.10:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, feats], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, feats], train.loc[val_mask, target].astype(float)\n\n    cat = [\"state_enc\"] if \"state_enc\" in feats else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=12000,\n        learning_rate=0.02,\n        num_leaves=63,\n        min_child_samples=35,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit(ytr, yva, seed):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        m.fit(\n            X_tr, ytr,\n            eval_set=[(X_va, yva)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat,\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        return m, getattr(m, \"best_iteration_\", None)\n\n    m1, it1 = fit(y_tr, y_va, 42)\n    m2, it2 = fit(np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)), 7)\n\n    p1 = m1.predict(test[feats], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[feats], num_iteration=it2))\n    pred = np.clip(0.6 * p1 + 0.4 * p2, 0.0, None)\n\n    out = test.copy()\n    out[\"yield\"] = pred\n    out = out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.002036625544392101, "mape": 0.005695612220607715, "rmse": 20.40854752896871}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 65.28463959693909, "changes": "Change 1: Replace 128 lines with 102 lines"}}
{"iteration": 13, "timestamp": 1765778172.830992, "parent_id": "9c245639-f20c-4b4b-9500-97c61456413a", "child_id": "d2b7b272-c2fe-4d71-98f1-601147a84a2c", "parent_metrics": {"combined_score": 0.42093282109938507, "mape": 0.18971198345515472, "rmse": 753.1072322699193}, "child_metrics": {"combined_score": 0.45205899795335663, "mape": 0.10798226720139836, "rmse": 633.3263732758448}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, GroupKFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef oof_target_encode(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    smooth: float = 25.0,\n    n_splits: int = 5,\n    seed: int = 42,\n):\n    tr = train_df[[*cols, target]].copy()\n    te = test_df[[*cols]].copy()\n    prior = float(tr[target].mean())\n\n    n_years = int(train_df[\"year\"].nunique()) if \"year\" in train_df.columns else 0\n    if n_years >= 2:\n        splitter = GroupKFold(n_splits=min(n_splits, n_years))\n        splits = splitter.split(tr, groups=train_df[\"year\"].values)\n    else:\n        splitter = KFold(n_splits=min(n_splits, len(tr)), shuffle=True, random_state=seed)\n        splits = splitter.split(tr)\n\n    oof = pd.Series(index=train_df.index, dtype=float)\n    for tr_idx, va_idx in splits:\n        fold = tr.iloc[tr_idx]\n        agg = fold.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"enc\"] = (agg[\"sum\"] + smooth * prior) / (agg[\"count\"] + smooth)\n        va = tr.iloc[va_idx][cols].merge(agg[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n        oof.iloc[va_idx] = va.values\n\n    agg_full = tr.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"enc\"] = (agg_full[\"sum\"] + smooth * prior) / (agg_full[\"count\"] + smooth)\n    te_enc = te.merge(agg_full[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n    return oof.astype(float), te_enc.astype(float)\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (OOF) target encodings\n    te_s_tr, te_s_te = oof_target_encode(train, test, [\"state\"], \"yield\", smooth=35.0, n_splits=5, seed=42)\n    train[\"te_state\"] = te_s_tr\n    test[\"te_state\"] = te_s_te\n    te_sm_tr, te_sm_te = oof_target_encode(train, test, [\"state\", \"month\"], \"yield\", smooth=60.0, n_splits=5, seed=7)\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric casting + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=15000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=18,\n            subsample=0.86,\n            subsample_freq=1,\n            colsample_bytree=0.86,\n            reg_alpha=0.05,\n            reg_lambda=1.0,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_lgb(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 40.0):\n    gm = float(train[\"yield\"].mean())\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # simple (non-OOF) smoothed region/season baselines (often strong)\n    train, test = add_smoothed_means(train, test, smooth=55.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # tune iterations on last-year holdout, then refit on full data\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_then_refit(seed: int, ytr: pd.Series, yva: pd.Series | None, is_log: bool):\n        params = dict(\n            objective=\"regression\",\n            n_estimators=8000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=25,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        best_it = None\n        if len(X_va) > 0:\n            m = lgb.LGBMRegressor(**params)\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        n_est = max(1200, best_it) if best_it else params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**params, \"n_estimators\": n_est})\n        m_full.fit(train[features], (np.log1p(train[target].clip(lower=0.0)) if is_log else train[target]).astype(float),\n                   categorical_feature=cat_feats)\n        return m_full\n\n    m_raw = fit_then_refit(42, y_tr, y_va if len(X_va) > 0 else None, is_log=False)\n    m_log = fit_then_refit(7, np.log1p(y_tr.clip(lower=0.0)),\n                           np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None, is_log=True)\n\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = 0.65 * p_raw + 0.35 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4209\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4484 \u2192 0.4209. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 132 lines with 100 lines\n- Metrics: combined_score: 0.4484, mape: 0.1176, rmse: 497.7301\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n### Program 3 (Score: 0.4484)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4484), Performs well on mape (0.1176), Performs well on rmse (497.7301)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4226)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_group_stats(train: pd.DataFrame, test: pd.DataFrame, target: str = \"yield\", smooth: float = 30.0):\n    gm = float(train[target].mean())\n    # state mean\n    s = train.groupby(\"state\")[target].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[target].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n    train, test = add_group_stats(train, test, target=\"yield\", smooth=35.0)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.9,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    m_log, it_log = fit_lgb(7, np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.62 * p_raw + 0.38 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4212, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 158 lines, Alternative mape approach, Excellent rmse (749.913)\n\n### Inspiration 2 (Score: 0.4250, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 106 lines, Alternative mape approach, Excellent rmse (715.891)\n\n### Inspiration 3 (Score: 0.3763, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2))\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # cyclical month features\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    # interactions (help trees capture seasonality per state)\n    if \"state_enc\" in df.columns and \"months_since_crop_start\" in df.columns:\n        df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, target: str,\n                      n_splits: int = 5, seed: int = 42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Leakage-reduced target mean encoding for a single column.\"\"\"\n    tr = train_df[[col, target]].copy()\n    te = test_df[[col]].copy()\n    global_mean = tr[target].mean()\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = pd.Series(index=train_df.index, dtype=float)\n\n    for tr_idx, va_idx in kf.split(tr):\n        fold_tr = tr.iloc[tr_idx]\n        means = fold_tr.groupby(col)[target].mean()\n        oof.iloc[va_idx] = tr.iloc[va_idx][col].map(means)\n\n    oof = oof.fillna(global_mean)\n    full_means = tr.groupby(col)[target].mean()\n    te_enc = te[col].map(full_means).fillna(global_mean)\n    return oof, te_enc\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encoding on state (often strong for yield by region)\n    tr_state_mean, te_state_mean = kfold_target_mean(train, test, \"state\", \"yield\", n_splits=5, seed=42)\n    train[\"state_yield_te\"] = tr_state_mean\n    test[\"state_yield_te\"] = te_state_mean\n\n    # year normalization (helps generalization across time)\n    min_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = train[\"year\"] - min_year\n    test[\"year_rel\"] = test[\"year\"] - min_year\n\n    # simple interaction terms\n    train[\"phase_x_year\"] = train[\"months_since_crop_start\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"phase_x_year\"] = test[\"months_since_crop_start\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    # feature selection (numeric only; keep engineered numeric features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # validation split: last year holdout if possible, else random split\n    last_year = train[\"year\"].max()\n    is_val = train[\"year\"] == last_year\n    if is_val.mean() < 0.1 or is_val.mean() > 0.6:\n        rng = np.random.default_rng(42)\n        is_val = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, target]\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, target]\n\n    # log1p target to reduce relative error (often helps MAPE)\n    y_tr_log = np.log1p(y_tr.clip(lower=0))\n    y_va_log = np.log1p(y_va.clip(lower=0))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.3,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr, y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    # predict (invert log transform)\n    test_pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 132 lines, Excellent rmse (1217.181), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, GroupKFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef oof_target_encode(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    smooth: float = 25.0,\n    n_splits: int = 5,\n    seed: int = 42,\n):\n    tr = train_df[[*cols, target]].copy()\n    te = test_df[[*cols]].copy()\n    prior = float(tr[target].mean())\n\n    n_years = int(train_df[\"year\"].nunique()) if \"year\" in train_df.columns else 0\n    if n_years >= 2:\n        splitter = GroupKFold(n_splits=min(n_splits, n_years))\n        splits = splitter.split(tr, groups=train_df[\"year\"].values)\n    else:\n        splitter = KFold(n_splits=min(n_splits, len(tr)), shuffle=True, random_state=seed)\n        splits = splitter.split(tr)\n\n    oof = pd.Series(index=train_df.index, dtype=float)\n    for tr_idx, va_idx in splits:\n        fold = tr.iloc[tr_idx]\n        agg = fold.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"enc\"] = (agg[\"sum\"] + smooth * prior) / (agg[\"count\"] + smooth)\n        va = tr.iloc[va_idx][cols].merge(agg[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n        oof.iloc[va_idx] = va.values\n\n    agg_full = tr.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"enc\"] = (agg_full[\"sum\"] + smooth * prior) / (agg_full[\"count\"] + smooth)\n    te_enc = te.merge(agg_full[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n    return oof.astype(float), te_enc.astype(float)\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (OOF) target encodings\n    te_s_tr, te_s_te = oof_target_encode(train, test, [\"state\"], \"yield\", smooth=35.0, n_splits=5, seed=42)\n    train[\"te_state\"] = te_s_tr\n    test[\"te_state\"] = te_s_te\n    te_sm_tr, te_sm_te = oof_target_encode(train, test, [\"state\", \"month\"], \"yield\", smooth=60.0, n_splits=5, seed=7)\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric casting + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=15000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=18,\n            subsample=0.86,\n            subsample_freq=1,\n            colsample_bytree=0.86,\n            reg_alpha=0.05,\n            reg_lambda=1.0,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_lgb(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, GroupKFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef oof_target_encode(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    smooth: float = 25.0,\n    n_splits: int = 5,\n    seed: int = 42,\n):\n    tr = train_df[[*cols, target]].copy()\n    te = test_df[[*cols]].copy()\n    prior = float(tr[target].mean())\n\n    n_years = int(train_df[\"year\"].nunique()) if \"year\" in train_df.columns else 0\n    if n_years >= 2:\n        splitter = GroupKFold(n_splits=min(n_splits, n_years))\n        splits = splitter.split(tr, groups=train_df[\"year\"].values)\n    else:\n        splitter = KFold(n_splits=min(n_splits, len(tr)), shuffle=True, random_state=seed)\n        splits = splitter.split(tr)\n\n    oof = pd.Series(index=train_df.index, dtype=float)\n    for tr_idx, va_idx in splits:\n        fold = tr.iloc[tr_idx]\n        agg = fold.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"enc\"] = (agg[\"sum\"] + smooth * prior) / (agg[\"count\"] + smooth)\n        va = tr.iloc[va_idx][cols].merge(agg[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n        oof.iloc[va_idx] = va.values\n\n    agg_full = tr.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"enc\"] = (agg_full[\"sum\"] + smooth * prior) / (agg_full[\"count\"] + smooth)\n    te_enc = te.merge(agg_full[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n    return oof.astype(float), te_enc.astype(float)\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (OOF) target encodings\n    te_s_tr, te_s_te = oof_target_encode(train, test, [\"state\"], \"yield\", smooth=35.0, n_splits=5, seed=42)\n    train[\"te_state\"] = te_s_tr\n    test[\"te_state\"] = te_s_te\n    te_sm_tr, te_sm_te = oof_target_encode(train, test, [\"state\", \"month\"], \"yield\", smooth=60.0, n_splits=5, seed=7)\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric casting + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=15000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=18,\n            subsample=0.86,\n            subsample_freq=1,\n            colsample_bytree=0.86,\n            reg_alpha=0.05,\n            reg_lambda=1.0,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_lgb(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 40.0):\n    gm = float(train[\"yield\"].mean())\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # simple (non-OOF) smoothed region/season baselines (often strong)\n    train, test = add_smoothed_means(train, test, smooth=55.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # tune iterations on last-year holdout, then refit on full data\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_then_refit(seed: int, ytr: pd.Series, yva: pd.Series | None, is_log: bool):\n        params = dict(\n            objective=\"regression\",\n            n_estimators=8000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=25,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        best_it = None\n        if len(X_va) > 0:\n            m = lgb.LGBMRegressor(**params)\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        n_est = max(1200, best_it) if best_it else params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**params, \"n_estimators\": n_est})\n        m_full.fit(train[features], (np.log1p(train[target].clip(lower=0.0)) if is_log else train[target]).astype(float),\n                   categorical_feature=cat_feats)\n        return m_full\n\n    m_raw = fit_then_refit(42, y_tr, y_va if len(X_va) > 0 else None, is_log=False)\n    m_log = fit_then_refit(7, np.log1p(y_tr.clip(lower=0.0)),\n                           np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None, is_log=True)\n\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = 0.65 * p_raw + 0.35 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.031126176853971566, "mape": -0.08172971625375636, "rmse": -119.78085899407449}, "island_id": 0, "generation": 4, "metadata": {"iteration_time": 327.5698072910309, "changes": "Change 1: Replace 156 lines with 131 lines"}}
{"iteration": 14, "timestamp": 1765778234.4951122, "parent_id": "939939dd-d79f-42fd-b757-75a168222b7f", "child_id": "2d83271d-621f-472f-a7ca-e268c0eaeea5", "parent_metrics": {"combined_score": 0.4250264999265601, "mape": 0.17833084328049664, "rmse": 715.8909632091413}, "child_metrics": {"combined_score": 0.3760810485765266, "mape": 0.3309547368775683, "rmse": 1215.9217397949915}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        st = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(st)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"phase\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"st_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"st_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"phase\"].astype(float)\n    return df\n\ndef smooth_te(train: pd.DataFrame, test: pd.DataFrame, keys, y: str = \"yield\", k: float = 40.0, name: str = \"te\"):\n    gm = float(train[y].mean())\n    g = train.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    g[name] = (g[\"count\"] * g[\"mean\"] + k * gm) / (g[\"count\"] + k)\n    train = train.merge(g[keys + [name]], on=keys, how=\"left\")\n    test = test.merge(g[keys + [name]], on=keys, how=\"left\")\n    train[name] = train[name].fillna(gm)\n    test[name] = test[name].fillna(gm)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, mp = encode_state(train)\n    test, _ = encode_state(test, mapping=mp)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # smoothed target encodings (strong seasonal + regional prior)\n    train, test = smooth_te(train, test, [\"state\"], k=50.0, name=\"te_state\")\n    train, test = smooth_te(train, test, [\"state\", \"month\"], k=60.0, name=\"te_state_month\")\n\n    target = \"yield\"\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in numk]\n\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats] = train[feats].fillna(med)\n    test[feats] = test[feats].fillna(med)\n\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, feats], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, feats], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[feats], train[target].astype(float)\n        X_va = y_va = None\n\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = None if y_va is None else np.log1p(y_va.clip(lower=0.0))\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=12000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.9,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=[\"state_enc\"] if \"state_enc\" in feats else \"auto\",\n            callbacks=[lgb.early_stopping(350, verbose=False)],\n        )\n        pred_log = model.predict(test[feats], num_iteration=getattr(model, \"best_iteration_\", None))\n    else:\n        model.fit(train[feats], np.log1p(train[target].astype(float).clip(lower=0.0)),\n                  categorical_feature=[\"state_enc\"] if \"state_enc\" in feats else \"auto\")\n        pred_log = model.predict(test[feats])\n\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4250\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4484 \u2192 0.4250. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 132 lines with 100 lines\n- Metrics: combined_score: 0.4484, mape: 0.1176, rmse: 497.7301\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n### Program 3 (Score: 0.4484)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4484), Performs well on mape (0.1176), Performs well on rmse (497.7301)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4226)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_group_stats(train: pd.DataFrame, test: pd.DataFrame, target: str = \"yield\", smooth: float = 30.0):\n    gm = float(train[target].mean())\n    # state mean\n    s = train.groupby(\"state\")[target].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[target].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n    train, test = add_group_stats(train, test, target=\"yield\", smooth=35.0)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.9,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    m_log, it_log = fit_lgb(7, np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.62 * p_raw + 0.38 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 2 (Score: 0.3761, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef _smooth_te(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k: float = 20.0, name: str = \"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    gm = tr[y].mean()\n    g[name] = (g[\"count\"] * g[\"mean\"] + k * gm) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [name]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [name]], on=keys, how=\"left\")\n    tr[name] = tr[name].fillna(gm)\n    te[name] = te[name].fillna(gm)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n\n    # smoothed target encodings (strong for region/season yield)\n    train, test = _smooth_te(train, test, [\"state\"], name=\"te_state\", k=30.0)\n    train, test = _smooth_te(train, test, [\"state\", \"month\"], name=\"te_state_month\", k=40.0)\n\n    # year relative\n    base_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = (train[\"year\"] - base_year).astype(int)\n    test[\"year_rel\"] = (test[\"year\"] - base_year).astype(int)\n    train[\"state_x_year\"] = train[\"state_enc\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"state_x_year\"] = test[\"state_enc\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # last-year validation for early stopping (if possible)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = None if y_va is None else np.log1p(y_va.clip(lower=0.0))\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=[\"state_enc\"],\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    else:\n        model.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=[\"state_enc\"])\n        pred_log = model.predict(test[features])\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 126 lines, Excellent rmse (1218.494), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.4206, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        st = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(st)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"phase\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"m_sin\"], df[\"m_cos\"] = np.sin(ang), np.cos(ang)\n    df[\"st_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"st_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"phase\"].astype(float)\n    df[\"st_x_sin\"] = df[\"state_enc\"].astype(float) * df[\"m_sin\"].astype(float)\n    df[\"st_x_cos\"] = df[\"state_enc\"].astype(float) * df[\"m_cos\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, mp = encode_state(train)\n    test, _ = encode_state(test, mapping=mp)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    target = \"yield\"\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in numk]\n\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats], test[feats] = train[feats].fillna(med), test[feats].fillna(med)\n\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.10:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, feats], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, feats], train.loc[val_mask, target].astype(float)\n\n    cat = [\"state_enc\"] if \"state_enc\" in feats else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=12000,\n        learning_rate=0.02,\n        num_leaves=63,\n        min_child_samples=35,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit(ytr, yva, seed):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        m.fit(\n            X_tr, ytr,\n            eval_set=[(X_va, yva)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat,\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        return m, getattr(m, \"best_iteration_\", None)\n\n    m1, it1 = fit(y_tr, y_va, 42)\n    m2, it2 = fit(np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)), 7)\n\n    p1 = m1.predict(test[feats], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[feats], num_iteration=it2))\n    pred = np.clip(0.6 * p1 + 0.4 * p2, 0.0, None)\n\n    out = test.copy()\n    out[\"yield\"] = pred\n    out = out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 128 lines with 102 lines, Alternative mape approach, Excellent rmse (756.728)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        st = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(st)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"phase\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"st_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"st_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"phase\"].astype(float)\n    return df\n\ndef smooth_te(train: pd.DataFrame, test: pd.DataFrame, keys, y: str = \"yield\", k: float = 40.0, name: str = \"te\"):\n    gm = float(train[y].mean())\n    g = train.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    g[name] = (g[\"count\"] * g[\"mean\"] + k * gm) / (g[\"count\"] + k)\n    train = train.merge(g[keys + [name]], on=keys, how=\"left\")\n    test = test.merge(g[keys + [name]], on=keys, how=\"left\")\n    train[name] = train[name].fillna(gm)\n    test[name] = test[name].fillna(gm)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, mp = encode_state(train)\n    test, _ = encode_state(test, mapping=mp)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # smoothed target encodings (strong seasonal + regional prior)\n    train, test = smooth_te(train, test, [\"state\"], k=50.0, name=\"te_state\")\n    train, test = smooth_te(train, test, [\"state\", \"month\"], k=60.0, name=\"te_state_month\")\n\n    target = \"yield\"\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in numk]\n\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats] = train[feats].fillna(med)\n    test[feats] = test[feats].fillna(med)\n\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, feats], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, feats], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[feats], train[target].astype(float)\n        X_va = y_va = None\n\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = None if y_va is None else np.log1p(y_va.clip(lower=0.0))\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=12000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.9,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=[\"state_enc\"] if \"state_enc\" in feats else \"auto\",\n            callbacks=[lgb.early_stopping(350, verbose=False)],\n        )\n        pred_log = model.predict(test[feats], num_iteration=getattr(model, \"best_iteration_\", None))\n    else:\n        model.fit(train[feats], np.log1p(train[target].astype(float).clip(lower=0.0)),\n                  categorical_feature=[\"state_enc\"] if \"state_enc\" in feats else \"auto\")\n        pred_log = model.predict(test[feats])\n\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.04894545135003353, "mape": 0.15262389359707168, "rmse": 500.03077658585016}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 61.643471479415894, "changes": "Change 1: Replace 106 lines with 115 lines"}}
{"iteration": 15, "timestamp": 1765778486.8721108, "parent_id": "d2b7b272-c2fe-4d71-98f1-601147a84a2c", "child_id": "29d06bf7-d3f3-4558-9ea1-f9e80e79abfe", "parent_metrics": {"combined_score": 0.45205899795335663, "mape": 0.10798226720139836, "rmse": 633.3263732758448}, "child_metrics": {"combined_score": 0.0, "error": "Length of labels differs from the length of #data"}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 40.0):\n    gm = float(train[\"yield\"].mean())\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # simple (non-OOF) smoothed region/season baselines (often strong)\n    train, test = add_smoothed_means(train, test, smooth=55.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # tune iterations on last-year holdout, then refit on full data\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_then_refit(seed: int, ytr: pd.Series, yva: pd.Series | None, is_log: bool):\n        params = dict(\n            objective=\"regression\",\n            n_estimators=8000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=25,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        best_it = None\n        if len(X_va) > 0:\n            m = lgb.LGBMRegressor(**params)\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        n_est = max(1200, best_it) if best_it else params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**params, \"n_estimators\": n_est})\n        m_full.fit(train[features], (np.log1p(train[target].clip(lower=0.0)) if is_log else train[target]).astype(float),\n                   categorical_feature=cat_feats)\n        return m_full\n\n    m_raw = fit_then_refit(42, y_tr, y_va if len(X_va) > 0 else None, is_log=False)\n    m_log = fit_then_refit(7, np.log1p(y_tr.clip(lower=0.0)),\n                           np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None, is_log=True)\n\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = 0.65 * p_raw + 0.35 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        u = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(u)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"phase\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m.astype(float) / 12.0)\n    df[\"m_sin\"] = np.sin(ang)\n    df[\"m_cos\"] = np.cos(ang)\n    df[\"y_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"st_x_y\"] = df[\"state_enc\"].astype(float) * df[\"y_rel\"].astype(float)\n    df[\"st_x_p\"] = df[\"state_enc\"].astype(float) * df[\"phase\"].astype(float)\n    return df\n\ndef _prep_num(train: pd.DataFrame, test: pd.DataFrame, target: str):\n    kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in kinds]\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats] = train[feats].fillna(med)\n    test[feats] = test[feats].fillna(med)\n    return feats, train, test\n\ndef _mape(y, p):\n    y = np.asarray(y, dtype=float)\n    p = np.asarray(p, dtype=float)\n    den = np.maximum(np.abs(y), 1.0)\n    return float(np.mean(np.abs(y - p) / den))\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    target = \"yield\"\n    features, train, test = _prep_num(train, test, target)\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.20\n    else:\n        val_mask = rng.random(len(train)) < 0.20\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    base_params = dict(\n        objective=\"regression\",\n        n_estimators=12000,\n        learning_rate=0.025,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.9,\n        extra_trees=True,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit_model(seed: int, ytr, yva):\n        m = lgb.LGBMRegressor(**{**base_params, \"random_state\": seed})\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(300, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            best_it = 0\n        n_est = max(1200, best_it) if best_it else base_params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**base_params, \"random_state\": seed, \"n_estimators\": n_est})\n        m_full.fit(train[features], ytr, categorical_feature=cat_feats)\n        return m, m_full\n\n    m_raw_val, m_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log_val, m_log = fit_model(7, y_tr_log, y_va_log)\n\n    w = 0.70\n    if len(X_va) > 0:\n        p_va_raw = np.clip(m_raw_val.predict(X_va), 0.0, None)\n        p_va_log = np.clip(np.expm1(m_log_val.predict(X_va)), 0.0, None)\n        ym = float(np.mean(np.abs(y_va))) + 1e-9\n        best_s, best_w = 1e18, w\n        for ww in np.linspace(0.0, 1.0, 11):\n            pv = ww * p_va_raw + (1.0 - ww) * p_va_log\n            s = _mape(y_va, pv) + (np.sqrt(np.mean((y_va - pv) ** 2)) / ym)\n            if s < best_s:\n                best_s, best_w = s, float(ww)\n        w = best_w\n\n    p_raw = np.clip(m_raw.predict(test[features]), 0.0, None)\n    p_log = np.clip(np.expm1(m_log.predict(test[features])), 0.0, None)\n    pred = np.clip(w * p_raw + (1.0 - w) * p_log, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4521\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4492 \u2192 0.4521\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 156 lines with 131 lines\n- Metrics: combined_score: 0.4521, mape: 0.1080, rmse: 633.3264\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4521)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 40.0):\n    gm = float(train[\"yield\"].mean())\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # simple (non-OOF) smoothed region/season baselines (often strong)\n    train, test = add_smoothed_means(train, test, smooth=55.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # tune iterations on last-year holdout, then refit on full data\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_then_refit(seed: int, ytr: pd.Series, yva: pd.Series | None, is_log: bool):\n        params = dict(\n            objective=\"regression\",\n            n_estimators=8000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=25,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        best_it = None\n        if len(X_va) > 0:\n            m = lgb.LGBMRegressor(**params)\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        n_est = max(1200, best_it) if best_it else params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**params, \"n_estimators\": n_est})\n        m_full.fit(train[features], (np.log1p(train[target].clip(lower=0.0)) if is_log else train[target]).astype(float),\n                   categorical_feature=cat_feats)\n        return m_full\n\n    m_raw = fit_then_refit(42, y_tr, y_va if len(X_va) > 0 else None, is_log=False)\n    m_log = fit_then_refit(7, np.log1p(y_tr.clip(lower=0.0)),\n                           np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None, is_log=True)\n\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = 0.65 * p_raw + 0.35 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4521), Performs well on mape (0.1080), Performs well on rmse (633.3264)\n\n### Program 3 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4484)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.3761, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef _smooth_te(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k: float = 20.0, name: str = \"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    gm = tr[y].mean()\n    g[name] = (g[\"count\"] * g[\"mean\"] + k * gm) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [name]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [name]], on=keys, how=\"left\")\n    tr[name] = tr[name].fillna(gm)\n    te[name] = te[name].fillna(gm)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n\n    # smoothed target encodings (strong for region/season yield)\n    train, test = _smooth_te(train, test, [\"state\"], name=\"te_state\", k=30.0)\n    train, test = _smooth_te(train, test, [\"state\", \"month\"], name=\"te_state_month\", k=40.0)\n\n    # year relative\n    base_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = (train[\"year\"] - base_year).astype(int)\n    test[\"year_rel\"] = (test[\"year\"] - base_year).astype(int)\n    train[\"state_x_year\"] = train[\"state_enc\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"state_x_year\"] = test[\"state_enc\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # last-year validation for early stopping (if possible)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = None if y_va is None else np.log1p(y_va.clip(lower=0.0))\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=[\"state_enc\"],\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    else:\n        model.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=[\"state_enc\"])\n        pred_log = model.predict(test[features])\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 126 lines, Excellent rmse (1218.494), NumPy-based implementation\n\n### Inspiration 2 (Score: 0.4209, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, GroupKFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef oof_target_encode(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    smooth: float = 25.0,\n    n_splits: int = 5,\n    seed: int = 42,\n):\n    tr = train_df[[*cols, target]].copy()\n    te = test_df[[*cols]].copy()\n    prior = float(tr[target].mean())\n\n    n_years = int(train_df[\"year\"].nunique()) if \"year\" in train_df.columns else 0\n    if n_years >= 2:\n        splitter = GroupKFold(n_splits=min(n_splits, n_years))\n        splits = splitter.split(tr, groups=train_df[\"year\"].values)\n    else:\n        splitter = KFold(n_splits=min(n_splits, len(tr)), shuffle=True, random_state=seed)\n        splits = splitter.split(tr)\n\n    oof = pd.Series(index=train_df.index, dtype=float)\n    for tr_idx, va_idx in splits:\n        fold = tr.iloc[tr_idx]\n        agg = fold.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"enc\"] = (agg[\"sum\"] + smooth * prior) / (agg[\"count\"] + smooth)\n        va = tr.iloc[va_idx][cols].merge(agg[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n        oof.iloc[va_idx] = va.values\n\n    agg_full = tr.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"enc\"] = (agg_full[\"sum\"] + smooth * prior) / (agg_full[\"count\"] + smooth)\n    te_enc = te.merge(agg_full[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n    return oof.astype(float), te_enc.astype(float)\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (OOF) target encodings\n    te_s_tr, te_s_te = oof_target_encode(train, test, [\"state\"], \"yield\", smooth=35.0, n_splits=5, seed=42)\n    train[\"te_state\"] = te_s_tr\n    test[\"te_state\"] = te_s_te\n    te_sm_tr, te_sm_te = oof_target_encode(train, test, [\"state\", \"month\"], \"yield\", smooth=60.0, n_splits=5, seed=7)\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric casting + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=15000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=18,\n            subsample=0.86,\n            subsample_freq=1,\n            colsample_bytree=0.86,\n            reg_alpha=0.05,\n            reg_lambda=1.0,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_lgb(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 98 lines with 156 lines, Alternative mape approach, Excellent rmse (753.107)\n\n### Inspiration 3 (Score: 0.4250, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 106 lines, Alternative mape approach, Excellent rmse (715.891)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 40.0):\n    gm = float(train[\"yield\"].mean())\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # simple (non-OOF) smoothed region/season baselines (often strong)\n    train, test = add_smoothed_means(train, test, smooth=55.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # tune iterations on last-year holdout, then refit on full data\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_then_refit(seed: int, ytr: pd.Series, yva: pd.Series | None, is_log: bool):\n        params = dict(\n            objective=\"regression\",\n            n_estimators=8000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=25,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        best_it = None\n        if len(X_va) > 0:\n            m = lgb.LGBMRegressor(**params)\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        n_est = max(1200, best_it) if best_it else params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**params, \"n_estimators\": n_est})\n        m_full.fit(train[features], (np.log1p(train[target].clip(lower=0.0)) if is_log else train[target]).astype(float),\n                   categorical_feature=cat_feats)\n        return m_full\n\n    m_raw = fit_then_refit(42, y_tr, y_va if len(X_va) > 0 else None, is_log=False)\n    m_log = fit_then_refit(7, np.log1p(y_tr.clip(lower=0.0)),\n                           np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None, is_log=True)\n\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = 0.65 * p_raw + 0.35 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 40.0):\n    gm = float(train[\"yield\"].mean())\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # simple (non-OOF) smoothed region/season baselines (often strong)\n    train, test = add_smoothed_means(train, test, smooth=55.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # tune iterations on last-year holdout, then refit on full data\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_then_refit(seed: int, ytr: pd.Series, yva: pd.Series | None, is_log: bool):\n        params = dict(\n            objective=\"regression\",\n            n_estimators=8000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=25,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        best_it = None\n        if len(X_va) > 0:\n            m = lgb.LGBMRegressor(**params)\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        n_est = max(1200, best_it) if best_it else params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**params, \"n_estimators\": n_est})\n        m_full.fit(train[features], (np.log1p(train[target].clip(lower=0.0)) if is_log else train[target]).astype(float),\n                   categorical_feature=cat_feats)\n        return m_full\n\n    m_raw = fit_then_refit(42, y_tr, y_va if len(X_va) > 0 else None, is_log=False)\n    m_log = fit_then_refit(7, np.log1p(y_tr.clip(lower=0.0)),\n                           np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None, is_log=True)\n\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = 0.65 * p_raw + 0.35 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        u = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(u)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"phase\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m.astype(float) / 12.0)\n    df[\"m_sin\"] = np.sin(ang)\n    df[\"m_cos\"] = np.cos(ang)\n    df[\"y_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"st_x_y\"] = df[\"state_enc\"].astype(float) * df[\"y_rel\"].astype(float)\n    df[\"st_x_p\"] = df[\"state_enc\"].astype(float) * df[\"phase\"].astype(float)\n    return df\n\ndef _prep_num(train: pd.DataFrame, test: pd.DataFrame, target: str):\n    kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in kinds]\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats] = train[feats].fillna(med)\n    test[feats] = test[feats].fillna(med)\n    return feats, train, test\n\ndef _mape(y, p):\n    y = np.asarray(y, dtype=float)\n    p = np.asarray(p, dtype=float)\n    den = np.maximum(np.abs(y), 1.0)\n    return float(np.mean(np.abs(y - p) / den))\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    target = \"yield\"\n    features, train, test = _prep_num(train, test, target)\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.20\n    else:\n        val_mask = rng.random(len(train)) < 0.20\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    base_params = dict(\n        objective=\"regression\",\n        n_estimators=12000,\n        learning_rate=0.025,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.9,\n        extra_trees=True,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit_model(seed: int, ytr, yva):\n        m = lgb.LGBMRegressor(**{**base_params, \"random_state\": seed})\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(300, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            best_it = 0\n        n_est = max(1200, best_it) if best_it else base_params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**base_params, \"random_state\": seed, \"n_estimators\": n_est})\n        m_full.fit(train[features], ytr, categorical_feature=cat_feats)\n        return m, m_full\n\n    m_raw_val, m_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log_val, m_log = fit_model(7, y_tr_log, y_va_log)\n\n    w = 0.70\n    if len(X_va) > 0:\n        p_va_raw = np.clip(m_raw_val.predict(X_va), 0.0, None)\n        p_va_log = np.clip(np.expm1(m_log_val.predict(X_va)), 0.0, None)\n        ym = float(np.mean(np.abs(y_va))) + 1e-9\n        best_s, best_w = 1e18, w\n        for ww in np.linspace(0.0, 1.0, 11):\n            pv = ww * p_va_raw + (1.0 - ww) * p_va_log\n            s = _mape(y_va, pv) + (np.sqrt(np.mean((y_va - pv) ** 2)) / ym)\n            if s < best_s:\n                best_s, best_w = s, float(ww)\n        w = best_w\n\n    p_raw = np.clip(m_raw.predict(test[features]), 0.0, None)\n    p_log = np.clip(np.expm1(m_log.predict(test[features])), 0.0, None)\n    pred = np.clip(w * p_raw + (1.0 - w) * p_log, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.45205899795335663}, "island_id": 0, "generation": 5, "metadata": {"iteration_time": 252.46447467803955, "changes": "Change 1: Replace 131 lines with 137 lines"}}
{"iteration": 16, "timestamp": 1765778650.839908, "parent_id": "939939dd-d79f-42fd-b757-75a168222b7f", "child_id": "e3c1ad01-152c-418a-8eeb-7b2317e459a8", "parent_metrics": {"combined_score": 0.4250264999265601, "mape": 0.17833084328049664, "rmse": 715.8909632091413}, "child_metrics": {"combined_score": 0.4522421114839621, "mape": 0.10757428975346768, "rmse": 620.0218079363765}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n    train = add_time_feats(train)\n    test = add_time_feats(test)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    # robust numeric + median impute (better than fill 0)\n    train[features] = train[features].apply(pd.to_numeric, errors=\"coerce\")\n    test[features] = test[features].apply(pd.to_numeric, errors=\"coerce\")\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=1800,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_lambda=0.8,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[target].astype(float), categorical_feature=[\"state_enc\"])\n\n    pred = model.predict(test[features])\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4250\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4492 \u2192 0.4250. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 156 lines with 131 lines\n- Metrics: combined_score: 0.4521, mape: 0.1080, rmse: 633.3264\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4521)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 40.0):\n    gm = float(train[\"yield\"].mean())\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # simple (non-OOF) smoothed region/season baselines (often strong)\n    train, test = add_smoothed_means(train, test, smooth=55.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # tune iterations on last-year holdout, then refit on full data\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_then_refit(seed: int, ytr: pd.Series, yva: pd.Series | None, is_log: bool):\n        params = dict(\n            objective=\"regression\",\n            n_estimators=8000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=25,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        best_it = None\n        if len(X_va) > 0:\n            m = lgb.LGBMRegressor(**params)\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        n_est = max(1200, best_it) if best_it else params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**params, \"n_estimators\": n_est})\n        m_full.fit(train[features], (np.log1p(train[target].clip(lower=0.0)) if is_log else train[target]).astype(float),\n                   categorical_feature=cat_feats)\n        return m_full\n\n    m_raw = fit_then_refit(42, y_tr, y_va if len(X_va) > 0 else None, is_log=False)\n    m_log = fit_then_refit(7, np.log1p(y_tr.clip(lower=0.0)),\n                           np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None, is_log=True)\n\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = 0.65 * p_raw + 0.35 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4521), Performs well on mape (0.1080), Performs well on rmse (633.3264)\n\n### Program 3 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4484)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 2 (Score: 0.3761, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    return df\n\ndef _add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef _smooth_te(train: pd.DataFrame, test: pd.DataFrame, keys, y=\"yield\", k: float = 20.0, name: str = \"te\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n    g = tr.groupby(keys)[y].agg([\"mean\", \"count\"]).reset_index()\n    gm = tr[y].mean()\n    g[name] = (g[\"count\"] * g[\"mean\"] + k * gm) / (g[\"count\"] + k)\n    tr = tr.merge(g[keys + [name]], on=keys, how=\"left\")\n    te = te.merge(g[keys + [name]], on=keys, how=\"left\")\n    tr[name] = tr[name].fillna(gm)\n    te[name] = te[name].fillna(gm)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_feats(train)\n    test = _add_time_feats(test)\n\n    # smoothed target encodings (strong for region/season yield)\n    train, test = _smooth_te(train, test, [\"state\"], name=\"te_state\", k=30.0)\n    train, test = _smooth_te(train, test, [\"state\", \"month\"], name=\"te_state_month\", k=40.0)\n\n    # year relative\n    base_year = min(train[\"year\"].min(), test[\"year\"].min())\n    train[\"year_rel\"] = (train[\"year\"] - base_year).astype(int)\n    test[\"year_rel\"] = (test[\"year\"] - base_year).astype(int)\n    train[\"state_x_year\"] = train[\"state_enc\"].astype(float) * train[\"year_rel\"].astype(float)\n    test[\"state_x_year\"] = test[\"state_enc\"].astype(float) * test[\"year_rel\"].astype(float)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # last-year validation for early stopping (if possible)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = None if y_va is None else np.log1p(y_va.clip(lower=0.0))\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val and len(X_va) > 0:\n        model.fit(\n            X_tr, y_tr_log,\n            eval_set=[(X_va, y_va_log)],\n            eval_metric=\"rmse\",\n            categorical_feature=[\"state_enc\"],\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        pred_log = model.predict(test[features], num_iteration=getattr(model, \"best_iteration_\", None))\n    else:\n        model.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=[\"state_enc\"])\n        pred_log = model.predict(test[features])\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 126 lines, Excellent rmse (1218.494), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.4212, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 158 lines, Alternative mape approach, Excellent rmse (749.913)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n    train = add_time_feats(train)\n    test = add_time_feats(test)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    # robust numeric + median impute (better than fill 0)\n    train[features] = train[features].apply(pd.to_numeric, errors=\"coerce\")\n    test[features] = test[features].apply(pd.to_numeric, errors=\"coerce\")\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=1800,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_lambda=0.8,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[target].astype(float), categorical_feature=[\"state_enc\"])\n\n    pred = model.predict(test[features])\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.027215611557401986, "mape": -0.07075655352702896, "rmse": -95.86915527276483}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 164.05368900299072, "changes": "Change 1: Replace 106 lines with 60 lines"}}
{"iteration": 17, "timestamp": 1765778713.0723054, "parent_id": "9c8d89bf-59db-498f-a153-347bfaabfe00", "child_id": "484db896-5529-43b2-90d1-65b68eb2a9fd", "parent_metrics": {"combined_score": 0.4211351081403701, "mape": 0.18914337310412893, "rmse": 751.6004515362335}, "child_metrics": {"combined_score": 0.0, "error": "lightgbm.sklearn.LGBMRegressor() got multiple values for keyword argument 'n_estimators'"}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    unk = len(mapping)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(unk).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_loo_te(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    prefix: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leave-one-out smoothed target encoding (train) + smoothed mean encoding (test).\"\"\"\n    tr = train.copy()\n    te = test.copy()\n    global_mean = float(tr[target].mean())\n\n    stats = tr.groupby(keys)[target].agg([\"sum\", \"count\"]).reset_index()\n    stats.columns = keys + [\"_sum\", \"_cnt\"]\n\n    tr = tr.merge(stats, on=keys, how=\"left\")\n    cnt_loo = (tr[\"_cnt\"] - 1).clip(lower=0)\n    sum_loo = tr[\"_sum\"] - tr[target].astype(float)\n    mean_loo = np.where(cnt_loo.to_numpy() > 0, (sum_loo / cnt_loo).to_numpy(), np.nan)\n    te_tr = (cnt_loo.to_numpy() * np.nan_to_num(mean_loo, nan=global_mean) + smooth * global_mean) / (cnt_loo.to_numpy() + smooth)\n    tr[prefix] = np.where(np.isfinite(te_tr), te_tr, global_mean)\n\n    te = te.merge(stats, on=keys, how=\"left\")\n    mean_full = (te[\"_sum\"] / te[\"_cnt\"]).to_numpy()\n    cnt_full = te[\"_cnt\"].fillna(0.0).to_numpy()\n    te_te = (cnt_full * np.nan_to_num(mean_full, nan=global_mean) + smooth * global_mean) / (cnt_full + smooth)\n    te[prefix] = np.where(np.isfinite(te_te), te_te, global_mean)\n\n    tr = tr.drop(columns=[\"_sum\", \"_cnt\"])\n    te = te.drop(columns=[\"_sum\", \"_cnt\"])\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (LOO) target encodings\n    train, test = add_loo_te(train, test, [\"state_enc\"], smooth=40.0, prefix=\"te_state\")\n    train, test = add_loo_te(train, test, [\"month\"], smooth=25.0, prefix=\"te_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"month\"], smooth=60.0, prefix=\"te_state_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + robust NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=15000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.82,\n        subsample_freq=1,\n        colsample_bytree=0.82,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit_model(y_train, y_valid, seed: int):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n        else:\n            m.fit(train[features], y_train, categorical_feature=cat_feats)\n        return m, getattr(m, \"best_iteration_\", None)\n\n    # raw + log blend tends to improve both RMSE and MAPE\n    m_raw, it_raw = fit_model(y_tr, y_va, seed=42)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_model(y_tr_log, y_va_log, seed=7)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 55.0):\n    gm = float(train[\"yield\"].mean())\n    # state mean\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    # month mean\n    m = train.groupby(\"month\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    m[\"te_month\"] = (m[\"count\"] * m[\"mean\"] + smooth * gm) / (m[\"count\"] + smooth)\n    train = train.merge(m[[\"month\", \"te_month\"]], on=\"month\", how=\"left\")\n    test = test.merge(m[[\"month\", \"te_month\"]], on=\"month\", how=\"left\")\n\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_month\"] = train[\"te_month\"].fillna(gm)\n    test[\"te_month\"] = test[\"te_month\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n    train, test = add_smoothed_means(train, test, smooth=60.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n    base = dict(\n        objective=\"regression\",\n        n_estimators=9000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.8,\n        n_jobs=-1,\n    )\n\n    def fit(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(**base, random_state=seed)\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            it = int(getattr(m, \"best_iteration_\", 0) or 0)\n            if it > 0:\n                m = lgb.LGBMRegressor(**base, random_state=seed, n_estimators=max(1200, it))\n                m.fit(train[features], ytr_full, categorical_feature=cat_feats)\n        else:\n            m.fit(train[features], ytr_full, categorical_feature=cat_feats)\n        return m\n\n    ytr_full = train[target].astype(float)\n    m_raw = fit(42, y_tr, y_va if len(X_va) > 0 else None)\n    m_log = fit(7, np.log1p(y_tr.clip(lower=0.0)),\n                np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = np.clip(0.65 * p_raw + 0.35 * p_log, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4211\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4492 \u2192 0.4211. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 111 lines with 98 lines\n- Metrics: combined_score: 0.4492, mape: 0.1157, rmse: 497.9983\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 156 lines with 131 lines\n- Metrics: combined_score: 0.4521, mape: 0.1080, rmse: 633.3264\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4521)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 40.0):\n    gm = float(train[\"yield\"].mean())\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # simple (non-OOF) smoothed region/season baselines (often strong)\n    train, test = add_smoothed_means(train, test, smooth=55.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # tune iterations on last-year holdout, then refit on full data\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_then_refit(seed: int, ytr: pd.Series, yva: pd.Series | None, is_log: bool):\n        params = dict(\n            objective=\"regression\",\n            n_estimators=8000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=25,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        best_it = None\n        if len(X_va) > 0:\n            m = lgb.LGBMRegressor(**params)\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        n_est = max(1200, best_it) if best_it else params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**params, \"n_estimators\": n_est})\n        m_full.fit(train[features], (np.log1p(train[target].clip(lower=0.0)) if is_log else train[target]).astype(float),\n                   categorical_feature=cat_feats)\n        return m_full\n\n    m_raw = fit_then_refit(42, y_tr, y_va if len(X_va) > 0 else None, is_log=False)\n    m_log = fit_then_refit(7, np.log1p(y_tr.clip(lower=0.0)),\n                           np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None, is_log=True)\n\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = 0.65 * p_raw + 0.35 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4521), Performs well on mape (0.1080), Performs well on rmse (633.3264)\n\n### Program 3 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1157), Performs well on rmse (497.9983)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4484)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4250)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4226, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_group_stats(train: pd.DataFrame, test: pd.DataFrame, target: str = \"yield\", smooth: float = 30.0):\n    gm = float(train[target].mean())\n    # state mean\n    s = train.groupby(\"state\")[target].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[target].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n    train, test = add_group_stats(train, test, target=\"yield\", smooth=35.0)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.9,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    m_log, it_log = fit_lgb(7, np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.62 * p_raw + 0.38 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 128 lines, Alternative mape approach, Excellent rmse (736.319)\n\n### Inspiration 2 (Score: 0.4209, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, GroupKFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef oof_target_encode(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    smooth: float = 25.0,\n    n_splits: int = 5,\n    seed: int = 42,\n):\n    tr = train_df[[*cols, target]].copy()\n    te = test_df[[*cols]].copy()\n    prior = float(tr[target].mean())\n\n    n_years = int(train_df[\"year\"].nunique()) if \"year\" in train_df.columns else 0\n    if n_years >= 2:\n        splitter = GroupKFold(n_splits=min(n_splits, n_years))\n        splits = splitter.split(tr, groups=train_df[\"year\"].values)\n    else:\n        splitter = KFold(n_splits=min(n_splits, len(tr)), shuffle=True, random_state=seed)\n        splits = splitter.split(tr)\n\n    oof = pd.Series(index=train_df.index, dtype=float)\n    for tr_idx, va_idx in splits:\n        fold = tr.iloc[tr_idx]\n        agg = fold.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"enc\"] = (agg[\"sum\"] + smooth * prior) / (agg[\"count\"] + smooth)\n        va = tr.iloc[va_idx][cols].merge(agg[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n        oof.iloc[va_idx] = va.values\n\n    agg_full = tr.groupby(cols)[target].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"enc\"] = (agg_full[\"sum\"] + smooth * prior) / (agg_full[\"count\"] + smooth)\n    te_enc = te.merge(agg_full[cols + [\"enc\"]], on=cols, how=\"left\")[\"enc\"].fillna(prior)\n    return oof.astype(float), te_enc.astype(float)\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (OOF) target encodings\n    te_s_tr, te_s_te = oof_target_encode(train, test, [\"state\"], \"yield\", smooth=35.0, n_splits=5, seed=42)\n    train[\"te_state\"] = te_s_tr\n    test[\"te_state\"] = te_s_te\n    te_sm_tr, te_sm_te = oof_target_encode(train, test, [\"state\", \"month\"], \"yield\", smooth=60.0, n_splits=5, seed=7)\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric casting + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation: last-year holdout if reasonable else random\n    rng = np.random.default_rng(42)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_lgb(seed: int, ytr: pd.Series, yva: pd.Series):\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=15000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=18,\n            subsample=0.86,\n            subsample_freq=1,\n            colsample_bytree=0.86,\n            reg_alpha=0.05,\n            reg_lambda=1.0,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n        else:\n            m.fit(train[features], ytr, categorical_feature=cat_feats)\n            it = None\n        return m, it\n\n    m_raw, it_raw = fit_lgb(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_lgb(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 98 lines with 156 lines, Alternative mape approach, Excellent rmse (753.107)\n\n### Inspiration 3 (Score: 0.4212, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: (m - 10) if m >= 10 else (m + 2)).astype(int)\n    return df\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef kfold_target_mean(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n    out_col: str = \"te\",\n) -> Tuple[pd.Series, pd.Series]:\n    global_mean = float(train_df[target].mean())\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    for tr_idx, va_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        stats = tr.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n        stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n        va = train_df.iloc[va_idx][keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n        oof.iloc[va_idx] = va.to_numpy()\n\n    stats = train_df.groupby(keys)[target].agg([\"mean\", \"count\"]).reset_index()\n    stats[out_col] = (stats[\"count\"] * stats[\"mean\"] + smooth * global_mean) / (stats[\"count\"] + smooth)\n    test_enc = test_df[keys].merge(stats[keys + [out_col]], on=keys, how=\"left\")[out_col]\n\n    oof = oof.fillna(global_mean)\n    test_enc = test_enc.fillna(global_mean)\n    return oof, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced target encodings\n    tr_te_s, te_te_s = kfold_target_mean(train, test, [\"state\"], out_col=\"te_state\", smooth=30.0)\n    train[\"te_state\"] = tr_te_s\n    test[\"te_state\"] = te_te_s\n    tr_te_sm, te_te_sm = kfold_target_mean(train, test, [\"state\", \"month\"], out_col=\"te_state_month\", smooth=25.0)\n    train[\"te_state_month\"] = tr_te_sm\n    test[\"te_state_month\"] = te_te_sm\n    tr_te_sp, te_te_sp = kfold_target_mean(train, test, [\"state\", \"months_since_crop_start\"], out_col=\"te_state_phase\", smooth=25.0)\n    train[\"te_state_phase\"] = tr_te_sp\n    test[\"te_state_phase\"] = te_te_sp\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        rng = np.random.default_rng(42)\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n\n    def fit_model(seed: int, y_train: pd.Series, y_valid: pd.Series | None):\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.80,\n            subsample_freq=1,\n            colsample_bytree=0.80,\n            reg_alpha=0.1,\n            reg_lambda=0.7,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if len(X_va) > 0:\n            model.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(350, verbose=False)],\n            )\n        else:\n            model.fit(train[features], y_train, categorical_feature=cat_feats)\n        return model, getattr(model, \"best_iteration_\", None)\n\n    m_raw, it_raw = fit_model(42, y_tr, y_va)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    m_log, it_log = fit_model(7, y_tr_log, y_va_log)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    test_pred = 0.60 * p_raw + 0.40 * p_log\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 158 lines, Alternative mape approach, Excellent rmse (749.913)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    unk = len(mapping)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(unk).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_loo_te(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    prefix: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leave-one-out smoothed target encoding (train) + smoothed mean encoding (test).\"\"\"\n    tr = train.copy()\n    te = test.copy()\n    global_mean = float(tr[target].mean())\n\n    stats = tr.groupby(keys)[target].agg([\"sum\", \"count\"]).reset_index()\n    stats.columns = keys + [\"_sum\", \"_cnt\"]\n\n    tr = tr.merge(stats, on=keys, how=\"left\")\n    cnt_loo = (tr[\"_cnt\"] - 1).clip(lower=0)\n    sum_loo = tr[\"_sum\"] - tr[target].astype(float)\n    mean_loo = np.where(cnt_loo.to_numpy() > 0, (sum_loo / cnt_loo).to_numpy(), np.nan)\n    te_tr = (cnt_loo.to_numpy() * np.nan_to_num(mean_loo, nan=global_mean) + smooth * global_mean) / (cnt_loo.to_numpy() + smooth)\n    tr[prefix] = np.where(np.isfinite(te_tr), te_tr, global_mean)\n\n    te = te.merge(stats, on=keys, how=\"left\")\n    mean_full = (te[\"_sum\"] / te[\"_cnt\"]).to_numpy()\n    cnt_full = te[\"_cnt\"].fillna(0.0).to_numpy()\n    te_te = (cnt_full * np.nan_to_num(mean_full, nan=global_mean) + smooth * global_mean) / (cnt_full + smooth)\n    te[prefix] = np.where(np.isfinite(te_te), te_te, global_mean)\n\n    tr = tr.drop(columns=[\"_sum\", \"_cnt\"])\n    te = te.drop(columns=[\"_sum\", \"_cnt\"])\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (LOO) target encodings\n    train, test = add_loo_te(train, test, [\"state_enc\"], smooth=40.0, prefix=\"te_state\")\n    train, test = add_loo_te(train, test, [\"month\"], smooth=25.0, prefix=\"te_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"month\"], smooth=60.0, prefix=\"te_state_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + robust NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=15000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.82,\n        subsample_freq=1,\n        colsample_bytree=0.82,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit_model(y_train, y_valid, seed: int):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n        else:\n            m.fit(train[features], y_train, categorical_feature=cat_feats)\n        return m, getattr(m, \"best_iteration_\", None)\n\n    # raw + log blend tends to improve both RMSE and MAPE\n    m_raw, it_raw = fit_model(y_tr, y_va, seed=42)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_model(y_tr_log, y_va_log, seed=7)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    unk = len(mapping)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(unk).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_yearrel\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_loo_te(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    keys: List[str],\n    target: str = \"yield\",\n    smooth: float = 30.0,\n    prefix: str = \"te\",\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leave-one-out smoothed target encoding (train) + smoothed mean encoding (test).\"\"\"\n    tr = train.copy()\n    te = test.copy()\n    global_mean = float(tr[target].mean())\n\n    stats = tr.groupby(keys)[target].agg([\"sum\", \"count\"]).reset_index()\n    stats.columns = keys + [\"_sum\", \"_cnt\"]\n\n    tr = tr.merge(stats, on=keys, how=\"left\")\n    cnt_loo = (tr[\"_cnt\"] - 1).clip(lower=0)\n    sum_loo = tr[\"_sum\"] - tr[target].astype(float)\n    mean_loo = np.where(cnt_loo.to_numpy() > 0, (sum_loo / cnt_loo).to_numpy(), np.nan)\n    te_tr = (cnt_loo.to_numpy() * np.nan_to_num(mean_loo, nan=global_mean) + smooth * global_mean) / (cnt_loo.to_numpy() + smooth)\n    tr[prefix] = np.where(np.isfinite(te_tr), te_tr, global_mean)\n\n    te = te.merge(stats, on=keys, how=\"left\")\n    mean_full = (te[\"_sum\"] / te[\"_cnt\"]).to_numpy()\n    cnt_full = te[\"_cnt\"].fillna(0.0).to_numpy()\n    te_te = (cnt_full * np.nan_to_num(mean_full, nan=global_mean) + smooth * global_mean) / (cnt_full + smooth)\n    te[prefix] = np.where(np.isfinite(te_te), te_te, global_mean)\n\n    tr = tr.drop(columns=[\"_sum\", \"_cnt\"])\n    te = te.drop(columns=[\"_sum\", \"_cnt\"])\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_time_feats(train, year0)\n    test = add_time_feats(test, year0)\n\n    # leakage-reduced (LOO) target encodings\n    train, test = add_loo_te(train, test, [\"state_enc\"], smooth=40.0, prefix=\"te_state\")\n    train, test = add_loo_te(train, test, [\"month\"], smooth=25.0, prefix=\"te_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"month\"], smooth=60.0, prefix=\"te_state_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"months_since_crop_start\"], smooth=60.0, prefix=\"te_state_phase\")\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # numeric + robust NA handling\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # validation split: last year if reasonable else random\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        frac = float(val_mask.mean())\n        if frac < 0.10 or frac > 0.60:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=15000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.82,\n        subsample_freq=1,\n        colsample_bytree=0.82,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit_model(y_train, y_valid, seed: int):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, y_train,\n                eval_set=[(X_va, y_valid)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(400, verbose=False)],\n            )\n        else:\n            m.fit(train[features], y_train, categorical_feature=cat_feats)\n        return m, getattr(m, \"best_iteration_\", None)\n\n    # raw + log blend tends to improve both RMSE and MAPE\n    m_raw, it_raw = fit_model(y_tr, y_va, seed=42)\n    y_tr_log = np.log1p(y_tr.clip(lower=0.0))\n    y_va_log = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else y_va\n    m_log, it_log = fit_model(y_tr_log, y_va_log, seed=7)\n\n    p_raw = m_raw.predict(test[features], num_iteration=it_raw)\n    p_log = np.expm1(m_log.predict(test[features], num_iteration=it_log))\n    pred = 0.60 * p_raw + 0.40 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 55.0):\n    gm = float(train[\"yield\"].mean())\n    # state mean\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    # month mean\n    m = train.groupby(\"month\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    m[\"te_month\"] = (m[\"count\"] * m[\"mean\"] + smooth * gm) / (m[\"count\"] + smooth)\n    train = train.merge(m[[\"month\", \"te_month\"]], on=\"month\", how=\"left\")\n    test = test.merge(m[[\"month\", \"te_month\"]], on=\"month\", how=\"left\")\n\n    # state-month mean\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_month\"] = train[\"te_month\"].fillna(gm)\n    test[\"te_month\"] = test[\"te_month\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n    train, test = add_smoothed_means(train, test, smooth=60.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n    base = dict(\n        objective=\"regression\",\n        n_estimators=9000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.8,\n        n_jobs=-1,\n    )\n\n    def fit(seed: int, ytr: pd.Series, yva: pd.Series | None):\n        m = lgb.LGBMRegressor(**base, random_state=seed)\n        if len(X_va) > 0:\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            it = int(getattr(m, \"best_iteration_\", 0) or 0)\n            if it > 0:\n                m = lgb.LGBMRegressor(**base, random_state=seed, n_estimators=max(1200, it))\n                m.fit(train[features], ytr_full, categorical_feature=cat_feats)\n        else:\n            m.fit(train[features], ytr_full, categorical_feature=cat_feats)\n        return m\n\n    ytr_full = train[target].astype(float)\n    m_raw = fit(42, y_tr, y_va if len(X_va) > 0 else None)\n    m_log = fit(7, np.log1p(y_tr.clip(lower=0.0)),\n                np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None)\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = np.clip(0.65 * p_raw + 0.35 * p_log, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.4211351081403701}, "island_id": 0, "generation": 4, "metadata": {"iteration_time": 62.30177402496338, "changes": "Change 1: Replace 153 lines with 136 lines"}}
{"iteration": 18, "timestamp": 1765778765.6367764, "parent_id": "5d11f3ab-be9c-4119-9e03-1ae8ef03b658", "child_id": "76c1a9c6-3ab6-4805-91ac-fe4252e5cc24", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.44923476147735464, "mape": 0.11550001887027928, "rmse": 496.3760631710385}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"m_sin\"] = np.sin(2 * np.pi * (m - 1) / 12.0)\n    df[\"m_cos\"] = np.cos(2 * np.pi * (m - 1) / 12.0)\n    df[\"crop_phase\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\ndef _kfold_target_encode(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, y: str, n_splits: int = 5, alpha: float = 20.0):\n    tr = train_df[[col, y]].copy()\n    te = test_df[[col]].copy()\n\n    global_mean = tr[y].mean()\n    counts = tr.groupby(col)[y].count()\n    means = tr.groupby(col)[y].mean()\n    smooth = (counts * means + alpha * global_mean) / (counts + alpha)\n\n    # OOF for train\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    for tr_idx, va_idx in kf.split(train_df):\n        fold = train_df.iloc[tr_idx][[col, y]]\n        gmean = fold[y].mean()\n        c = fold.groupby(col)[y].count()\n        m = fold.groupby(col)[y].mean()\n        s = (c * m + alpha * gmean) / (c + alpha)\n        oof.iloc[va_idx] = train_df.iloc[va_idx][col].map(s).fillna(gmean).values\n\n    test_enc = te[col].map(smooth).fillna(global_mean).values\n    return oof.values, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # robust target encoding on key categorical interactions\n    for col in [\"state\", \"state_enc\", \"month\", \"crop_phase\"]:\n        if col in train.columns:\n            tr_enc, te_enc = _kfold_target_encode(train, test, col=col, y=\"yield\", n_splits=5, alpha=30.0)\n            train[f\"te_{col}\"] = tr_enc\n            test[f\"te_{col}\"] = te_enc\n\n    if \"state_enc\" in train.columns and \"crop_phase\" in train.columns:\n        train[\"state_phase\"] = train[\"state_enc\"].astype(str) + \"_\" + train[\"crop_phase\"].astype(str)\n        test[\"state_phase\"] = test[\"state_enc\"].astype(str) + \"_\" + test[\"crop_phase\"].astype(str)\n        tr_enc, te_enc = _kfold_target_encode(train, test, col=\"state_phase\", y=\"yield\", n_splits=5, alpha=50.0)\n        train[\"te_state_phase\"] = tr_enc\n        test[\"te_state_phase\"] = te_enc\n\n    # numeric features + engineered encodings\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate = [c for c in train.columns if c != \"yield\"]\n    features: List[str] = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # fill NA with train medians (safe for both train/test)\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    # simple time-aware split if possible (use last year as validation)\n    years = np.sort(train[\"year\"].unique())\n    if len(years) >= 4:\n        val_year = years[-1]\n        tr_mask = train[\"year\"].values != val_year\n        va_mask = ~tr_mask\n        X_tr, y_tr = X.loc[tr_mask], y[tr_mask]\n        X_va, y_va = X.loc[va_mask], y[va_mask]\n    else:\n        # fallback: random split\n        rng = np.random.RandomState(42)\n        idx = np.arange(len(train))\n        rng.shuffle(idx)\n        cut = int(len(idx) * 0.85)\n        tr_idx, va_idx = idx[:cut], idx[cut:]\n        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.2,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"m_sin\"] = np.sin(2 * np.pi * (m - 1) / 12.0)\n    df[\"m_cos\"] = np.cos(2 * np.pi * (m - 1) / 12.0)\n    df[\"crop_phase\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\ndef _kfold_target_encode(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, y: str, n_splits: int = 5, alpha: float = 20.0):\n    tr = train_df[[col, y]].copy()\n    te = test_df[[col]].copy()\n\n    global_mean = tr[y].mean()\n    counts = tr.groupby(col)[y].count()\n    means = tr.groupby(col)[y].mean()\n    smooth = (counts * means + alpha * global_mean) / (counts + alpha)\n\n    # OOF for train\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    for tr_idx, va_idx in kf.split(train_df):\n        fold = train_df.iloc[tr_idx][[col, y]]\n        gmean = fold[y].mean()\n        c = fold.groupby(col)[y].count()\n        m = fold.groupby(col)[y].mean()\n        s = (c * m + alpha * gmean) / (c + alpha)\n        oof.iloc[va_idx] = train_df.iloc[va_idx][col].map(s).fillna(gmean).values\n\n    test_enc = te[col].map(smooth).fillna(global_mean).values\n    return oof.values, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # robust target encoding on key categorical interactions\n    for col in [\"state\", \"state_enc\", \"month\", \"crop_phase\"]:\n        if col in train.columns:\n            tr_enc, te_enc = _kfold_target_encode(train, test, col=col, y=\"yield\", n_splits=5, alpha=30.0)\n            train[f\"te_{col}\"] = tr_enc\n            test[f\"te_{col}\"] = te_enc\n\n    if \"state_enc\" in train.columns and \"crop_phase\" in train.columns:\n        train[\"state_phase\"] = train[\"state_enc\"].astype(str) + \"_\" + train[\"crop_phase\"].astype(str)\n        test[\"state_phase\"] = test[\"state_enc\"].astype(str) + \"_\" + test[\"crop_phase\"].astype(str)\n        tr_enc, te_enc = _kfold_target_encode(train, test, col=\"state_phase\", y=\"yield\", n_splits=5, alpha=50.0)\n        train[\"te_state_phase\"] = tr_enc\n        test[\"te_state_phase\"] = te_enc\n\n    # numeric features + engineered encodings\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate = [c for c in train.columns if c != \"yield\"]\n    features: List[str] = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # fill NA with train medians (safe for both train/test)\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    # simple time-aware split if possible (use last year as validation)\n    years = np.sort(train[\"year\"].unique())\n    if len(years) >= 4:\n        val_year = years[-1]\n        tr_mask = train[\"year\"].values != val_year\n        va_mask = ~tr_mask\n        X_tr, y_tr = X.loc[tr_mask], y[tr_mask]\n        X_va, y_va = X.loc[va_mask], y[va_mask]\n    else:\n        # fallback: random split\n        rng = np.random.RandomState(42)\n        idx = np.arange(len(train))\n        rng.shuffle(idx)\n        cut = int(len(idx) * 0.85)\n        tr_idx, va_idx = idx[:cut], idx[cut:]\n        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.2,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.003866978246600483, "mape": 0.00995853732943451, "rmse": -101.65897865785541}, "island_id": 1, "generation": 1, "metadata": {"iteration_time": 52.56302452087402, "changes": "Change 1: Replace 70 lines with 131 lines"}}
{"iteration": 19, "timestamp": 1765778901.1497984, "parent_id": "7cdeffc3-ff53-4f5b-976b-1df16d3571da", "child_id": "c0ba20af-5b70-4f61-9b7c-49ef721b0ab7", "parent_metrics": {"combined_score": 0.4205860818323881, "mape": 0.19068537114890574, "rmse": 756.7275227547476}, "child_metrics": {"combined_score": 0.4560550086886484, "mape": 0.09857379501103909, "rmse": 542.8044627285327}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        st = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(st)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"phase\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"m_sin\"], df[\"m_cos\"] = np.sin(ang), np.cos(ang)\n    df[\"st_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"st_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"phase\"].astype(float)\n    df[\"st_x_sin\"] = df[\"state_enc\"].astype(float) * df[\"m_sin\"].astype(float)\n    df[\"st_x_cos\"] = df[\"state_enc\"].astype(float) * df[\"m_cos\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, mp = encode_state(train)\n    test, _ = encode_state(test, mapping=mp)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    target = \"yield\"\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in numk]\n\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats], test[feats] = train[feats].fillna(med), test[feats].fillna(med)\n\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.10:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, feats], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, feats], train.loc[val_mask, target].astype(float)\n\n    cat = [\"state_enc\"] if \"state_enc\" in feats else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=12000,\n        learning_rate=0.02,\n        num_leaves=63,\n        min_child_samples=35,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit(ytr, yva, seed):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        m.fit(\n            X_tr, ytr,\n            eval_set=[(X_va, yva)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat,\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        return m, getattr(m, \"best_iteration_\", None)\n\n    m1, it1 = fit(y_tr, y_va, 42)\n    m2, it2 = fit(np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)), 7)\n\n    p1 = m1.predict(test[feats], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[feats], num_iteration=it2))\n    pred = np.clip(0.6 * p1 + 0.4 * p2, 0.0, None)\n\n    out = test.copy()\n    out[\"yield\"] = pred\n    out = out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        st = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(st)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"phase\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"m_sin\"] = np.sin(ang)\n    df[\"m_cos\"] = np.cos(ang)\n    return df\n\ndef _smooth_te(stats: pd.DataFrame, gm: float, smooth: float, name: str) -> pd.DataFrame:\n    te = (stats[\"count\"] * stats[\"mean\"] + smooth * gm) / (stats[\"count\"] + smooth)\n    out = stats.reset_index()[stats.index.names].copy()\n    out[name] = te.values\n    return out\n\ndef add_oof_target_enc(train: pd.DataFrame, test: pd.DataFrame, cols: List[str], target: str, smooth: float = 50.0, n_splits: int = 5):\n    gm = float(train[target].mean())\n    name = \"te_\" + \"_\".join(cols)\n\n    n = len(train)\n    if n < 50 or train[target].nunique() < 10:\n        # fallback: single fit on full (still smoothed)\n        stats = train.groupby(cols)[target].agg(mean=\"mean\", count=\"count\")\n        te_map = _smooth_te(stats, gm, smooth, name)\n        train[name] = train[cols].merge(te_map, on=cols, how=\"left\")[name].fillna(gm).values\n        test[name] = test[cols].merge(te_map, on=cols, how=\"left\")[name].fillna(gm).values\n        return train, test\n\n    oof = np.full(n, np.nan, dtype=float)\n    k = min(n_splits, 5)\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n\n    for tr_idx, va_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        va = train.iloc[va_idx]\n        stats = tr.groupby(cols)[target].agg(mean=\"mean\", count=\"count\")\n        te_map = _smooth_te(stats, gm, smooth, name)\n        oof[va_idx] = va[cols].merge(te_map, on=cols, how=\"left\")[name].fillna(gm).values\n\n    train[name] = np.where(np.isfinite(oof), oof, gm)\n\n    stats_full = train.groupby(cols)[target].agg(mean=\"mean\", count=\"count\")\n    te_map_full = _smooth_te(stats_full, gm, smooth, name)\n    test[name] = test[cols].merge(te_map_full, on=cols, how=\"left\")[name].fillna(gm).values\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, mp = encode_state(train)\n    test, _ = encode_state(test, mapping=mp)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # OOF target encoding (reduces leakage vs. naive TE)\n    train, test = add_oof_target_enc(train, test, [\"state\"], \"yield\", smooth=60.0, n_splits=5)\n    train, test = add_oof_target_enc(train, test, [\"state\", \"month\"], \"yield\", smooth=80.0, n_splits=5)\n\n    target = \"yield\"\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in numk]\n\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats] = train[feats].fillna(med)\n    test[feats] = test[feats].fillna(med)\n\n    # last-year holdout for early stopping if viable\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.06 or float(val_mask.mean()) > 0.65:\n            val_mask = pd.Series(False, index=train.index)\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, feats], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, feats], train.loc[val_mask, target].astype(float)\n\n    cat = [\"state_enc\"] if \"state_enc\" in feats else \"auto\"\n    base_params = dict(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.04,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.8,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    best_it = None\n    if len(X_va) > 0:\n        m = lgb.LGBMRegressor(**base_params)\n        m.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat,\n            callbacks=[lgb.early_stopping(200, verbose=False)],\n        )\n        best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n\n    n_est = max(1200, best_it) if best_it else base_params[\"n_estimators\"]\n    model = lgb.LGBMRegressor(**{**base_params, \"n_estimators\": n_est})\n    model.fit(train[feats], train[target].astype(float), categorical_feature=cat)\n\n    pred = model.predict(test[feats])\n    pred = np.clip(pred, 0.0, None)\n\n    out = test.copy()\n    out[\"yield\"] = pred\n    out = out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4206\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4521 \u2192 0.4206. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 156 lines with 131 lines\n- Metrics: combined_score: 0.4521, mape: 0.1080, rmse: 633.3264\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 106 lines with 60 lines\n- Metrics: combined_score: 0.4522, mape: 0.1076, rmse: 620.0218\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4522)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n    train = add_time_feats(train)\n    test = add_time_feats(test)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    # robust numeric + median impute (better than fill 0)\n    train[features] = train[features].apply(pd.to_numeric, errors=\"coerce\")\n    test[features] = test[features].apply(pd.to_numeric, errors=\"coerce\")\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=1800,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_lambda=0.8,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[target].astype(float), categorical_feature=[\"state_enc\"])\n\n    pred = model.predict(test[features])\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4522), Performs well on mape (0.1076), Performs well on rmse (620.0218)\n\n### Program 3 (Score: 0.4521)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef add_smoothed_means(train: pd.DataFrame, test: pd.DataFrame, smooth: float = 40.0):\n    gm = float(train[\"yield\"].mean())\n    s = train.groupby(\"state\")[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    s[\"te_state\"] = (s[\"count\"] * s[\"mean\"] + smooth * gm) / (s[\"count\"] + smooth)\n    train = train.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n    test = test.merge(s[[\"state\", \"te_state\"]], on=\"state\", how=\"left\")\n\n    sm = train.groupby([\"state\", \"month\"])[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    sm[\"te_state_month\"] = (sm[\"count\"] * sm[\"mean\"] + smooth * gm) / (sm[\"count\"] + smooth)\n    train = train.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n    test = test.merge(sm[[\"state\", \"month\", \"te_state_month\"]], on=[\"state\", \"month\"], how=\"left\")\n\n    train[\"te_state\"] = train[\"te_state\"].fillna(gm)\n    test[\"te_state\"] = test[\"te_state\"].fillna(gm)\n    train[\"te_state_month\"] = train[\"te_state_month\"].fillna(train[\"te_state\"])\n    test[\"te_state_month\"] = test[\"te_state_month\"].fillna(test[\"te_state\"])\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, st_map = encode_state(train)\n    test, _ = encode_state(test, mapping=st_map)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # simple (non-OOF) smoothed region/season baselines (often strong)\n    train, test = add_smoothed_means(train, test, smooth=55.0)\n\n    target = \"yield\"\n    num_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in num_kinds]\n\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # tune iterations on last-year holdout, then refit on full data\n    use_val = train[\"year\"].nunique() > 1\n    val_mask = train[\"year\"].eq(train[\"year\"].max()) if use_val else pd.Series(False, index=train.index)\n    frac = float(val_mask.mean()) if use_val else 0.0\n    if (not use_val) or frac < 0.08 or frac > 0.65:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_then_refit(seed: int, ytr: pd.Series, yva: pd.Series | None, is_log: bool):\n        params = dict(\n            objective=\"regression\",\n            n_estimators=8000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=25,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.05,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        best_it = None\n        if len(X_va) > 0:\n            m = lgb.LGBMRegressor(**params)\n            m.fit(\n                X_tr, ytr,\n                eval_set=[(X_va, yva)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(250, verbose=False)],\n            )\n            best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n        n_est = max(1200, best_it) if best_it else params[\"n_estimators\"]\n        m_full = lgb.LGBMRegressor(**{**params, \"n_estimators\": n_est})\n        m_full.fit(train[features], (np.log1p(train[target].clip(lower=0.0)) if is_log else train[target]).astype(float),\n                   categorical_feature=cat_feats)\n        return m_full\n\n    m_raw = fit_then_refit(42, y_tr, y_va if len(X_va) > 0 else None, is_log=False)\n    m_log = fit_then_refit(7, np.log1p(y_tr.clip(lower=0.0)),\n                           np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None, is_log=True)\n\n    p_raw = m_raw.predict(test[features])\n    p_log = np.expm1(m_log.predict(test[features]))\n    pred = 0.65 * p_raw + 0.35 * p_log\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4521), Performs well on mape (0.1080), Performs well on rmse (633.3264)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4484)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4250, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2)\n    return df\n\ndef add_cyc_month(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_cyc_month(months_since_crop_start(train))\n    test = add_cyc_month(months_since_crop_start(test))\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n    # validation: last-year holdout if exists and not too tiny\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if val_mask.mean() < 0.08:\n            rng = np.random.default_rng(42)\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n\n    cat_feats = [features.index(\"state_enc\")] if \"state_enc\" in features else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    if len(X_va) > 0:\n        m1.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it1 = getattr(m1, \"best_iteration_\", None)\n    else:\n        m1.fit(train[features], train[target].astype(float), categorical_feature=cat_feats)\n        it1 = None\n\n    # second model: log-target to help relative error; blend for stability\n    m2 = lgb.LGBMRegressor(**params)\n    y_tr2 = np.log1p(y_tr.clip(lower=0.0))\n    y_va2 = np.log1p(y_va.clip(lower=0.0)) if len(X_va) > 0 else None\n    if len(X_va) > 0:\n        m2.fit(X_tr, y_tr2, eval_set=[(X_va, y_va2)], eval_metric=\"rmse\",\n               categorical_feature=cat_feats,\n               callbacks=[lgb.early_stopping(250, verbose=False)])\n        it2 = getattr(m2, \"best_iteration_\", None)\n    else:\n        m2.fit(train[features], np.log1p(train[target].astype(float).clip(lower=0.0)), categorical_feature=cat_feats)\n        it2 = None\n\n    p1 = m1.predict(test[features], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[features], num_iteration=it2))\n    test_pred = 0.65 * p1 + 0.35 * p2\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 106 lines, Alternative mape approach, Excellent rmse (715.891)\n\n### Inspiration 2 (Score: 0.4484, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_rel\"] = (df[\"year\"] - min_year).astype(int)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    min_year = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, min_year)\n    test = add_feats(test, min_year)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # robust numeric casting + NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time split: last year holdout (more stable than random for yield)\n    use_val = train[\"year\"].nunique() > 1\n    if use_val:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        m = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=7000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            m.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            m.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return m\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 132 lines with 100 lines, Alternative mape approach, Excellent rmse (497.730)\n\n### Inspiration 3 (Score: 0.4492, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_basic_time_feats(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"state_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_basic_time_feats(train)\n    test = add_basic_time_feats(test)\n\n    target = \"yield\"\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != target and train[c].dtype.kind in numeric_kinds]\n\n    # ensure numeric + stable NA handling using train medians\n    for c in features:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[features] = train[features].replace([np.inf, -np.inf], np.nan)\n    test[features] = test[features].replace([np.inf, -np.inf], np.nan)\n    med = train[features].median(numeric_only=True)\n    train[features] = train[features].fillna(med)\n    test[features] = test[features].fillna(med)\n\n    # time-based validation: last year\n    years = train[\"year\"].unique()\n    use_val = len(years) > 1\n    if use_val:\n        max_year = train[\"year\"].max()\n        val_mask = train[\"year\"].eq(max_year)\n        X_tr, y_tr = train.loc[~val_mask, features], train.loc[~val_mask, target].astype(float)\n        X_va, y_va = train.loc[val_mask, features], train.loc[val_mask, target].astype(float)\n    else:\n        X_tr, y_tr = train[features], train[target].astype(float)\n        X_va = y_va = None\n\n    cat_feats = [\"state_enc\"] if \"state_enc\" in features else \"auto\"\n\n    def fit_one(seed: int) -> lgb.LGBMRegressor:\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            n_estimators=5000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_lambda=0.8,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if use_val and len(X_va) > 0:\n            model.fit(\n                X_tr, y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"rmse\",\n                categorical_feature=cat_feats,\n                callbacks=[lgb.early_stopping(200, verbose=False)],\n            )\n        else:\n            model.fit(X_tr, y_tr, categorical_feature=cat_feats)\n        return model\n\n    m1 = fit_one(42)\n    m2 = fit_one(7)\n    pred = 0.5 * m1.predict(test[features]) + 0.5 * m2.predict(test[features])\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 111 lines with 98 lines, Alternative mape approach, Excellent rmse (497.998)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        st = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(st)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"phase\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"m_sin\"], df[\"m_cos\"] = np.sin(ang), np.cos(ang)\n    df[\"st_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"st_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"phase\"].astype(float)\n    df[\"st_x_sin\"] = df[\"state_enc\"].astype(float) * df[\"m_sin\"].astype(float)\n    df[\"st_x_cos\"] = df[\"state_enc\"].astype(float) * df[\"m_cos\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, mp = encode_state(train)\n    test, _ = encode_state(test, mapping=mp)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    target = \"yield\"\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in numk]\n\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats], test[feats] = train[feats].fillna(med), test[feats].fillna(med)\n\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.10:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, feats], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, feats], train.loc[val_mask, target].astype(float)\n\n    cat = [\"state_enc\"] if \"state_enc\" in feats else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=12000,\n        learning_rate=0.02,\n        num_leaves=63,\n        min_child_samples=35,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit(ytr, yva, seed):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        m.fit(\n            X_tr, ytr,\n            eval_set=[(X_va, yva)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat,\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        return m, getattr(m, \"best_iteration_\", None)\n\n    m1, it1 = fit(y_tr, y_va, 42)\n    m2, it2 = fit(np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)), 7)\n\n    p1 = m1.predict(test[feats], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[feats], num_iteration=it2))\n    pred = np.clip(0.6 * p1 + 0.4 * p2, 0.0, None)\n\n    out = test.copy()\n    out[\"yield\"] = pred\n    out = out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        st = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(st)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"phase\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"m_sin\"], df[\"m_cos\"] = np.sin(ang), np.cos(ang)\n    df[\"st_x_year\"] = df[\"state_enc\"].astype(float) * df[\"year_rel\"].astype(float)\n    df[\"st_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"phase\"].astype(float)\n    df[\"st_x_sin\"] = df[\"state_enc\"].astype(float) * df[\"m_sin\"].astype(float)\n    df[\"st_x_cos\"] = df[\"state_enc\"].astype(float) * df[\"m_cos\"].astype(float)\n    return df\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, mp = encode_state(train)\n    test, _ = encode_state(test, mapping=mp)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    target = \"yield\"\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in numk]\n\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats], test[feats] = train[feats].fillna(med), test[feats].fillna(med)\n\n    rng = np.random.default_rng(42)\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.10:\n            val_mask = rng.random(len(train)) < 0.2\n    else:\n        val_mask = rng.random(len(train)) < 0.2\n\n    X_tr, y_tr = train.loc[~val_mask, feats], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, feats], train.loc[val_mask, target].astype(float)\n\n    cat = [\"state_enc\"] if \"state_enc\" in feats else \"auto\"\n    params = dict(\n        objective=\"regression\",\n        n_estimators=12000,\n        learning_rate=0.02,\n        num_leaves=63,\n        min_child_samples=35,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    def fit(ytr, yva, seed):\n        m = lgb.LGBMRegressor(**{**params, \"random_state\": seed})\n        m.fit(\n            X_tr, ytr,\n            eval_set=[(X_va, yva)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat,\n            callbacks=[lgb.early_stopping(300, verbose=False)],\n        )\n        return m, getattr(m, \"best_iteration_\", None)\n\n    m1, it1 = fit(y_tr, y_va, 42)\n    m2, it2 = fit(np.log1p(y_tr.clip(lower=0.0)), np.log1p(y_va.clip(lower=0.0)), 7)\n\n    p1 = m1.predict(test[feats], num_iteration=it1)\n    p2 = np.expm1(m2.predict(test[feats], num_iteration=it2))\n    pred = np.clip(0.6 * p1 + 0.4 * p2, 0.0, None)\n\n    out = test.copy()\n    out[\"yield\"] = pred\n    out = out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    df[\"state\"] = df[\"state\"].astype(str)\n    if mapping is None:\n        st = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(st)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_feats(df: pd.DataFrame, year0: int) -> pd.DataFrame:\n    df = df.copy()\n    df[\"year_rel\"] = (df[\"year\"].astype(int) - int(year0)).astype(int)\n    df[\"phase\"] = df[\"month\"].apply(lambda m: m - 10 if m >= 10 else m + 2).astype(int)\n    ang = 2.0 * np.pi * (df[\"month\"].astype(float) / 12.0)\n    df[\"m_sin\"] = np.sin(ang)\n    df[\"m_cos\"] = np.cos(ang)\n    return df\n\ndef _smooth_te(stats: pd.DataFrame, gm: float, smooth: float, name: str) -> pd.DataFrame:\n    te = (stats[\"count\"] * stats[\"mean\"] + smooth * gm) / (stats[\"count\"] + smooth)\n    out = stats.reset_index()[stats.index.names].copy()\n    out[name] = te.values\n    return out\n\ndef add_oof_target_enc(train: pd.DataFrame, test: pd.DataFrame, cols: List[str], target: str, smooth: float = 50.0, n_splits: int = 5):\n    gm = float(train[target].mean())\n    name = \"te_\" + \"_\".join(cols)\n\n    n = len(train)\n    if n < 50 or train[target].nunique() < 10:\n        # fallback: single fit on full (still smoothed)\n        stats = train.groupby(cols)[target].agg(mean=\"mean\", count=\"count\")\n        te_map = _smooth_te(stats, gm, smooth, name)\n        train[name] = train[cols].merge(te_map, on=cols, how=\"left\")[name].fillna(gm).values\n        test[name] = test[cols].merge(te_map, on=cols, how=\"left\")[name].fillna(gm).values\n        return train, test\n\n    oof = np.full(n, np.nan, dtype=float)\n    k = min(n_splits, 5)\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n\n    for tr_idx, va_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        va = train.iloc[va_idx]\n        stats = tr.groupby(cols)[target].agg(mean=\"mean\", count=\"count\")\n        te_map = _smooth_te(stats, gm, smooth, name)\n        oof[va_idx] = va[cols].merge(te_map, on=cols, how=\"left\")[name].fillna(gm).values\n\n    train[name] = np.where(np.isfinite(oof), oof, gm)\n\n    stats_full = train.groupby(cols)[target].agg(mean=\"mean\", count=\"count\")\n    te_map_full = _smooth_te(stats_full, gm, smooth, name)\n    test[name] = test[cols].merge(te_map_full, on=cols, how=\"left\")[name].fillna(gm).values\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, mp = encode_state(train)\n    test, _ = encode_state(test, mapping=mp)\n\n    year0 = int(min(train[\"year\"].min(), test[\"year\"].min()))\n    train = add_feats(train, year0)\n    test = add_feats(test, year0)\n\n    # OOF target encoding (reduces leakage vs. naive TE)\n    train, test = add_oof_target_enc(train, test, [\"state\"], \"yield\", smooth=60.0, n_splits=5)\n    train, test = add_oof_target_enc(train, test, [\"state\", \"month\"], \"yield\", smooth=80.0, n_splits=5)\n\n    target = \"yield\"\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    feats = [c for c in train.columns if c != target and train[c].dtype.kind in numk]\n\n    for c in feats:\n        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n    train[feats] = train[feats].replace([np.inf, -np.inf], np.nan)\n    test[feats] = test[feats].replace([np.inf, -np.inf], np.nan)\n    med = train[feats].median(numeric_only=True)\n    train[feats] = train[feats].fillna(med)\n    test[feats] = test[feats].fillna(med)\n\n    # last-year holdout for early stopping if viable\n    if train[\"year\"].nunique() > 1:\n        val_mask = train[\"year\"].eq(train[\"year\"].max())\n        if float(val_mask.mean()) < 0.06 or float(val_mask.mean()) > 0.65:\n            val_mask = pd.Series(False, index=train.index)\n    else:\n        val_mask = pd.Series(False, index=train.index)\n\n    X_tr, y_tr = train.loc[~val_mask, feats], train.loc[~val_mask, target].astype(float)\n    X_va, y_va = train.loc[val_mask, feats], train.loc[val_mask, target].astype(float)\n\n    cat = [\"state_enc\"] if \"state_enc\" in feats else \"auto\"\n    base_params = dict(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.04,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.8,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    best_it = None\n    if len(X_va) > 0:\n        m = lgb.LGBMRegressor(**base_params)\n        m.fit(\n            X_tr, y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            categorical_feature=cat,\n            callbacks=[lgb.early_stopping(200, verbose=False)],\n        )\n        best_it = int(getattr(m, \"best_iteration_\", 0) or 0)\n\n    n_est = max(1200, best_it) if best_it else base_params[\"n_estimators\"]\n    model = lgb.LGBMRegressor(**{**base_params, \"n_estimators\": n_est})\n    model.fit(train[feats], train[target].astype(float), categorical_feature=cat)\n\n    pred = model.predict(test[feats])\n    pred = np.clip(pred, 0.0, None)\n\n    out = test.copy()\n    out[\"yield\"] = pred\n    out = out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.0354689268562603, "mape": -0.09211157613786665, "rmse": -213.9230600262149}, "island_id": 0, "generation": 3, "metadata": {"iteration_time": 135.40189218521118, "changes": "Change 1: Replace 102 lines with 143 lines"}}
{"iteration": 20, "timestamp": 1765779030.318076, "parent_id": "76c1a9c6-3ab6-4805-91ac-fe4252e5cc24", "child_id": "eccd9004-f4a7-497e-aa71-3a87d1c48b1e", "parent_metrics": {"combined_score": 0.44923476147735464, "mape": 0.11550001887027928, "rmse": 496.3760631710385}, "child_metrics": {"combined_score": 0.4211588643535626, "mape": 0.18907108656461868, "rmse": 753.6510168028377}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"m_sin\"] = np.sin(2 * np.pi * (m - 1) / 12.0)\n    df[\"m_cos\"] = np.cos(2 * np.pi * (m - 1) / 12.0)\n    df[\"crop_phase\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\ndef _kfold_target_encode(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, y: str, n_splits: int = 5, alpha: float = 20.0):\n    tr = train_df[[col, y]].copy()\n    te = test_df[[col]].copy()\n\n    global_mean = tr[y].mean()\n    counts = tr.groupby(col)[y].count()\n    means = tr.groupby(col)[y].mean()\n    smooth = (counts * means + alpha * global_mean) / (counts + alpha)\n\n    # OOF for train\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    for tr_idx, va_idx in kf.split(train_df):\n        fold = train_df.iloc[tr_idx][[col, y]]\n        gmean = fold[y].mean()\n        c = fold.groupby(col)[y].count()\n        m = fold.groupby(col)[y].mean()\n        s = (c * m + alpha * gmean) / (c + alpha)\n        oof.iloc[va_idx] = train_df.iloc[va_idx][col].map(s).fillna(gmean).values\n\n    test_enc = te[col].map(smooth).fillna(global_mean).values\n    return oof.values, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # robust target encoding on key categorical interactions\n    for col in [\"state\", \"state_enc\", \"month\", \"crop_phase\"]:\n        if col in train.columns:\n            tr_enc, te_enc = _kfold_target_encode(train, test, col=col, y=\"yield\", n_splits=5, alpha=30.0)\n            train[f\"te_{col}\"] = tr_enc\n            test[f\"te_{col}\"] = te_enc\n\n    if \"state_enc\" in train.columns and \"crop_phase\" in train.columns:\n        train[\"state_phase\"] = train[\"state_enc\"].astype(str) + \"_\" + train[\"crop_phase\"].astype(str)\n        test[\"state_phase\"] = test[\"state_enc\"].astype(str) + \"_\" + test[\"crop_phase\"].astype(str)\n        tr_enc, te_enc = _kfold_target_encode(train, test, col=\"state_phase\", y=\"yield\", n_splits=5, alpha=50.0)\n        train[\"te_state_phase\"] = tr_enc\n        test[\"te_state_phase\"] = te_enc\n\n    # numeric features + engineered encodings\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate = [c for c in train.columns if c != \"yield\"]\n    features: List[str] = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # fill NA with train medians (safe for both train/test)\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    # simple time-aware split if possible (use last year as validation)\n    years = np.sort(train[\"year\"].unique())\n    if len(years) >= 4:\n        val_year = years[-1]\n        tr_mask = train[\"year\"].values != val_year\n        va_mask = ~tr_mask\n        X_tr, y_tr = X.loc[tr_mask], y[tr_mask]\n        X_va, y_va = X.loc[va_mask], y[va_mask]\n    else:\n        # fallback: random split\n        rng = np.random.RandomState(42)\n        idx = np.arange(len(train))\n        rng.shuffle(idx)\n        cut = int(len(idx) * 0.85)\n        tr_idx, va_idx = idx[:cut], idx[cut:]\n        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.2,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        s = sorted(df[\"state\"].astype(str).unique())\n        mapping = {k: i for i, k in enumerate(s)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"m_sin\"] = np.sin(2 * np.pi * (m - 1) / 12.0)\n    df[\"m_cos\"] = np.cos(2 * np.pi * (m - 1) / 12.0)\n    df[\"crop_phase\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\ndef add_group_means(train: pd.DataFrame, test: pd.DataFrame, keys: List[List[str]], y: str = \"yield\", alpha: float = 30.0):\n    gm = float(train[y].mean())\n    for k in keys:\n        name = \"gm_\" + \"_\".join(k)\n        g = train.groupby(k)[y].agg([\"sum\", \"count\"]).reset_index()\n        g[name] = (g[\"sum\"] + alpha * gm) / (g[\"count\"] + alpha)\n        g = g[k + [name]]\n        train = train.merge(g, on=k, how=\"left\")\n        test = test.merge(g, on=k, how=\"left\")\n        train[name] = train[name].fillna(gm)\n        test[name] = test[name].fillna(gm)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_group_means(train, test, keys=[[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]], alpha=40.0)\n\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    cand = [c for c in train.columns if c != \"yield\" and c != \"state\"]\n    feats: List[str] = [c for c in cand if train[c].dtype.kind in numk]\n\n    med = train[feats].median(numeric_only=True)\n    X = train[feats].fillna(med)\n    X_test = test[feats].fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    years = np.sort(train[\"year\"].unique())\n    if len(years) >= 4:\n        vy = years[-1]\n        tr_mask = train[\"year\"].values != vy\n        va_mask = ~tr_mask\n        X_tr, y_tr = X.loc[tr_mask], y[tr_mask]\n        X_va, y_va = X.loc[va_mask], y[va_mask]\n    else:\n        rng = np.random.RandomState(42)\n        idx = np.arange(len(train))\n        rng.shuffle(idx)\n        cut = int(len(idx) * 0.85)\n        tr, va = idx[:cut], idx[cut:]\n        X_tr, y_tr = X.iloc[tr], y[tr]\n        X_va, y_va = X.iloc[va], y[va]\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"crop_phase\"] if c in X.columns]\n\n    m1 = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.2,\n        random_state=42,\n        n_jobs=-1,\n    )\n    m1.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        categorical_feature=cat_feats if cat_feats else \"auto\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n    p1 = m1.predict(X_test, num_iteration=m1.best_iteration_)\n\n    ylog = np.log1p(y)\n    m2 = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.2,\n        random_state=43,\n        n_jobs=-1,\n    )\n    m2.fit(\n        X_tr, ylog[train.index.isin(X_tr.index) if hasattr(X_tr, \"index\") else np.arange(len(ylog))[:len(X_tr)]],\n        eval_set=[(X_va, ylog[train.index.isin(X_va.index) if hasattr(X_va, \"index\") else np.arange(len(ylog))[:len(X_va)]] )],\n        eval_metric=\"rmse\",\n        categorical_feature=cat_feats if cat_feats else \"auto\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    p2 = np.expm1(m2.predict(X_test, num_iteration=m2.best_iteration_))\n\n    pred = 0.6 * p1 + 0.4 * p2\n    pred = np.clip(pred, 0, None)\n\n    out = test.copy()\n    out[\"yield\"] = pred\n    out = out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\\\\n\\\\nHard constraints:\\\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\\\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\\\\n do not drop all features, you can combine them to generate new features for better performance.\\\\n- You can use any other standard Python libraries for better model.\\\\n\\\\nFreedom:\\\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\\\n\\\\nOutput format:\\\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.4492\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4492\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Change 1: Replace 70 lines with 131 lines\n- Metrics: combined_score: 0.4492, mape: 0.1155, rmse: 496.3761\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4492)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"m_sin\"] = np.sin(2 * np.pi * (m - 1) / 12.0)\n    df[\"m_cos\"] = np.cos(2 * np.pi * (m - 1) / 12.0)\n    df[\"crop_phase\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\ndef _kfold_target_encode(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, y: str, n_splits: int = 5, alpha: float = 20.0):\n    tr = train_df[[col, y]].copy()\n    te = test_df[[col]].copy()\n\n    global_mean = tr[y].mean()\n    counts = tr.groupby(col)[y].count()\n    means = tr.groupby(col)[y].mean()\n    smooth = (counts * means + alpha * global_mean) / (counts + alpha)\n\n    # OOF for train\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    for tr_idx, va_idx in kf.split(train_df):\n        fold = train_df.iloc[tr_idx][[col, y]]\n        gmean = fold[y].mean()\n        c = fold.groupby(col)[y].count()\n        m = fold.groupby(col)[y].mean()\n        s = (c * m + alpha * gmean) / (c + alpha)\n        oof.iloc[va_idx] = train_df.iloc[va_idx][col].map(s).fillna(gmean).values\n\n    test_enc = te[col].map(smooth).fillna(global_mean).values\n    return oof.values, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # robust target encoding on key categorical interactions\n    for col in [\"state\", \"state_enc\", \"month\", \"crop_phase\"]:\n        if col in train.columns:\n            tr_enc, te_enc = _kfold_target_encode(train, test, col=col, y=\"yield\", n_splits=5, alpha=30.0)\n            train[f\"te_{col}\"] = tr_enc\n            test[f\"te_{col}\"] = te_enc\n\n    if \"state_enc\" in train.columns and \"crop_phase\" in train.columns:\n        train[\"state_phase\"] = train[\"state_enc\"].astype(str) + \"_\" + train[\"crop_phase\"].astype(str)\n        test[\"state_phase\"] = test[\"state_enc\"].astype(str) + \"_\" + test[\"crop_phase\"].astype(str)\n        tr_enc, te_enc = _kfold_target_encode(train, test, col=\"state_phase\", y=\"yield\", n_splits=5, alpha=50.0)\n        train[\"te_state_phase\"] = tr_enc\n        test[\"te_state_phase\"] = te_enc\n\n    # numeric features + engineered encodings\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate = [c for c in train.columns if c != \"yield\"]\n    features: List[str] = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # fill NA with train medians (safe for both train/test)\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    # simple time-aware split if possible (use last year as validation)\n    years = np.sort(train[\"year\"].unique())\n    if len(years) >= 4:\n        val_year = years[-1]\n        tr_mask = train[\"year\"].values != val_year\n        va_mask = ~tr_mask\n        X_tr, y_tr = X.loc[tr_mask], y[tr_mask]\n        X_va, y_va = X.loc[va_mask], y[va_mask]\n    else:\n        # fallback: random split\n        rng = np.random.RandomState(42)\n        idx = np.arange(len(train))\n        rng.shuffle(idx)\n        cut = int(len(idx) * 0.85)\n        tr_idx, va_idx = idx[:cut], idx[cut:]\n        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.2,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4492), Performs well on mape (0.1155), Performs well on rmse (496.3761)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"m_sin\"] = np.sin(2 * np.pi * (m - 1) / 12.0)\n    df[\"m_cos\"] = np.cos(2 * np.pi * (m - 1) / 12.0)\n    df[\"crop_phase\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\ndef _kfold_target_encode(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, y: str, n_splits: int = 5, alpha: float = 20.0):\n    tr = train_df[[col, y]].copy()\n    te = test_df[[col]].copy()\n\n    global_mean = tr[y].mean()\n    counts = tr.groupby(col)[y].count()\n    means = tr.groupby(col)[y].mean()\n    smooth = (counts * means + alpha * global_mean) / (counts + alpha)\n\n    # OOF for train\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    for tr_idx, va_idx in kf.split(train_df):\n        fold = train_df.iloc[tr_idx][[col, y]]\n        gmean = fold[y].mean()\n        c = fold.groupby(col)[y].count()\n        m = fold.groupby(col)[y].mean()\n        s = (c * m + alpha * gmean) / (c + alpha)\n        oof.iloc[va_idx] = train_df.iloc[va_idx][col].map(s).fillna(gmean).values\n\n    test_enc = te[col].map(smooth).fillna(global_mean).values\n    return oof.values, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # robust target encoding on key categorical interactions\n    for col in [\"state\", \"state_enc\", \"month\", \"crop_phase\"]:\n        if col in train.columns:\n            tr_enc, te_enc = _kfold_target_encode(train, test, col=col, y=\"yield\", n_splits=5, alpha=30.0)\n            train[f\"te_{col}\"] = tr_enc\n            test[f\"te_{col}\"] = te_enc\n\n    if \"state_enc\" in train.columns and \"crop_phase\" in train.columns:\n        train[\"state_phase\"] = train[\"state_enc\"].astype(str) + \"_\" + train[\"crop_phase\"].astype(str)\n        test[\"state_phase\"] = test[\"state_enc\"].astype(str) + \"_\" + test[\"crop_phase\"].astype(str)\n        tr_enc, te_enc = _kfold_target_encode(train, test, col=\"state_phase\", y=\"yield\", n_splits=5, alpha=50.0)\n        train[\"te_state_phase\"] = tr_enc\n        test[\"te_state_phase\"] = te_enc\n\n    # numeric features + engineered encodings\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate = [c for c in train.columns if c != \"yield\"]\n    features: List[str] = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # fill NA with train medians (safe for both train/test)\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    # simple time-aware split if possible (use last year as validation)\n    years = np.sort(train[\"year\"].unique())\n    if len(years) >= 4:\n        val_year = years[-1]\n        tr_mask = train[\"year\"].values != val_year\n        va_mask = ~tr_mask\n        X_tr, y_tr = X.loc[tr_mask], y[tr_mask]\n        X_va, y_va = X.loc[va_mask], y[va_mask]\n    else:\n        # fallback: random split\n        rng = np.random.RandomState(42)\n        idx = np.arange(len(train))\n        rng.shuffle(idx)\n        cut = int(len(idx) * 0.85)\n        tr_idx, va_idx = idx[:cut], idx[cut:]\n        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.2,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"m_sin\"] = np.sin(2 * np.pi * (m - 1) / 12.0)\n    df[\"m_cos\"] = np.cos(2 * np.pi * (m - 1) / 12.0)\n    df[\"crop_phase\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\ndef _kfold_target_encode(train_df: pd.DataFrame, test_df: pd.DataFrame, col: str, y: str, n_splits: int = 5, alpha: float = 20.0):\n    tr = train_df[[col, y]].copy()\n    te = test_df[[col]].copy()\n\n    global_mean = tr[y].mean()\n    counts = tr.groupby(col)[y].count()\n    means = tr.groupby(col)[y].mean()\n    smooth = (counts * means + alpha * global_mean) / (counts + alpha)\n\n    # OOF for train\n    oof = pd.Series(index=train_df.index, dtype=float)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    for tr_idx, va_idx in kf.split(train_df):\n        fold = train_df.iloc[tr_idx][[col, y]]\n        gmean = fold[y].mean()\n        c = fold.groupby(col)[y].count()\n        m = fold.groupby(col)[y].mean()\n        s = (c * m + alpha * gmean) / (c + alpha)\n        oof.iloc[va_idx] = train_df.iloc[va_idx][col].map(s).fillna(gmean).values\n\n    test_enc = te[col].map(smooth).fillna(global_mean).values\n    return oof.values, test_enc\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # robust target encoding on key categorical interactions\n    for col in [\"state\", \"state_enc\", \"month\", \"crop_phase\"]:\n        if col in train.columns:\n            tr_enc, te_enc = _kfold_target_encode(train, test, col=col, y=\"yield\", n_splits=5, alpha=30.0)\n            train[f\"te_{col}\"] = tr_enc\n            test[f\"te_{col}\"] = te_enc\n\n    if \"state_enc\" in train.columns and \"crop_phase\" in train.columns:\n        train[\"state_phase\"] = train[\"state_enc\"].astype(str) + \"_\" + train[\"crop_phase\"].astype(str)\n        test[\"state_phase\"] = test[\"state_enc\"].astype(str) + \"_\" + test[\"crop_phase\"].astype(str)\n        tr_enc, te_enc = _kfold_target_encode(train, test, col=\"state_phase\", y=\"yield\", n_splits=5, alpha=50.0)\n        train[\"te_state_phase\"] = tr_enc\n        test[\"te_state_phase\"] = te_enc\n\n    # numeric features + engineered encodings\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate = [c for c in train.columns if c != \"yield\"]\n    features: List[str] = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # fill NA with train medians (safe for both train/test)\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    # simple time-aware split if possible (use last year as validation)\n    years = np.sort(train[\"year\"].unique())\n    if len(years) >= 4:\n        val_year = years[-1]\n        tr_mask = train[\"year\"].values != val_year\n        va_mask = ~tr_mask\n        X_tr, y_tr = X.loc[tr_mask], y[tr_mask]\n        X_va, y_va = X.loc[va_mask], y[va_mask]\n    else:\n        # fallback: random split\n        rng = np.random.RandomState(42)\n        idx = np.arange(len(train))\n        rng.shuffle(idx)\n        cut = int(len(idx) * 0.85)\n        tr_idx, va_idx = idx[:cut], idx[cut:]\n        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y[va_idx]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.2,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        s = sorted(df[\"state\"].astype(str).unique())\n        mapping = {k: i for i, k in enumerate(s)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"m_sin\"] = np.sin(2 * np.pi * (m - 1) / 12.0)\n    df[\"m_cos\"] = np.cos(2 * np.pi * (m - 1) / 12.0)\n    df[\"crop_phase\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\ndef add_group_means(train: pd.DataFrame, test: pd.DataFrame, keys: List[List[str]], y: str = \"yield\", alpha: float = 30.0):\n    gm = float(train[y].mean())\n    for k in keys:\n        name = \"gm_\" + \"_\".join(k)\n        g = train.groupby(k)[y].agg([\"sum\", \"count\"]).reset_index()\n        g[name] = (g[\"sum\"] + alpha * gm) / (g[\"count\"] + alpha)\n        g = g[k + [name]]\n        train = train.merge(g, on=k, how=\"left\")\n        test = test.merge(g, on=k, how=\"left\")\n        train[name] = train[name].fillna(gm)\n        test[name] = test[name].fillna(gm)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_group_means(train, test, keys=[[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]], alpha=40.0)\n\n    numk = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    cand = [c for c in train.columns if c != \"yield\" and c != \"state\"]\n    feats: List[str] = [c for c in cand if train[c].dtype.kind in numk]\n\n    med = train[feats].median(numeric_only=True)\n    X = train[feats].fillna(med)\n    X_test = test[feats].fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    years = np.sort(train[\"year\"].unique())\n    if len(years) >= 4:\n        vy = years[-1]\n        tr_mask = train[\"year\"].values != vy\n        va_mask = ~tr_mask\n        X_tr, y_tr = X.loc[tr_mask], y[tr_mask]\n        X_va, y_va = X.loc[va_mask], y[va_mask]\n    else:\n        rng = np.random.RandomState(42)\n        idx = np.arange(len(train))\n        rng.shuffle(idx)\n        cut = int(len(idx) * 0.85)\n        tr, va = idx[:cut], idx[cut:]\n        X_tr, y_tr = X.iloc[tr], y[tr]\n        X_va, y_va = X.iloc[va], y[va]\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"crop_phase\"] if c in X.columns]\n\n    m1 = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.2,\n        random_state=42,\n        n_jobs=-1,\n    )\n    m1.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        categorical_feature=cat_feats if cat_feats else \"auto\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n    p1 = m1.predict(X_test, num_iteration=m1.best_iteration_)\n\n    ylog = np.log1p(y)\n    m2 = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.2,\n        random_state=43,\n        n_jobs=-1,\n    )\n    m2.fit(\n        X_tr, ylog[train.index.isin(X_tr.index) if hasattr(X_tr, \"index\") else np.arange(len(ylog))[:len(X_tr)]],\n        eval_set=[(X_va, ylog[train.index.isin(X_va.index) if hasattr(X_va, \"index\") else np.arange(len(ylog))[:len(X_va)]] )],\n        eval_metric=\"rmse\",\n        categorical_feature=cat_feats if cat_feats else \"auto\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    p2 = np.expm1(m2.predict(X_test, num_iteration=m2.best_iteration_))\n\n    pred = 0.6 * p1 + 0.4 * p2\n    pred = np.clip(pred, 0, None)\n\n    out = test.copy()\n    out[\"yield\"] = pred\n    out = out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.02807589712379205, "mape": 0.07357106769433941, "rmse": 257.27495363179924}, "island_id": 1, "generation": 2, "metadata": {"iteration_time": 129.1810371875763, "changes": "Change 1: Replace 131 lines with 132 lines"}}

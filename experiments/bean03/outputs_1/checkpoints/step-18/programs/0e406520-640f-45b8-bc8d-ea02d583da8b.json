{"id": "0e406520-640f-45b8-bc8d-ea02d583da8b", "code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "language": "python", "parent_id": "8d905737-a8c2-4217-8ab4-6c7839d81b6e", "generation": 4, "timestamp": 1765793823.07494, "iteration_found": 16, "metrics": {"combined_score": 0.45356018296102407, "mape": 0.10447447762961996, "rmse": 582.9787314982473}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 133 lines with 76 lines", "parent_metrics": {"combined_score": 0.4376787923970922, "mape": 0.14414254490634923, "rmse": 744.9262556416859}, "island": 1}, "prompts": {"diff_user": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4377\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4213 \u2192 0.4377\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 133 lines with 84 lines\n- Metrics: combined_score: 0.4213, mape: 0.1881, rmse: 1042.7370\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 80 lines with 133 lines\n- Metrics: combined_score: 0.4377, mape: 0.1441, rmse: 744.9263\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 2 (Score: 0.4377)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4377), Performs well on mape (0.1441), Performs well on rmse (744.9263)\n\n### Program 3 (Score: 0.4213)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4213), Performs well on mape (0.1881), Performs well on rmse (1042.7370)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.0000)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = (df[\"year\"].astype(float) - 2000.0).astype(float)\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to error\n\n### Program D2 (Score: 0.0000)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = df[\"year\"].astype(float) - 2000.0\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # leakage-reduced target encoding (leave-one-out for train, smoothed mean for test)\n    y = train[\"yield\"].astype(float)\n    global_mean = float(y.mean())\n    alpha = 30.0\n\n    def _add_te(cols, name: str):\n        g = train.groupby(cols)[\"yield\"]\n        s = g.transform(\"sum\")\n        c = g.transform(\"count\")\n        te_tr = (s - y + alpha * global_mean) / (c - 1.0 + alpha)\n        te_tr = te_tr.where(c > 1, global_mean)\n        train[name] = te_tr.astype(float)\n\n        agg = train.groupby(cols)[\"yield\"].agg([\"sum\", \"count\"]).reset_index()\n        agg[name] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n        test[name] = (\n            test[cols].merge(agg[cols + [name]], on=cols, how=\"left\")[name].fillna(global_mean).astype(float)\n        )\n\n    _add_te([\"state_enc\"], \"te_state\")\n    _add_te([\"month\"], \"te_month\")\n    _add_te([\"state_enc\", \"month\"], \"te_state_month\")\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds_log = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y_log, categorical_feature=cat_feats)\n        preds_log += model.predict(X_test)\n\n    pred = np.expm1(preds_log / 2.0)\n    # small blend with strong seasonal/location prior\n    prior = test[\"te_state_month\"].to_numpy()\n    pred = 0.85 * pred + 0.15 * prior\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to error\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = (df[\"year\"].astype(float) - 2000.0).astype(float)\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 133 lines with 92 lines, Alternative combined_score approach, NumPy-based implementation\n\n### Inspiration 2 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (582.979), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.4213, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 133 lines with 84 lines, Alternative mape approach, Excellent rmse (1042.737)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}
{"iteration": 1, "timestamp": 1765792564.552657, "parent_id": "9e819819-db5f-4dc0-a56d-d6156309a46c", "child_id": "294921dd-55c0-4d98-977f-d713f104d85e", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.376171613486573, "mape": 0.33063117066457887, "rmse": 1218.253744642497}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.0769301262373821, "mape": 0.22508968912373412, "rmse": 620.218702813603}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 60.218008279800415, "changes": "Change 1: Replace 70 lines with 110 lines"}}
{"iteration": 2, "timestamp": 1765792627.427583, "parent_id": "9e819819-db5f-4dc0-a56d-d6156309a46c", "child_id": "ef1f80fe-d3a8-43db-8530-e42cf461502f", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.3761428425045881, "mape": 0.33073301641312935, "rmse": 1218.2912398997594}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.076958897219367, "mape": 0.2251915348722846, "rmse": 620.2561980708655}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 62.87387824058533, "changes": "Change 1: Replace 70 lines with 134 lines"}}
{"iteration": 3, "timestamp": 1765792686.754425, "parent_id": "294921dd-55c0-4d98-977f-d713f104d85e", "child_id": "e3eb8785-605e-4fb6-8b75-6f0b245d1a5b", "parent_metrics": {"combined_score": 0.376171613486573, "mape": 0.33063117066457887, "rmse": 1218.253744642497}, "child_metrics": {"combined_score": 0.4144391681226015, "mape": 0.20823549440372013, "rmse": 815.2504732387874}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.3762\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.3762\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Change 1: Replace 70 lines with 110 lines\n- Metrics: combined_score: 0.3762, mape: 0.3306, rmse: 1218.2537\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.3762)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.3762), Performs well on mape (0.3306), Performs well on rmse (1218.2537)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.03826755463602849, "mape": -0.12239567626085873, "rmse": -403.0032714037095}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 59.32571816444397, "changes": "Change 1: Replace 110 lines with 124 lines"}}
{"iteration": 4, "timestamp": 1765792748.6665587, "parent_id": "ef1f80fe-d3a8-43db-8530-e42cf461502f", "child_id": "81550c0c-d7d4-4632-bf1c-ebb6c66a291d", "parent_metrics": {"combined_score": 0.3761428425045881, "mape": 0.33073301641312935, "rmse": 1218.2912398997594}, "child_metrics": {"combined_score": 0.45356018296102407, "mape": 0.10447447762961996, "rmse": 582.9787314982473}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.3761\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.3761\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 134 lines\n- Metrics: combined_score: 0.3761, mape: 0.3307, rmse: 1218.2912\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 70 lines with 110 lines\n- Metrics: combined_score: 0.3762, mape: 0.3306, rmse: 1218.2537\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.3762)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.3762), Performs well on mape (0.3306), Performs well on rmse (1218.2537)\n\n### Program 3 (Score: 0.3761)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.3761), Performs well on mape (0.3307), Performs well on rmse (1218.2912)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.07741734045643595, "mape": -0.22625853878350938, "rmse": -635.3125084015121}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 61.90800428390503, "changes": "Change 1: Replace 134 lines with 80 lines"}}
{"iteration": 5, "timestamp": 1765792820.0644033, "parent_id": "e3eb8785-605e-4fb6-8b75-6f0b245d1a5b", "child_id": "dfb3b334-0ad8-4967-9d11-c2fa0f4c7a36", "parent_metrics": {"combined_score": 0.4144391681226015, "mape": 0.20823549440372013, "rmse": 815.2504732387874}, "child_metrics": {"combined_score": 0.43306048788804313, "mape": 0.15608315004369205, "rmse": 882.9461804411271}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # --------- CV training (GroupKFold by year) + log1p target ----------\n    from sklearn.model_selection import GroupKFold\n\n    X = train[features].copy()\n    X_te = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_te = X_te.fillna(med)\n\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0).values)\n    groups = train[\"year\"].astype(int).values if \"year\" in train.columns else np.arange(len(train))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    n_groups = len(np.unique(groups))\n    n_splits = int(min(5, n_groups)) if n_groups >= 2 else 1\n\n    params = dict(\n        n_estimators=20000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n\n    if n_splits == 1:\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        pred_log = model.predict(X_te)\n        test_pred = np.expm1(pred_log)\n    else:\n        gkf = GroupKFold(n_splits=n_splits)\n        pred_log_te = np.zeros(len(X_te), dtype=np.float64)\n\n        for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups)):\n            m = lgb.LGBMRegressor(**{**params, \"random_state\": 42 + fold * 17})\n            m.fit(\n                X.iloc[tr_idx],\n                y[tr_idx],\n                eval_set=[(X.iloc[va_idx], y[va_idx])],\n                eval_metric=\"l2\",\n                categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n                callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n            pred_log_te += m.predict(X_te, num_iteration=it) / n_splits\n\n        test_pred = np.expm1(pred_log_te)\n\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4144\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.3762 \u2192 0.4144\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 110 lines\n- Metrics: combined_score: 0.3762, mape: 0.3306, rmse: 1218.2537\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 110 lines with 124 lines\n- Metrics: combined_score: 0.4144, mape: 0.2082, rmse: 815.2505\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4144)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4144), Performs well on mape (0.2082), Performs well on rmse (815.2505)\n\n### Program 3 (Score: 0.3762)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.3762), Performs well on mape (0.3306), Performs well on rmse (1218.2537)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3761)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.3762, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 110 lines, Excellent rmse (1218.254), NumPy-based implementation\n\n### Inspiration 2 (Score: 0.3761, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 134 lines, Excellent rmse (1218.291), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n=======\n    # --------- CV training (GroupKFold by year) + log1p target ----------\n    from sklearn.model_selection import GroupKFold\n\n    X = train[features].copy()\n    X_te = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_te = X_te.fillna(med)\n\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0).values)\n    groups = train[\"year\"].astype(int).values if \"year\" in train.columns else np.arange(len(train))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    n_groups = len(np.unique(groups))\n    n_splits = int(min(5, n_groups)) if n_groups >= 2 else 1\n\n    params = dict(\n        n_estimators=20000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n\n    if n_splits == 1:\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        pred_log = model.predict(X_te)\n        test_pred = np.expm1(pred_log)\n    else:\n        gkf = GroupKFold(n_splits=n_splits)\n        pred_log_te = np.zeros(len(X_te), dtype=np.float64)\n\n        for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups)):\n            m = lgb.LGBMRegressor(**{**params, \"random_state\": 42 + fold * 17})\n            m.fit(\n                X.iloc[tr_idx],\n                y[tr_idx],\n                eval_set=[(X.iloc[va_idx], y[va_idx])],\n                eval_metric=\"l2\",\n                categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n                callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n            pred_log_te += m.predict(X_te, num_iteration=it) / n_splits\n\n        test_pred = np.expm1(pred_log_te)\n\n    test_pred = np.clip(test_pred, 0.0, None)\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.018621319765441624, "mape": -0.05215234436002808, "rmse": 67.69570720233969}, "island_id": 0, "generation": 3, "metadata": {"iteration_time": 71.3871397972107, "changes": "Change 1: Replace 46 lines with 59 lines"}}
{"iteration": 6, "timestamp": 1765792870.4739802, "parent_id": "ef1f80fe-d3a8-43db-8530-e42cf461502f", "child_id": "22ded253-36ab-4957-8989-c801664bbf35", "parent_metrics": {"combined_score": 0.3761428425045881, "mape": 0.33073301641312935, "rmse": 1218.2912398997594}, "child_metrics": {"combined_score": 0.45356018296102407, "mape": 0.10447447762961996, "rmse": 582.9787314982473}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.3761\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4144 \u2192 0.3761. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 110 lines with 124 lines\n- Metrics: combined_score: 0.4144, mape: 0.2082, rmse: 815.2505\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n### Attempt 1\n- Changes: Change 1: Replace 134 lines with 80 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 2 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 3 (Score: 0.4144)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4144), Performs well on mape (0.2082), Performs well on rmse (815.2505)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3761)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.3762)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 134 lines with 80 lines, Alternative mape approach, Excellent rmse (582.979)\n\n### Inspiration 2 (Score: 0.4144, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 110 lines with 124 lines, Alternative mape approach, Excellent rmse (815.250)\n\n### Inspiration 3 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.07741734045643595, "mape": -0.22625853878350938, "rmse": -635.3125084015121}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 50.40838360786438, "changes": "Change 1: Replace 134 lines with 72 lines"}}
{"iteration": 7, "timestamp": 1765792934.3444405, "parent_id": "294921dd-55c0-4d98-977f-d713f104d85e", "child_id": "9fb6a77c-f3e1-467b-bfce-9473fae40b13", "parent_metrics": {"combined_score": 0.376171613486573, "mape": 0.33063117066457887, "rmse": 1218.253744642497}, "child_metrics": {"combined_score": 0.454552556021338, "mape": 0.10202703141909038, "rmse": 591.9952449294428}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.3762\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4331 \u2192 0.3762. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 46 lines with 59 lines\n- Metrics: combined_score: 0.4331, mape: 0.1561, rmse: 882.9462\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n### Attempt 1\n- Changes: Change 1: Replace 134 lines with 80 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 2 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 3 (Score: 0.4331)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # --------- CV training (GroupKFold by year) + log1p target ----------\n    from sklearn.model_selection import GroupKFold\n\n    X = train[features].copy()\n    X_te = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_te = X_te.fillna(med)\n\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0).values)\n    groups = train[\"year\"].astype(int).values if \"year\" in train.columns else np.arange(len(train))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    n_groups = len(np.unique(groups))\n    n_splits = int(min(5, n_groups)) if n_groups >= 2 else 1\n\n    params = dict(\n        n_estimators=20000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n\n    if n_splits == 1:\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        pred_log = model.predict(X_te)\n        test_pred = np.expm1(pred_log)\n    else:\n        gkf = GroupKFold(n_splits=n_splits)\n        pred_log_te = np.zeros(len(X_te), dtype=np.float64)\n\n        for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups)):\n            m = lgb.LGBMRegressor(**{**params, \"random_state\": 42 + fold * 17})\n            m.fit(\n                X.iloc[tr_idx],\n                y[tr_idx],\n                eval_set=[(X.iloc[va_idx], y[va_idx])],\n                eval_metric=\"l2\",\n                categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n                callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n            pred_log_te += m.predict(X_te, num_iteration=it) / n_splits\n\n        test_pred = np.expm1(pred_log_te)\n\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4331), Performs well on mape (0.1561), Performs well on rmse (882.9462)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4144)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.3762)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 2 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 134 lines with 80 lines, Alternative mape approach, Excellent rmse (582.979)\n\n### Inspiration 3 (Score: 0.4331, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # --------- CV training (GroupKFold by year) + log1p target ----------\n    from sklearn.model_selection import GroupKFold\n\n    X = train[features].copy()\n    X_te = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_te = X_te.fillna(med)\n\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0).values)\n    groups = train[\"year\"].astype(int).values if \"year\" in train.columns else np.arange(len(train))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    n_groups = len(np.unique(groups))\n    n_splits = int(min(5, n_groups)) if n_groups >= 2 else 1\n\n    params = dict(\n        n_estimators=20000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n\n    if n_splits == 1:\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        pred_log = model.predict(X_te)\n        test_pred = np.expm1(pred_log)\n    else:\n        gkf = GroupKFold(n_splits=n_splits)\n        pred_log_te = np.zeros(len(X_te), dtype=np.float64)\n\n        for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups)):\n            m = lgb.LGBMRegressor(**{**params, \"random_state\": 42 + fold * 17})\n            m.fit(\n                X.iloc[tr_idx],\n                y[tr_idx],\n                eval_set=[(X.iloc[va_idx], y[va_idx])],\n                eval_metric=\"l2\",\n                categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n                callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n            pred_log_te += m.predict(X_te, num_iteration=it) / n_splits\n\n        test_pred = np.expm1(pred_log_te)\n\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 46 lines with 59 lines, Alternative mape approach, Excellent rmse (882.946)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.07838094253476496, "mape": -0.2286041392454885, "rmse": -626.2584997130541}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 63.808014154434204, "changes": "Change 1: Replace 110 lines with 78 lines"}}
{"iteration": 8, "timestamp": 1765793099.1477764, "parent_id": "55f960b7-0090-4c52-8c5a-6f9342d4363f", "child_id": "8d905737-a8c2-4217-8ab4-6c7839d81b6e", "parent_metrics": {"combined_score": 0.45356018296102407, "mape": 0.10447447762961996, "rmse": 582.9787314982473}, "child_metrics": {"combined_score": 0.4376787923970922, "mape": 0.14414254490634923, "rmse": 744.9262556416859}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4536\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4536\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.015881390563931896, "mape": 0.039668067276729266, "rmse": 161.9475241434386}, "island_id": 1, "generation": 3, "metadata": {"iteration_time": 164.80753588676453, "changes": "Change 1: Replace 80 lines with 133 lines"}}
{"iteration": 9, "timestamp": 1765793218.6390066, "parent_id": "81550c0c-d7d4-4632-bf1c-ebb6c66a291d", "child_id": "d7b79d49-2a84-46bc-bfcb-5a6bfd730b1c", "parent_metrics": {"combined_score": 0.45356018296102407, "mape": 0.10447447762961996, "rmse": 582.9787314982473}, "child_metrics": {"combined_score": 0.4521330430198769, "mape": 0.10791768374120854, "rmse": 597.1025380836257}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling (helps generalization)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    df[\"year_x_sin\"] = (df[\"year_rel\"] * df[\"month_sin\"]).astype(np.float32)\n    df[\"year_x_cos\"] = (df[\"year_rel\"] * df[\"month_cos\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4536\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4536\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 134 lines with 80 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 134 lines with 72 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 110 lines with 78 lines\n- Metrics: combined_score: 0.4546, mape: 0.1020, rmse: 591.9952\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4546)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4546), Performs well on mape (0.1020), Performs well on rmse (591.9952)\n\n### Program 2 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 3 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4331)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # --------- CV training (GroupKFold by year) + log1p target ----------\n    from sklearn.model_selection import GroupKFold\n\n    X = train[features].copy()\n    X_te = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_te = X_te.fillna(med)\n\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0).values)\n    groups = train[\"year\"].astype(int).values if \"year\" in train.columns else np.arange(len(train))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    n_groups = len(np.unique(groups))\n    n_splits = int(min(5, n_groups)) if n_groups >= 2 else 1\n\n    params = dict(\n        n_estimators=20000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n\n    if n_splits == 1:\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        pred_log = model.predict(X_te)\n        test_pred = np.expm1(pred_log)\n    else:\n        gkf = GroupKFold(n_splits=n_splits)\n        pred_log_te = np.zeros(len(X_te), dtype=np.float64)\n\n        for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups)):\n            m = lgb.LGBMRegressor(**{**params, \"random_state\": 42 + fold * 17})\n            m.fit(\n                X.iloc[tr_idx],\n                y[tr_idx],\n                eval_set=[(X.iloc[va_idx], y[va_idx])],\n                eval_metric=\"l2\",\n                categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n                callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n            pred_log_te += m.predict(X_te, num_iteration=it) / n_splits\n\n        test_pred = np.expm1(pred_log_te)\n\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 134 lines with 72 lines, Alternative mape approach, Excellent rmse (582.979)\n\n### Inspiration 2 (Score: 0.3762, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 110 lines, Excellent rmse (1218.254), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.4331, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # --------- CV training (GroupKFold by year) + log1p target ----------\n    from sklearn.model_selection import GroupKFold\n\n    X = train[features].copy()\n    X_te = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_te = X_te.fillna(med)\n\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0).values)\n    groups = train[\"year\"].astype(int).values if \"year\" in train.columns else np.arange(len(train))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    n_groups = len(np.unique(groups))\n    n_splits = int(min(5, n_groups)) if n_groups >= 2 else 1\n\n    params = dict(\n        n_estimators=20000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n\n    if n_splits == 1:\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        pred_log = model.predict(X_te)\n        test_pred = np.expm1(pred_log)\n    else:\n        gkf = GroupKFold(n_splits=n_splits)\n        pred_log_te = np.zeros(len(X_te), dtype=np.float64)\n\n        for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups)):\n            m = lgb.LGBMRegressor(**{**params, \"random_state\": 42 + fold * 17})\n            m.fit(\n                X.iloc[tr_idx],\n                y[tr_idx],\n                eval_set=[(X.iloc[va_idx], y[va_idx])],\n                eval_metric=\"l2\",\n                categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n                callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n            pred_log_te += m.predict(X_te, num_iteration=it) / n_splits\n\n        test_pred = np.expm1(pred_log_te)\n\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 46 lines with 59 lines, Alternative mape approach, Excellent rmse (882.946)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling (helps generalization)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    df[\"year_x_sin\"] = (df[\"year_rel\"] * df[\"month_sin\"]).astype(np.float32)\n    df[\"year_x_cos\"] = (df[\"year_rel\"] * df[\"month_cos\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.001427139941147182, "mape": 0.003443206111588576, "rmse": 14.123806585378475}, "island_id": 0, "generation": 3, "metadata": {"iteration_time": 119.4787049293518, "changes": "Change 1: Replace 80 lines with 89 lines"}}
{"iteration": 10, "timestamp": 1765793429.776654, "parent_id": "8d905737-a8c2-4217-8ab4-6c7839d81b6e", "child_id": "b22f2d1b-9fb5-4e7c-a7d5-4bcad9805b7b", "parent_metrics": {"combined_score": 0.4376787923970922, "mape": 0.14414254490634923, "rmse": 744.9262556416859}, "child_metrics": {"combined_score": 0.42130306156689346, "mape": 0.1881451241482136, "rmse": 1042.7370306355226}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4377\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4377\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Change 1: Replace 80 lines with 133 lines\n- Metrics: combined_score: 0.4377, mape: 0.1441, rmse: 744.9263\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 2 (Score: 0.4377)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4377), Performs well on mape (0.1441), Performs well on rmse (744.9263)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (582.979), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.01637573083019872, "mape": 0.044002579241864376, "rmse": 297.81077499383673}, "island_id": 1, "generation": 4, "metadata": {"iteration_time": 211.14892673492432, "changes": "Change 1: Replace 133 lines with 84 lines"}}
{"iteration": 11, "timestamp": 1765793488.7905395, "parent_id": "294921dd-55c0-4d98-977f-d713f104d85e", "child_id": "7dfbbd2c-cc65-441f-a108-7ba61510fb74", "parent_metrics": {"combined_score": 0.376171613486573, "mape": 0.33063117066457887, "rmse": 1218.253744642497}, "child_metrics": {"combined_score": 0.45231253980434083, "mape": 0.10747924173523025, "rmse": 596.507905578307}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=3200,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.3762\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4536 \u2192 0.3762. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 134 lines with 80 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 134 lines with 72 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 110 lines with 78 lines\n- Metrics: combined_score: 0.4546, mape: 0.1020, rmse: 591.9952\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4546)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4546), Performs well on mape (0.1020), Performs well on rmse (591.9952)\n\n### Program 2 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 3 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4521)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling (helps generalization)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    df[\"year_x_sin\"] = (df[\"year_rel\"] * df[\"month_sin\"]).astype(np.float32)\n    df[\"year_x_cos\"] = (df[\"year_rel\"] * df[\"month_cos\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 134 lines with 72 lines, Alternative mape approach, Excellent rmse (582.979)\n\n### Inspiration 2 (Score: 0.4144, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 110 lines with 124 lines, Alternative mape approach, Excellent rmse (815.250)\n\n### Inspiration 3 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 134 lines with 80 lines, Alternative mape approach, Excellent rmse (582.979)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=3200,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.07614092631776781, "mape": -0.2231519289293486, "rmse": -621.7458390641899}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 59.001089334487915, "changes": "Change 1: Replace 110 lines with 80 lines"}}
{"iteration": 12, "timestamp": 1765793548.3881521, "parent_id": "b22f2d1b-9fb5-4e7c-a7d5-4bcad9805b7b", "child_id": "740749a5-e0a3-4fdb-b420-0c40d41c7eeb", "parent_metrics": {"combined_score": 0.42130306156689346, "mape": 0.1881451241482136, "rmse": 1042.7370306355226}, "child_metrics": {"combined_score": 0.0, "error": "lightgbm.sklearn.LGBMRegressor() got multiple values for keyword argument 'random_state'"}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = df[\"year\"].astype(float) - 2000.0\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # leakage-reduced target encoding (leave-one-out for train, smoothed mean for test)\n    y = train[\"yield\"].astype(float)\n    global_mean = float(y.mean())\n    alpha = 30.0\n\n    def _add_te(cols, name: str):\n        g = train.groupby(cols)[\"yield\"]\n        s = g.transform(\"sum\")\n        c = g.transform(\"count\")\n        te_tr = (s - y + alpha * global_mean) / (c - 1.0 + alpha)\n        te_tr = te_tr.where(c > 1, global_mean)\n        train[name] = te_tr.astype(float)\n\n        agg = train.groupby(cols)[\"yield\"].agg([\"sum\", \"count\"]).reset_index()\n        agg[name] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n        test[name] = (\n            test[cols].merge(agg[cols + [name]], on=cols, how=\"left\")[name].fillna(global_mean).astype(float)\n        )\n\n    _add_te([\"state_enc\"], \"te_state\")\n    _add_te([\"month\"], \"te_month\")\n    _add_te([\"state_enc\", \"month\"], \"te_state_month\")\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds_log = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y_log, categorical_feature=cat_feats)\n        preds_log += model.predict(X_test)\n\n    pred = np.expm1(preds_log / 2.0)\n    # small blend with strong seasonal/location prior\n    prior = test[\"te_state_month\"].to_numpy()\n    pred = 0.85 * pred + 0.15 * prior\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4213\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4213\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 133 lines with 84 lines\n- Metrics: combined_score: 0.4213, mape: 0.1881, rmse: 1042.7370\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 80 lines with 133 lines\n- Metrics: combined_score: 0.4377, mape: 0.1441, rmse: 744.9263\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 2 (Score: 0.4377)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4377), Performs well on mape (0.1441), Performs well on rmse (744.9263)\n\n### Program 3 (Score: 0.4213)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4213), Performs well on mape (0.1881), Performs well on rmse (1042.7370)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4377, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 80 lines with 133 lines, Alternative mape approach, Excellent rmse (744.926)\n\n### Inspiration 2 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (582.979), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n=======\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = df[\"year\"].astype(float) - 2000.0\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n=======\n    # leakage-reduced target encoding (leave-one-out for train, smoothed mean for test)\n    y = train[\"yield\"].astype(float)\n    global_mean = float(y.mean())\n    alpha = 30.0\n\n    def _add_te(cols, name: str):\n        g = train.groupby(cols)[\"yield\"]\n        s = g.transform(\"sum\")\n        c = g.transform(\"count\")\n        te_tr = (s - y + alpha * global_mean) / (c - 1.0 + alpha)\n        te_tr = te_tr.where(c > 1, global_mean)\n        train[name] = te_tr.astype(float)\n\n        agg = train.groupby(cols)[\"yield\"].agg([\"sum\", \"count\"]).reset_index()\n        agg[name] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n        test[name] = (\n            test[cols].merge(agg[cols + [name]], on=cols, how=\"left\")[name].fillna(global_mean).astype(float)\n        )\n\n    _add_te([\"state_enc\"], \"te_state\")\n    _add_te([\"month\"], \"te_month\")\n    _add_te([\"state_enc\", \"month\"], \"te_state_month\")\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds_log = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y_log, categorical_feature=cat_feats)\n        preds_log += model.predict(X_test)\n\n    pred = np.expm1(preds_log / 2.0)\n    # small blend with strong seasonal/location prior\n    prior = test[\"te_state_month\"].to_numpy()\n    pred = 0.85 * pred + 0.15 * prior\n    pred = np.clip(pred, 0.0, None)\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.42130306156689346}, "island_id": 1, "generation": 5, "metadata": {"iteration_time": 59.59904098510742, "changes": "Change 1: Replace 14 lines with 14 lines\nChange 2: Replace 35 lines with 63 lines"}}
{"iteration": 13, "timestamp": 1765793666.1735244, "parent_id": "81550c0c-d7d4-4632-bf1c-ebb6c66a291d", "child_id": "c5cd5c68-5ba0-461e-93a8-4fd40f5a9490", "parent_metrics": {"combined_score": 0.45356018296102407, "mape": 0.10447447762961996, "rmse": 582.9787314982473}, "child_metrics": {"combined_score": 0.43162285449589305, "mape": 0.15986693465056986, "rmse": 926.7789940606955}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    phase = np.where(m >= 10, m - 10, m + 2).astype(int)\n    df[\"months_since_crop_start\"] = phase\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling + interactions (keep original year too)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"].astype(np.float32) * phase.astype(np.float32)).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log1p target for stability / MAPE robustness\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0).values)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    base_params = dict(\n        n_estimators=3200,\n        learning_rate=0.025,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        objective=\"regression\",\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    # small seed ensemble to reduce variance\n    preds_log = np.zeros(len(X_test), dtype=np.float64)\n    for i, rs in enumerate([42, 202, 777]):\n        model = lgb.LGBMRegressor(**{**base_params, \"random_state\": rs})\n        model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        preds_log += model.predict(X_test) / 3.0\n\n    pred = np.expm1(preds_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4536\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4536\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 134 lines with 80 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 134 lines with 72 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 110 lines with 78 lines\n- Metrics: combined_score: 0.4546, mape: 0.1020, rmse: 591.9952\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4546)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4546), Performs well on mape (0.1020), Performs well on rmse (591.9952)\n\n### Program 2 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 3 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4523)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=3200,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4521, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling (helps generalization)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    df[\"year_x_sin\"] = (df[\"year_rel\"] * df[\"month_sin\"]).astype(np.float32)\n    df[\"year_x_cos\"] = (df[\"year_rel\"] * df[\"month_cos\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 80 lines with 89 lines, Alternative mape approach, Excellent rmse (597.103)\n\n### Inspiration 2 (Score: 0.4331, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # --------- CV training (GroupKFold by year) + log1p target ----------\n    from sklearn.model_selection import GroupKFold\n\n    X = train[features].copy()\n    X_te = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_te = X_te.fillna(med)\n\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0).values)\n    groups = train[\"year\"].astype(int).values if \"year\" in train.columns else np.arange(len(train))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    n_groups = len(np.unique(groups))\n    n_splits = int(min(5, n_groups)) if n_groups >= 2 else 1\n\n    params = dict(\n        n_estimators=20000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n\n    if n_splits == 1:\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        pred_log = model.predict(X_te)\n        test_pred = np.expm1(pred_log)\n    else:\n        gkf = GroupKFold(n_splits=n_splits)\n        pred_log_te = np.zeros(len(X_te), dtype=np.float64)\n\n        for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups)):\n            m = lgb.LGBMRegressor(**{**params, \"random_state\": 42 + fold * 17})\n            m.fit(\n                X.iloc[tr_idx],\n                y[tr_idx],\n                eval_set=[(X.iloc[va_idx], y[va_idx])],\n                eval_metric=\"l2\",\n                categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n                callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False)],\n            )\n            it = getattr(m, \"best_iteration_\", None)\n            pred_log_te += m.predict(X_te, num_iteration=it) / n_splits\n\n        test_pred = np.expm1(pred_log_te)\n\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 46 lines with 59 lines, Alternative mape approach, Excellent rmse (882.946)\n\n### Inspiration 3 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 134 lines with 72 lines, Alternative mape approach, Excellent rmse (582.979)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    phase = np.where(m >= 10, m - 10, m + 2).astype(int)\n    df[\"months_since_crop_start\"] = phase\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling + interactions (keep original year too)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"].astype(np.float32) * phase.astype(np.float32)).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log1p target for stability / MAPE robustness\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0).values)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    base_params = dict(\n        n_estimators=3200,\n        learning_rate=0.025,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.05,\n        reg_lambda=0.6,\n        objective=\"regression\",\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    # small seed ensemble to reduce variance\n    preds_log = np.zeros(len(X_test), dtype=np.float64)\n    for i, rs in enumerate([42, 202, 777]):\n        model = lgb.LGBMRegressor(**{**base_params, \"random_state\": rs})\n        model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        preds_log += model.predict(X_test) / 3.0\n\n    pred = np.expm1(preds_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.021937328465131023, "mape": 0.0553924570209499, "rmse": 343.8002625624482}, "island_id": 0, "generation": 3, "metadata": {"iteration_time": 117.78190588951111, "changes": "Change 1: Replace 80 lines with 95 lines"}}
{"iteration": 14, "timestamp": 1765793714.8501058, "parent_id": "8d905737-a8c2-4217-8ab4-6c7839d81b6e", "child_id": "e2455573-309e-4b3a-ab06-97788a72c348", "parent_metrics": {"combined_score": 0.4376787923970922, "mape": 0.14414254490634923, "rmse": 744.9262556416859}, "child_metrics": {"combined_score": 0.0, "error": "lightgbm.sklearn.LGBMRegressor() got multiple values for keyword argument 'random_state'"}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = (df[\"year\"].astype(float) - 2000.0).astype(float)\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4377\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4213 \u2192 0.4377\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 133 lines with 84 lines\n- Metrics: combined_score: 0.4213, mape: 0.1881, rmse: 1042.7370\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 80 lines with 133 lines\n- Metrics: combined_score: 0.4377, mape: 0.1441, rmse: 744.9263\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 2 (Score: 0.4377)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4377), Performs well on mape (0.1441), Performs well on rmse (744.9263)\n\n### Program 3 (Score: 0.4213)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4213), Performs well on mape (0.1881), Performs well on rmse (1042.7370)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.0000)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = df[\"year\"].astype(float) - 2000.0\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # leakage-reduced target encoding (leave-one-out for train, smoothed mean for test)\n    y = train[\"yield\"].astype(float)\n    global_mean = float(y.mean())\n    alpha = 30.0\n\n    def _add_te(cols, name: str):\n        g = train.groupby(cols)[\"yield\"]\n        s = g.transform(\"sum\")\n        c = g.transform(\"count\")\n        te_tr = (s - y + alpha * global_mean) / (c - 1.0 + alpha)\n        te_tr = te_tr.where(c > 1, global_mean)\n        train[name] = te_tr.astype(float)\n\n        agg = train.groupby(cols)[\"yield\"].agg([\"sum\", \"count\"]).reset_index()\n        agg[name] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n        test[name] = (\n            test[cols].merge(agg[cols + [name]], on=cols, how=\"left\")[name].fillna(global_mean).astype(float)\n        )\n\n    _add_te([\"state_enc\"], \"te_state\")\n    _add_te([\"month\"], \"te_month\")\n    _add_te([\"state_enc\", \"month\"], \"te_state_month\")\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds_log = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y_log, categorical_feature=cat_feats)\n        preds_log += model.predict(X_test)\n\n    pred = np.expm1(preds_log / 2.0)\n    # small blend with strong seasonal/location prior\n    prior = test[\"te_state_month\"].to_numpy()\n    pred = 0.85 * pred + 0.15 * prior\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to error\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4213, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 133 lines with 84 lines, Alternative mape approach, Excellent rmse (1042.737)\n\n### Inspiration 2 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (582.979), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = df[\"year\"].astype(float) - 2000.0\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # leakage-reduced target encoding (leave-one-out for train, smoothed mean for test)\n    y = train[\"yield\"].astype(float)\n    global_mean = float(y.mean())\n    alpha = 30.0\n\n    def _add_te(cols, name: str):\n        g = train.groupby(cols)[\"yield\"]\n        s = g.transform(\"sum\")\n        c = g.transform(\"count\")\n        te_tr = (s - y + alpha * global_mean) / (c - 1.0 + alpha)\n        te_tr = te_tr.where(c > 1, global_mean)\n        train[name] = te_tr.astype(float)\n\n        agg = train.groupby(cols)[\"yield\"].agg([\"sum\", \"count\"]).reset_index()\n        agg[name] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n        test[name] = (\n            test[cols].merge(agg[cols + [name]], on=cols, how=\"left\")[name].fillna(global_mean).astype(float)\n        )\n\n    _add_te([\"state_enc\"], \"te_state\")\n    _add_te([\"month\"], \"te_month\")\n    _add_te([\"state_enc\", \"month\"], \"te_state_month\")\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds_log = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y_log, categorical_feature=cat_feats)\n        preds_log += model.predict(X_test)\n\n    pred = np.expm1(preds_log / 2.0)\n    # small blend with strong seasonal/location prior\n    prior = test[\"te_state_month\"].to_numpy()\n    pred = 0.85 * pred + 0.15 * prior\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 14 lines with 14 lines\nChange 2: Replace 35 lines with 63 lines, Alternative combined_score approach, NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = (df[\"year\"].astype(float) - 2000.0).astype(float)\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.4376787923970922}, "island_id": 1, "generation": 4, "metadata": {"iteration_time": 48.68542718887329, "changes": "Change 1: Replace 133 lines with 92 lines"}}
{"iteration": 15, "timestamp": 1765793777.7580173, "parent_id": "9e819819-db5f-4dc0-a56d-d6156309a46c", "child_id": "00c980e4-860c-4532-9f05-b71a6f0371ed", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.41859906914814926, "mape": 0.19574354939417798, "rmse": 1112.0718801123073}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.7,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if cat_feats else \"auto\")\n\n    pred = np.expm1(model.predict(X_test))\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4536 \u2192 0.4531. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 134 lines with 80 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 134 lines with 72 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 110 lines with 78 lines\n- Metrics: combined_score: 0.4546, mape: 0.1020, rmse: 591.9952\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4546)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4546), Performs well on mape (0.1020), Performs well on rmse (591.9952)\n\n### Program 2 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 3 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4523)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=3200,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 134 lines with 80 lines, Alternative mape approach, Excellent rmse (582.979)\n\n### Inspiration 2 (Score: 0.4546, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 110 lines with 78 lines, Alternative mape approach, Excellent rmse (591.995)\n\n### Inspiration 3 (Score: 0.3761, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    df[\"month_sin\"] = np.sin(2.0 * np.pi * m / 12.0)\n    df[\"month_cos\"] = np.cos(2.0 * np.pi * m / 12.0)\n    df[\"month_sq\"] = m * m\n    # simple interactions that often help tree models\n    df[\"state_x_month\"] = df[\"state_enc\"].astype(float) * m\n    df[\"state_x_phase\"] = df[\"state_enc\"].astype(float) * df[\"months_since_crop_start\"].astype(float)\n    return df\n\n\ndef _target_encode(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str = \"yield\",\n    alpha: float = 20.0,\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Smoothed target encoding computed on full train; mapped onto train/test.\"\"\"\n    global_mean = float(train[target].mean())\n    agg = train.groupby(cols, dropna=False)[target].agg([\"mean\", \"count\"]).reset_index()\n    agg[\"te\"] = (agg[\"mean\"] * agg[\"count\"] + global_mean * alpha) / (agg[\"count\"] + alpha)\n    key = cols\n    tr = train[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    te = test[key].merge(agg[key + [\"te\"]], on=key, how=\"left\")[\"te\"].fillna(global_mean)\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # base transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # smoothed target encodings (strong for region + season)\n    for cols in ([\"state_enc\"], [\"month\"], [\"year\"], [\"state_enc\", \"month\"], [\"state_enc\", \"months_since_crop_start\"]):\n        tr_te, te_te = _target_encode(train, test, cols=cols, target=\"yield\", alpha=30.0)\n        name = \"te_\" + \"_\".join(cols)\n        train[name] = tr_te.astype(float)\n        test[name] = te_te.astype(float)\n\n    # feature selection (numeric only; keep engineered TE/time features)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # robust missing handling\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    # log-transform helps RMSE+MAPE tradeoff; clip at 0 on inverse\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # simple time-aware validation for early stopping (hold out last year if possible)\n    last_year = int(train[\"year\"].max()) if \"year\" in train.columns else None\n    use_val = last_year is not None and (train[\"year\"] == last_year).sum() >= 50 and (train[\"year\"] != last_year).sum() >= 200\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    if use_val:\n        tr_idx = train[\"year\"] != last_year\n        va_idx = ~tr_idx\n        model.fit(\n            X.loc[tr_idx],\n            y.loc[tr_idx],\n            eval_set=[(X.loc[va_idx], y.loc[va_idx])],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        pred_log = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        model.fit(X, y)\n        pred_log = model.predict(X_test)\n\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 134 lines, Excellent rmse (1218.291), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=0.7,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if cat_feats else \"auto\")\n\n    pred = np.expm1(model.predict(X_test))\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.03450267057580586, "mape": 0.09020206785333322, "rmse": 514.0368382834134}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 62.901495695114136, "changes": "Change 1: Replace 70 lines with 77 lines"}}
{"iteration": 16, "timestamp": 1765793823.0873353, "parent_id": "8d905737-a8c2-4217-8ab4-6c7839d81b6e", "child_id": "0e406520-640f-45b8-bc8d-ea02d583da8b", "parent_metrics": {"combined_score": 0.4376787923970922, "mape": 0.14414254490634923, "rmse": 744.9262556416859}, "child_metrics": {"combined_score": 0.45356018296102407, "mape": 0.10447447762961996, "rmse": 582.9787314982473}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4377\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4213 \u2192 0.4377\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 133 lines with 84 lines\n- Metrics: combined_score: 0.4213, mape: 0.1881, rmse: 1042.7370\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 80 lines with 133 lines\n- Metrics: combined_score: 0.4377, mape: 0.1441, rmse: 744.9263\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 2 (Score: 0.4377)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4377), Performs well on mape (0.1441), Performs well on rmse (744.9263)\n\n### Program 3 (Score: 0.4213)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4213), Performs well on mape (0.1881), Performs well on rmse (1042.7370)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.0000)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = (df[\"year\"].astype(float) - 2000.0).astype(float)\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to error\n\n### Program D2 (Score: 0.0000)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = df[\"year\"].astype(float) - 2000.0\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # leakage-reduced target encoding (leave-one-out for train, smoothed mean for test)\n    y = train[\"yield\"].astype(float)\n    global_mean = float(y.mean())\n    alpha = 30.0\n\n    def _add_te(cols, name: str):\n        g = train.groupby(cols)[\"yield\"]\n        s = g.transform(\"sum\")\n        c = g.transform(\"count\")\n        te_tr = (s - y + alpha * global_mean) / (c - 1.0 + alpha)\n        te_tr = te_tr.where(c > 1, global_mean)\n        train[name] = te_tr.astype(float)\n\n        agg = train.groupby(cols)[\"yield\"].agg([\"sum\", \"count\"]).reset_index()\n        agg[name] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n        test[name] = (\n            test[cols].merge(agg[cols + [name]], on=cols, how=\"left\")[name].fillna(global_mean).astype(float)\n        )\n\n    _add_te([\"state_enc\"], \"te_state\")\n    _add_te([\"month\"], \"te_month\")\n    _add_te([\"state_enc\", \"month\"], \"te_state_month\")\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=6000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds_log = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y_log, categorical_feature=cat_feats)\n        preds_log += model.predict(X_test)\n\n    pred = np.expm1(preds_log / 2.0)\n    # small blend with strong seasonal/location prior\n    prior = test[\"te_state_month\"].to_numpy()\n    pred = 0.85 * pred + 0.15 * prior\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to error\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        yc = (df[\"year\"].astype(float) - 2000.0).astype(float)\n        df[\"year_centered\"] = yc\n        df[\"year2\"] = yc * yc\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**params, random_state=sd)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 133 lines with 92 lines, Alternative combined_score approach, NumPy-based implementation\n\n### Inspiration 2 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (582.979), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.4213, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 133 lines with 84 lines, Alternative mape approach, Excellent rmse (1042.737)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.015881390563931896, "mape": -0.039668067276729266, "rmse": -161.9475241434386}, "island_id": 1, "generation": 4, "metadata": {"iteration_time": 45.324339866638184, "changes": "Change 1: Replace 133 lines with 76 lines"}}
{"iteration": 17, "timestamp": 1765793881.7522175, "parent_id": "d7b79d49-2a84-46bc-bfcb-5a6bfd730b1c", "child_id": "13bceabb-dda4-4d7a-8e30-484ff9eba61e", "parent_metrics": {"combined_score": 0.4521330430198769, "mape": 0.10791768374120854, "rmse": 597.1025380836257}, "child_metrics": {"combined_score": 0.41377944142848666, "mape": 0.21014691348095846, "rmse": 823.4474608447152}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling (helps generalization)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    df[\"year_x_sin\"] = (df[\"year_rel\"] * df[\"month_sin\"]).astype(np.float32)\n    df[\"year_x_cos\"] = (df[\"year_rel\"] * df[\"month_cos\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef add_target_encoding(\n    train: pd.DataFrame, test: pd.DataFrame, cols: List[str], name: str, smooth: float = 20.0\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Simple smoothed mean encoding computed on train, applied to both train/test.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[name] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [name]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[name] = train[name].fillna(global_mean).astype(np.float32)\n    test[name] = test[name].fillna(global_mean).astype(np.float32)\n    return train, test\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    # target encodings (often strong with state/month IDs)\n    train, test = add_target_encoding(train, test, [\"state\"], \"te_state\", smooth=30.0)\n    train, test = add_target_encoding(train, test, [\"month\"], \"te_month\", smooth=30.0)\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], \"te_state_month\", smooth=15.0)\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], \"te_state_cropm\", smooth=15.0)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    # time-based validation: last year\n    years = train[\"year\"].astype(int).values\n    max_year = int(years.max())\n    is_val = years == max_year\n    has_val = bool(is_val.any() and (~is_val).any())\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        yt = np.log1p(y) if use_log else y\n        model = lgb.LGBMRegressor(\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.1,\n            reg_lambda=0.3,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if has_val:\n            X_tr, y_tr = X.loc[~is_val], yt[~is_val]\n            X_va, y_va = X.loc[is_val], yt[is_val]\n            model.fit(\n                X_tr,\n                y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"l2\",\n                categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n                callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False)],\n            )\n        else:\n            model.fit(X, yt, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        return model, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    for m, use_log in models:\n        it = getattr(m, \"best_iteration_\", None)\n        p = m.predict(X_test, num_iteration=it)\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    pred = np.mean(np.vstack(preds), axis=0)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4521\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4536 \u2192 0.4521. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 134 lines with 80 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 134 lines with 72 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 110 lines with 78 lines\n- Metrics: combined_score: 0.4546, mape: 0.1020, rmse: 591.9952\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4546)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4546), Performs well on mape (0.1020), Performs well on rmse (591.9952)\n\n### Program 2 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 3 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4523)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=3200,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.3762, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List, Optional\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year like index (Oct=0 ... Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    # cyclic month features\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_target_encoding(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    smooth: float = 20.0,\n    name: Optional[str] = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    key = name or (\"te_\" + \"_\".join(cols))\n    global_mean = train[\"yield\"].mean()\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[key] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [key]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[key] = train[key].fillna(global_mean)\n    test[key] = test[key].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # target encodings (train-only statistics)\n    train, test = add_target_encoding(train, test, [\"state\"], smooth=30.0, name=\"te_state\")\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], smooth=15.0, name=\"te_state_month\")\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], smooth=15.0, name=\"te_state_cropm\")\n\n    # simple interactions\n    train[\"year2\"] = train[\"year\"].astype(float) ** 2\n    test[\"year2\"] = test[\"year\"].astype(float) ** 2\n    train[\"state_year\"] = train[\"state_enc\"].astype(float) * train[\"year\"].astype(float)\n    test[\"state_year\"] = test[\"state_enc\"].astype(float) * test[\"year\"].astype(float)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log-transform target to improve MAPE stability\n    y = np.log1p(train[\"yield\"].astype(float).values)\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], y[~is_val]\n    X_va, y_va = train.loc[is_val, features], y[is_val]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.2,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)] if len(X_va) else None,\n        eval_metric=\"l2\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)] if len(X_va) else None,\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 110 lines, Excellent rmse (1218.254), NumPy-based implementation\n\n### Inspiration 2 (Score: 0.4523, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=3200,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 110 lines with 80 lines, Alternative mape approach, Excellent rmse (596.508)\n\n### Inspiration 3 (Score: 0.4144, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 110 lines with 124 lines, Alternative mape approach, Excellent rmse (815.250)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling (helps generalization)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    df[\"year_x_sin\"] = (df[\"year_rel\"] * df[\"month_sin\"]).astype(np.float32)\n    df[\"year_x_cos\"] = (df[\"year_rel\"] * df[\"month_cos\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling (helps generalization)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    df[\"year_x_sin\"] = (df[\"year_rel\"] * df[\"month_sin\"]).astype(np.float32)\n    df[\"year_x_cos\"] = (df[\"year_rel\"] * df[\"month_cos\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef add_target_encoding(\n    train: pd.DataFrame, test: pd.DataFrame, cols: List[str], name: str, smooth: float = 20.0\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Simple smoothed mean encoding computed on train, applied to both train/test.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n    agg = train.groupby(cols, dropna=False)[\"yield\"].agg([\"mean\", \"count\"]).reset_index()\n    agg[name] = (agg[\"mean\"] * agg[\"count\"] + global_mean * smooth) / (agg[\"count\"] + smooth)\n    agg = agg[cols + [name]]\n    train = train.merge(agg, on=cols, how=\"left\")\n    test = test.merge(agg, on=cols, how=\"left\")\n    train[name] = train[name].fillna(global_mean).astype(np.float32)\n    test[name] = test[name].fillna(global_mean).astype(np.float32)\n    return train, test\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    # target encodings (often strong with state/month IDs)\n    train, test = add_target_encoding(train, test, [\"state\"], \"te_state\", smooth=30.0)\n    train, test = add_target_encoding(train, test, [\"month\"], \"te_month\", smooth=30.0)\n    train, test = add_target_encoding(train, test, [\"state\", \"month\"], \"te_state_month\", smooth=15.0)\n    train, test = add_target_encoding(train, test, [\"state\", \"months_since_crop_start\"], \"te_state_cropm\", smooth=15.0)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    # time-based validation: last year\n    years = train[\"year\"].astype(int).values\n    max_year = int(years.max())\n    is_val = years == max_year\n    has_val = bool(is_val.any() and (~is_val).any())\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        yt = np.log1p(y) if use_log else y\n        model = lgb.LGBMRegressor(\n            n_estimators=12000,\n            learning_rate=0.02,\n            num_leaves=127,\n            min_child_samples=20,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.1,\n            reg_lambda=0.3,\n            random_state=seed,\n            n_jobs=-1,\n        )\n        if has_val:\n            X_tr, y_tr = X.loc[~is_val], yt[~is_val]\n            X_va, y_va = X.loc[is_val], yt[is_val]\n            model.fit(\n                X_tr,\n                y_tr,\n                eval_set=[(X_va, y_va)],\n                eval_metric=\"l2\",\n                categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n                callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False)],\n            )\n        else:\n            model.fit(X, yt, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n        return model, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    for m, use_log in models:\n        it = getattr(m, \"best_iteration_\", None)\n        p = m.predict(X_test, num_iteration=it)\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    pred = np.mean(np.vstack(preds), axis=0)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.03835360159139023, "mape": 0.10222922973974992, "rmse": 226.3449227610895}, "island_id": 0, "generation": 4, "metadata": {"iteration_time": 58.66208481788635, "changes": "Change 1: Replace 89 lines with 133 lines"}}
{"iteration": 18, "timestamp": 1765794141.4544535, "parent_id": "0e406520-640f-45b8-bc8d-ea02d583da8b", "child_id": "36b3ba73-1c74-45a0-9a6a-6d3d3ff4c41b", "parent_metrics": {"combined_score": 0.45356018296102407, "mape": 0.10447447762961996, "rmse": 582.9787314982473}, "child_metrics": {"combined_score": 0.45668931851385475, "mape": 0.09691387250884614, "rmse": 577.0234699934333}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # categorical interactions (often better than numeric interactions for trees)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_phase\"] = (df[\"state_enc\"] * 20 + df[\"months_since_crop_start\"]).astype(int)\n\n    # mild scaling for year if present\n    if \"year\" in df.columns:\n        y = df[\"year\"].astype(float)\n        df[\"year_centered\"] = (y - 2000.0).astype(float)\n        df[\"year_sq\"] = (df[\"year_centered\"] ** 2).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\n            \"state_enc\",\n            \"month\",\n            \"months_since_crop_start\",\n            \"quarter\",\n            \"state_month\",\n            \"state_quarter\",\n            \"state_phase\",\n        ]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=4500,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.9,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**{**params, \"random_state\": sd})\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4536\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4377 \u2192 0.4536\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 80 lines with 133 lines\n- Metrics: combined_score: 0.4377, mape: 0.1441, rmse: 744.9263\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Improvement in all metrics\n\n### Attempt 1\n- Changes: Change 1: Replace 133 lines with 76 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 2 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 3 (Score: 0.4377)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4377), Performs well on mape (0.1441), Performs well on rmse (744.9263)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4213)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4213, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 133 lines with 84 lines, Alternative mape approach, Excellent rmse (1042.737)\n\n### Inspiration 2 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (582.979), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.4377, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 80 lines with 133 lines, Alternative mape approach, Excellent rmse (744.926)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # categorical interactions (often better than numeric interactions for trees)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_phase\"] = (df[\"state_enc\"] * 20 + df[\"months_since_crop_start\"]).astype(int)\n\n    # mild scaling for year if present\n    if \"year\" in df.columns:\n        y = df[\"year\"].astype(float)\n        df[\"year_centered\"] = (y - 2000.0).astype(float)\n        df[\"year_sq\"] = (df[\"year_centered\"] ** 2).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\n            \"state_enc\",\n            \"month\",\n            \"months_since_crop_start\",\n            \"quarter\",\n            \"state_month\",\n            \"state_quarter\",\n            \"state_phase\",\n        ]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=4500,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.9,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**{**params, \"random_state\": sd})\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.0031291355528306752, "mape": -0.0075606051207738195, "rmse": -5.955261504813961}, "island_id": 1, "generation": 5, "metadata": {"iteration_time": 259.6930890083313, "changes": "Change 1: Replace 76 lines with 104 lines"}}
{"iteration": 19, "timestamp": 1765794258.383021, "parent_id": "d7b79d49-2a84-46bc-bfcb-5a6bfd730b1c", "child_id": "e9f4112e-f071-442b-950e-00db0394a65c", "parent_metrics": {"combined_score": 0.4521330430198769, "mape": 0.10791768374120854, "rmse": 597.1025380836257}, "child_metrics": {"combined_score": 0.4446871841079634, "mape": 0.12617619863241872, "rmse": 706.2881915232928}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling (helps generalization)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    df[\"year_x_sin\"] = (df[\"year_rel\"] * df[\"month_sin\"]).astype(np.float32)\n    df[\"year_x_cos\"] = (df[\"year_rel\"] * df[\"month_cos\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef add_loo_te(train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leave-one-out target encoding for train; mean encoding for test.\"\"\"\n    gmean = float(train[\"yield\"].mean())\n    agg = train.groupby(keys, dropna=False)[\"yield\"].agg(_sum=\"sum\", _cnt=\"count\").reset_index()\n    tr = train.merge(agg, on=keys, how=\"left\")\n    te = test.merge(agg, on=keys, how=\"left\")\n    tr[name] = np.where(tr[\"_cnt\"] > 1, (tr[\"_sum\"] - tr[\"yield\"]) / (tr[\"_cnt\"] - 1), gmean)\n    te[name] = (te[\"_sum\"] / te[\"_cnt\"]).astype(float)\n    tr[name] = tr[name].fillna(gmean).astype(np.float32)\n    te[name] = te[name].fillna(gmean).astype(np.float32)\n    tr = tr.drop(columns=[\"_sum\", \"_cnt\"])\n    te = te.drop(columns=[\"_sum\", \"_cnt\"])\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    # leakage-controlled encodings\n    train, test = add_loo_te(train, test, [\"state_enc\"], \"te_state\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"month\"], \"te_state_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"months_since_crop_start\"], \"te_state_phase\")\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    params = dict(\n        n_estimators=3600,\n        learning_rate=0.025,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.7,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    m1.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    m2 = lgb.LGBMRegressor(**params)\n    m2.fit(X, np.log1p(y), categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    p1 = m1.predict(X_test)\n    p2 = np.expm1(m2.predict(X_test))\n    pred = np.clip(0.55 * p1 + 0.45 * p2, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4521\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4536 \u2192 0.4521. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 134 lines with 80 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 134 lines with 72 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 110 lines with 78 lines\n- Metrics: combined_score: 0.4546, mape: 0.1020, rmse: 591.9952\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4546)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4546), Performs well on mape (0.1020), Performs well on rmse (591.9952)\n\n### Program 2 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 3 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4523)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=3200,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 134 lines with 80 lines, Alternative mape approach, Excellent rmse (582.979)\n\n### Inspiration 2 (Score: 0.4144, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state\"] = df[\"state\"].astype(str)\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    return df\n\ndef add_past_mean_by_year(\n    train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leakage-safe per-row mean: for train uses ONLY years < current year; for test uses all train years.\"\"\"\n    global_mean = float(train[\"yield\"].mean())\n\n    yearly = (\n        train.groupby(keys + [\"year\"], dropna=False)[\"yield\"]\n        .mean()\n        .reset_index(name=\"_y\")\n        .sort_values(\"year\")\n    )\n    # expanding mean per key, shifted so current year is not used\n    yearly[name] = yearly.groupby(keys)[\"_y\"].transform(lambda s: s.expanding().mean().shift(1))\n    train = train.merge(yearly[keys + [\"year\", name]], on=keys + [\"year\"], how=\"left\")\n    train[name] = train[name].fillna(global_mean)\n\n    # test: mean over all available years in train (fallback to global mean)\n    agg_all = train.groupby(keys, dropna=False)[\"yield\"].mean().reset_index(name=name)\n    test = test.merge(agg_all, on=keys, how=\"left\")\n    test[name] = test[name].fillna(global_mean)\n    return train, test\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    # leakage-safe historical means\n    train, test = add_past_mean_by_year(train, test, [\"state\"], \"pm_state\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"month\"], \"pm_state_month\")\n    train, test = add_past_mean_by_year(train, test, [\"state\", \"months_since_crop_start\"], \"pm_state_cropm\")\n\n    # light interactions\n    train[\"state_year\"] = train[\"state_enc\"].astype(np.float32) * train[\"year\"].astype(np.float32)\n    test[\"state_year\"] = test[\"state_enc\"].astype(np.float32) * test[\"year\"].astype(np.float32)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [c for c in train.columns if c != \"yield\"]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # time-based validation (last year as validation)\n    max_year = int(train[\"year\"].max())\n    is_val = train[\"year\"].astype(int).values == max_year\n    X_tr, y_tr = train.loc[~is_val, features], train.loc[~is_val, \"yield\"].astype(float).values\n    X_va, y_va = train.loc[is_val, features], train.loc[is_val, \"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"quarter\", \"months_since_crop_start\"] if c in features]\n\n    def fit_one(seed: int, use_log: bool):\n        ytr = np.log1p(y_tr) if use_log else y_tr\n        yva = np.log1p(y_va) if use_log else y_va\n\n        m = lgb.LGBMRegressor(\n            n_estimators=4000,\n            learning_rate=0.03,\n            num_leaves=63,\n            min_child_samples=30,\n            subsample=0.85,\n            subsample_freq=1,\n            colsample_bytree=0.85,\n            reg_alpha=0.2,\n            reg_lambda=0.4,\n            random_state=seed,\n            objective=\"regression\",\n        )\n        m.fit(\n            X_tr,\n            ytr,\n            eval_set=[(X_va, yva)] if len(X_va) else None,\n            eval_metric=\"l2\",\n            categorical_feature=cat_feats if len(cat_feats) else \"auto\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)] if len(X_va) else None,\n        )\n        return m, use_log\n\n    models = [fit_one(42, False), fit_one(52, True)]\n    preds = []\n    X_te = test[features]\n    for m, use_log in models:\n        p = m.predict(X_te, num_iteration=getattr(m, \"best_iteration_\", None))\n        if use_log:\n            p = np.expm1(p)\n        preds.append(p)\n\n    test_pred = np.mean(np.vstack(preds), axis=0)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 110 lines with 124 lines, Alternative mape approach, Excellent rmse (815.250)\n\n### Inspiration 3 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 134 lines with 72 lines, Alternative mape approach, Excellent rmse (582.979)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling (helps generalization)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    df[\"year_x_sin\"] = (df[\"year_rel\"] * df[\"month_sin\"]).astype(np.float32)\n    df[\"year_x_cos\"] = (df[\"year_rel\"] * df[\"month_cos\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # year scaling (helps generalization)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    df[\"state_x_year\"] = (df[\"state_enc\"].astype(np.float32) * df[\"year_rel\"]).astype(np.float32)\n    df[\"year_x_sin\"] = (df[\"year_rel\"] * df[\"month_sin\"]).astype(np.float32)\n    df[\"year_x_cos\"] = (df[\"year_rel\"] * df[\"month_cos\"]).astype(np.float32)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2800,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame, min_year: int) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    y = df[\"year\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang).astype(np.float32)\n    df[\"month_cos\"] = np.cos(ang).astype(np.float32)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"year_rel\"] = (y - int(min_year)).astype(np.float32)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(np.float32)\n    return df\n\n\ndef add_loo_te(train: pd.DataFrame, test: pd.DataFrame, keys: List[str], name: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Leave-one-out target encoding for train; mean encoding for test.\"\"\"\n    gmean = float(train[\"yield\"].mean())\n    agg = train.groupby(keys, dropna=False)[\"yield\"].agg(_sum=\"sum\", _cnt=\"count\").reset_index()\n    tr = train.merge(agg, on=keys, how=\"left\")\n    te = test.merge(agg, on=keys, how=\"left\")\n    tr[name] = np.where(tr[\"_cnt\"] > 1, (tr[\"_sum\"] - tr[\"yield\"]) / (tr[\"_cnt\"] - 1), gmean)\n    te[name] = (te[\"_sum\"] / te[\"_cnt\"]).astype(float)\n    tr[name] = tr[name].fillna(gmean).astype(np.float32)\n    te[name] = te[name].fillna(gmean).astype(np.float32)\n    tr = tr.drop(columns=[\"_sum\", \"_cnt\"])\n    te = te.drop(columns=[\"_sum\", \"_cnt\"])\n    return tr, te\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    min_year = int(train[\"year\"].min())\n    train = add_features(train, min_year=min_year)\n    test = add_features(test, min_year=min_year)\n\n    # leakage-controlled encodings\n    train, test = add_loo_te(train, test, [\"state_enc\"], \"te_state\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"month\"], \"te_state_month\")\n    train, test = add_loo_te(train, test, [\"state_enc\", \"months_since_crop_start\"], \"te_state_phase\")\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float).values\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    params = dict(\n        n_estimators=3600,\n        learning_rate=0.025,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.7,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    m1 = lgb.LGBMRegressor(**params)\n    m1.fit(X, y, categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    m2 = lgb.LGBMRegressor(**params)\n    m2.fit(X, np.log1p(y), categorical_feature=cat_feats if len(cat_feats) else \"auto\")\n\n    p1 = m1.predict(X_test)\n    p2 = np.expm1(m2.predict(X_test))\n    pred = np.clip(0.55 * p1 + 0.45 * p2, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.0074458589119135166, "mape": 0.01825851489121018, "rmse": 109.18565343966702}, "island_id": 0, "generation": 4, "metadata": {"iteration_time": 116.92953395843506, "changes": "Change 1: Replace 89 lines with 104 lines"}}
{"iteration": 20, "timestamp": 1765794445.4551568, "parent_id": "b22f2d1b-9fb5-4e7c-a7d5-4bcad9805b7b", "child_id": "bd51f7c3-c4e6-415e-97fb-96d06ebca6eb", "parent_metrics": {"combined_score": 0.42130306156689346, "mape": 0.1881451241482136, "rmse": 1042.7370306355226}, "child_metrics": {"combined_score": 0.45619929438051465, "mape": 0.09807738205435178, "rmse": 581.7651343398403}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # categorical interactions (strong for tree models)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_phase\"] = (df[\"state_enc\"] * 20 + df[\"months_since_crop_start\"]).astype(int)\n\n    # keep a light numeric interaction too\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n\n    if \"year\" in df.columns:\n        yc = (df[\"year\"].astype(float) - 2000.0).astype(float)\n        df[\"year_centered\"] = yc\n        df[\"year_sq\"] = (yc ** 2).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\n            \"state_enc\",\n            \"month\",\n            \"months_since_crop_start\",\n            \"quarter\",\n            \"state_month\",\n            \"state_quarter\",\n            \"state_phase\",\n        ]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=4500,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.9,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**{**params, \"random_state\": sd})\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "You are optimizing machine learning code for predicting Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set.\n\nHard constraints:\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\n- Keep the script runnable standalone: read train and test data, write submission.csv, the format of submission.csv must be same with test_answer.csv.\n do not drop all features, you can combine them to generate new features for better performance.\n- You can use any other standard Python libraries for better model.\n\nFreedom:\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\n\nOutput format:\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.\n", "user": "# Current Program Information\n- Fitness: 0.4213\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 0.4536 \u2192 0.4213. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Improvement in all metrics\n\n### Attempt 2\n- Changes: Change 1: Replace 133 lines with 76 lines\n- Metrics: combined_score: 0.4536, mape: 0.1045, rmse: 582.9787\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 76 lines with 104 lines\n- Metrics: combined_score: 0.4567, mape: 0.0969, rmse: 577.0235\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4567)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # categorical interactions (often better than numeric interactions for trees)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_phase\"] = (df[\"state_enc\"] * 20 + df[\"months_since_crop_start\"]).astype(int)\n\n    # mild scaling for year if present\n    if \"year\" in df.columns:\n        y = df[\"year\"].astype(float)\n        df[\"year_centered\"] = (y - 2000.0).astype(float)\n        df[\"year_sq\"] = (df[\"year_centered\"] ** 2).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\n            \"state_enc\",\n            \"month\",\n            \"months_since_crop_start\",\n            \"quarter\",\n            \"state_month\",\n            \"state_quarter\",\n            \"state_phase\",\n        ]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=4500,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.9,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**{**params, \"random_state\": sd})\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4567), Performs well on mape (0.0969), Performs well on rmse (577.0235)\n\n### Program 2 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n### Program 3 (Score: 0.4536)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    # crop-year phase (Oct=0..Sep=11)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    # light interactions\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # treat key IDs as categorical (often better than ordinal)\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = model.predict(X_test)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4536), Performs well on mape (0.1045), Performs well on rmse (582.9787)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4213)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4377)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4377, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    # mild year scaling (helps tree split stability if year is large)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef _target_encode_oof(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    group_cols: List[str],\n    target_col: str,\n    n_splits: int = 5,\n    alpha: float = 20.0,\n    seed: int = 42,\n) -> Tuple[pd.Series, pd.Series, str]:\n    \"\"\"\n    Smoothed mean target encoding with OOF values for train to reduce leakage.\n    te = (sum + alpha*global_mean) / (count + alpha)\n    \"\"\"\n    global_mean = float(train_df[target_col].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.empty(len(train_df), dtype=float)\n\n    for tr_idx, val_idx in kf.split(train_df):\n        tr = train_df.iloc[tr_idx]\n        agg = tr.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n        agg[\"te\"] = (agg[\"sum\"] + alpha * global_mean) / (agg[\"count\"] + alpha)\n\n        val = train_df.iloc[val_idx][group_cols].merge(agg[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"]\n        oof[val_idx] = val.fillna(global_mean).to_numpy()\n\n    agg_full = train_df.groupby(group_cols)[target_col].agg([\"sum\", \"count\"]).reset_index()\n    agg_full[\"te\"] = (agg_full[\"sum\"] + alpha * global_mean) / (agg_full[\"count\"] + alpha)\n    te_test = test_df[group_cols].merge(agg_full[group_cols + [\"te\"]], on=group_cols, how=\"left\")[\"te\"].fillna(global_mean)\n\n    name = \"te_\" + \"_\".join(group_cols)\n    return pd.Series(oof, index=train_df.index, name=name), te_test.rename(name), name\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    # OOF target encodings (additive signal for location/seasonality)\n    te_specs = [[\"state_enc\"], [\"month\"], [\"state_enc\", \"month\"]]\n    if \"year\" in train.columns:\n        te_specs.append([\"state_enc\", \"year\"])\n    te_feature_names: List[str] = []\n    for cols in te_specs:\n        tr_te, te_te, nm = _target_encode_oof(train, test, cols, \"yield\", n_splits=5, alpha=30.0, seed=42)\n        train[nm] = tr_te\n        test[nm] = te_te\n        te_feature_names.append(nm)\n\n    # numeric features (keep all original numeric cols + engineered)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n    # ensure TE features are included even if dtype inference differs\n    for nm in te_feature_names:\n        if nm not in features:\n            features.append(nm)\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n    y_log = np.log1p(np.clip(y.to_numpy(), 0.0, None))\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        n_estimators=5000,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y_log, categorical_feature=cat_feats)\n\n    pred_log = model.predict(X_test)\n    pred = np.expm1(pred_log)\n    pred = np.clip(pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 80 lines with 133 lines, Alternative mape approach, Excellent rmse (744.926)\n\n### Inspiration 2 (Score: 0.4567, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # categorical interactions (often better than numeric interactions for trees)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_phase\"] = (df[\"state_enc\"] * 20 + df[\"months_since_crop_start\"]).astype(int)\n\n    # mild scaling for year if present\n    if \"year\" in df.columns:\n        y = df[\"year\"].astype(float)\n        df[\"year_centered\"] = (y - 2000.0).astype(float)\n        df[\"year_sq\"] = (df[\"year_centered\"] ** 2).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [\n        c\n        for c in [\n            \"state_enc\",\n            \"month\",\n            \"months_since_crop_start\",\n            \"quarter\",\n            \"state_month\",\n            \"state_quarter\",\n            \"state_phase\",\n        ]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=4500,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.9,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**{**params, \"random_state\": sd})\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 76 lines with 104 lines, Alternative mape approach, Excellent rmse (577.023)\n\n### Inspiration 3 (Score: 0.4536, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\"] if c in features]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=2500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.5,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(X, y, categorical_feature=cat_feats)\n\n    pred = np.clip(model.predict(X_test), 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 133 lines with 76 lines, Alternative mape approach, Excellent rmse (582.979)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, s2i = encode_state(train)\n    test, _ = encode_state(test, mapping=s2i)\n    train = add_features(train)\n    test = add_features(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    med = X.median(numeric_only=True)\n    X = X.fillna(med)\n    X_test = X_test.fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"Model trained. Submission saved to: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n    if \"year\" in df.columns:\n        df[\"year_centered\"] = (df[\"year\"].astype(float) - 2000.0).astype(float)\n    return df\n=======\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"quarter\"] = ((m - 1) // 3 + 1).astype(int)\n\n    # categorical interactions (strong for tree models)\n    df[\"state_month\"] = (df[\"state_enc\"] * 13 + m).astype(int)\n    df[\"state_quarter\"] = (df[\"state_enc\"] * 10 + df[\"quarter\"]).astype(int)\n    df[\"state_phase\"] = (df[\"state_enc\"] * 20 + df[\"months_since_crop_start\"]).astype(int)\n\n    # keep a light numeric interaction too\n    df[\"state_x_phase\"] = (df[\"state_enc\"] * df[\"months_since_crop_start\"]).astype(float)\n\n    if \"year\" in df.columns:\n        yc = (df[\"year\"].astype(float) - 2000.0).astype(float)\n        df[\"year_centered\"] = yc\n        df[\"year_sq\"] = (yc ** 2).astype(float)\n    return df\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    cat_feats = [c for c in [\"state_enc\", \"month\", \"months_since_crop_start\", \"quarter\", \"state_month\", \"state_quarter\"] if c in features]\n\n    params = dict(\n        objective=\"tweedie\",\n        tweedie_variance_power=1.2,\n        n_estimators=3500,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=0.6,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(random_state=sd, **params)\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n=======\n    cat_feats = [\n        c\n        for c in [\n            \"state_enc\",\n            \"month\",\n            \"months_since_crop_start\",\n            \"quarter\",\n            \"state_month\",\n            \"state_quarter\",\n            \"state_phase\",\n        ]\n        if c in features\n    ]\n\n    params = dict(\n        objective=\"regression\",\n        n_estimators=4500,\n        learning_rate=0.02,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.9,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        n_jobs=-1,\n    )\n\n    preds = np.zeros(len(X_test), dtype=float)\n    for sd in (42, 7, 202):\n        model = lgb.LGBMRegressor(**{**params, \"random_state\": sd})\n        model.fit(X, y, categorical_feature=cat_feats)\n        preds += model.predict(X_test)\n    pred = np.clip(preds / 3.0, 0.0, None)\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.0348962328136212, "mape": -0.09006774209386183, "rmse": -460.9718962956823}, "island_id": 1, "generation": 5, "metadata": {"iteration_time": 187.0747001171112, "changes": "Change 1: Replace 14 lines with 22 lines\nChange 2: Replace 23 lines with 34 lines"}}

{"id": "cd9b9b7e-c0a4-4850-8267-8680c864b0bf", "code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "language": "python", "parent_id": "6d9d581c-1298-49cd-a613-6554f3646ba6", "generation": 1, "timestamp": 1765795407.8066177, "iteration_found": 2, "metrics": {"combined_score": 0.37600068272995874, "mape": 0.3312332371433151, "rmse": 1221.1983806148971}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 70 lines with 116 lines", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "island": 0}, "prompts": {"diff_user": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}
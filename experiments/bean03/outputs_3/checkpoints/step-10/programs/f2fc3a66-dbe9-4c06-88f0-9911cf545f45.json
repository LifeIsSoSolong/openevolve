{"id": "f2fc3a66-dbe9-4c06-88f0-9911cf545f45", "code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "language": "python", "parent_id": "37f7ea35-46f9-47c2-a1a4-ab87cac1034b", "generation": 2, "timestamp": 1765795552.9027066, "iteration_found": 5, "metrics": {"combined_score": 0.4550332384259089, "mape": 0.10090963012575396, "rmse": 578.1370339613667}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 145 lines with 67 lines", "parent_metrics": {"combined_score": 0.4048816689326994, "mape": 0.23600953258134025, "rmse": 1411.2518777714163}, "island": 0}, "prompts": {"diff_user": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4049\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4049\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 145 lines\n- Metrics: combined_score: 0.4049, mape: 0.2360, rmse: 1411.2519\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 70 lines with 138 lines\n- Metrics: combined_score: 0.4327, mape: 0.1576, rmse: 646.4251\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4327)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4327), Performs well on mape (0.1576), Performs well on rmse (646.4251)\n\n### Program 3 (Score: 0.4049)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4049), Performs well on mape (0.2360), Performs well on rmse (1411.2519)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3760)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 2 (Score: 0.4327, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 138 lines, Alternative mape approach, Excellent rmse (646.425)\n\n### Inspiration 3 (Score: 0.3760, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 116 lines, Excellent rmse (1221.198), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}
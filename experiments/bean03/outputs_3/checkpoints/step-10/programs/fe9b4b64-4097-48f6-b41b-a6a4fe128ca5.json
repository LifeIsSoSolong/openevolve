{"id": "fe9b4b64-4097-48f6-b41b-a6a4fe128ca5", "code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(int)\n    df[\"year_centered\"] = (y - y.min()).astype(int)\n    return df\n\ndef _smoothed_mean(stats: pd.DataFrame, global_mean: float, smooth: float) -> pd.Series:\n    # stats has columns: [\"mean\",\"count\"]\n    return (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n\ndef add_target_encoding_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    y: pd.Series,\n    cols: List[str],\n    n_splits: int = 5,\n    smooth: float = 20.0,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n\n    key_name = \"_\".join(cols)\n    te_col = f\"te_{key_name}\"\n    global_mean = float(y.mean())\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = pd.Series(index=tr.index, dtype=float)\n\n    # precompute key for speed/determinism\n    tr_key = tr[cols].astype(str).agg(\"|\".join, axis=1)\n    te_key = te[cols].astype(str).agg(\"|\".join, axis=1)\n\n    for tr_idx, va_idx in kf.split(tr):\n        stats = (\n            pd.DataFrame({\"key\": tr_key.iloc[tr_idx].values, \"y\": y.iloc[tr_idx].values})\n            .groupby(\"key\")[\"y\"]\n            .agg([\"mean\", \"count\"])\n        )\n        mapping = _smoothed_mean(stats, global_mean, smooth).to_dict()\n        oof.iloc[va_idx] = tr_key.iloc[va_idx].map(mapping).fillna(global_mean).astype(float)\n\n    # test mapping from full train\n    stats_full = (\n        pd.DataFrame({\"key\": tr_key.values, \"y\": y.values})\n        .groupby(\"key\")[\"y\"]\n        .agg([\"mean\", \"count\"])\n    )\n    mapping_full = _smoothed_mean(stats_full, global_mean, smooth).to_dict()\n    te_vals = te_key.map(mapping_full).fillna(global_mean).astype(float)\n\n    tr[te_col] = oof.values\n    te[te_col] = te_vals.values\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    y = train[\"yield\"].astype(float)\n\n    # Out-of-fold target encoding to reduce leakage/overfit\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\"], n_splits=5, smooth=30.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"month\"], n_splits=5, smooth=50.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"months_since_crop_start\"], n_splits=5, smooth=50.0)\n\n    # numeric-only features (state kept as state_enc + TE)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X_all = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log-target to better match MAPE-like behavior; ensure positivity on output\n    y_log = np.log1p(y.clip(lower=0.0).values)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(X_test), dtype=float)\n\n    for tr_idx, va_idx in kf.split(X_all):\n        X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n        y_tr, y_va = y_log[tr_idx], y_log[va_idx]\n\n        model = lgb.LGBMRegressor(\n            n_estimators=6000,\n            learning_rate=0.03,\n            num_leaves=127,\n            min_child_samples=25,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_alpha=0.0,\n            reg_lambda=1.0,\n            random_state=42,\n            objective=\"regression\",\n            n_jobs=-1,\n        )\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "language": "python", "parent_id": "3d8d3931-4570-4450-9cdc-d750cbf73a28", "generation": 3, "timestamp": 1765795813.9965944, "iteration_found": 8, "metrics": {"combined_score": 0.4009676171805823, "mape": 0.24800693617116382, "rmse": 1519.5969956653557}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 67 lines with 152 lines", "parent_metrics": {"combined_score": 0.4550332384259089, "mape": 0.10090963012575396, "rmse": 578.1370339613667}, "island": 1}, "prompts": {"diff_user": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4550\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4550\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4550, mape: 0.1009, rmse: 578.1370\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4550)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4550), Performs well on mape (0.1009), Performs well on rmse (578.1370)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(int)\n    df[\"year_centered\"] = (y - y.min()).astype(int)\n    return df\n\ndef _smoothed_mean(stats: pd.DataFrame, global_mean: float, smooth: float) -> pd.Series:\n    # stats has columns: [\"mean\",\"count\"]\n    return (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n\ndef add_target_encoding_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    y: pd.Series,\n    cols: List[str],\n    n_splits: int = 5,\n    smooth: float = 20.0,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n\n    key_name = \"_\".join(cols)\n    te_col = f\"te_{key_name}\"\n    global_mean = float(y.mean())\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = pd.Series(index=tr.index, dtype=float)\n\n    # precompute key for speed/determinism\n    tr_key = tr[cols].astype(str).agg(\"|\".join, axis=1)\n    te_key = te[cols].astype(str).agg(\"|\".join, axis=1)\n\n    for tr_idx, va_idx in kf.split(tr):\n        stats = (\n            pd.DataFrame({\"key\": tr_key.iloc[tr_idx].values, \"y\": y.iloc[tr_idx].values})\n            .groupby(\"key\")[\"y\"]\n            .agg([\"mean\", \"count\"])\n        )\n        mapping = _smoothed_mean(stats, global_mean, smooth).to_dict()\n        oof.iloc[va_idx] = tr_key.iloc[va_idx].map(mapping).fillna(global_mean).astype(float)\n\n    # test mapping from full train\n    stats_full = (\n        pd.DataFrame({\"key\": tr_key.values, \"y\": y.values})\n        .groupby(\"key\")[\"y\"]\n        .agg([\"mean\", \"count\"])\n    )\n    mapping_full = _smoothed_mean(stats_full, global_mean, smooth).to_dict()\n    te_vals = te_key.map(mapping_full).fillna(global_mean).astype(float)\n\n    tr[te_col] = oof.values\n    te[te_col] = te_vals.values\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    y = train[\"yield\"].astype(float)\n\n    # Out-of-fold target encoding to reduce leakage/overfit\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\"], n_splits=5, smooth=30.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"month\"], n_splits=5, smooth=50.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"months_since_crop_start\"], n_splits=5, smooth=50.0)\n\n    # numeric-only features (state kept as state_enc + TE)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X_all = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log-target to better match MAPE-like behavior; ensure positivity on output\n    y_log = np.log1p(y.clip(lower=0.0).values)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(X_test), dtype=float)\n\n    for tr_idx, va_idx in kf.split(X_all):\n        X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n        y_tr, y_va = y_log[tr_idx], y_log[va_idx]\n\n        model = lgb.LGBMRegressor(\n            n_estimators=6000,\n            learning_rate=0.03,\n            num_leaves=127,\n            min_child_samples=25,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_alpha=0.0,\n            reg_lambda=1.0,\n            random_state=42,\n            objective=\"regression\",\n            n_jobs=-1,\n        )\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}
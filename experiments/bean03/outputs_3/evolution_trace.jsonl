{"iteration": 1, "timestamp": 1765795364.6277997, "parent_id": "6d9d581c-1298-49cd-a613-6554f3646ba6", "child_id": "37f7ea35-46f9-47c2-a1a4-ab87cac1034b", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.4048816689326994, "mape": 0.23600953258134025, "rmse": 1411.2518777714163}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.04822007079125573, "mape": 0.13046805104049547, "rmse": 813.2168359425224}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 84.46290063858032, "changes": "Change 1: Replace 70 lines with 145 lines"}}
{"iteration": 2, "timestamp": 1765795407.8242903, "parent_id": "6d9d581c-1298-49cd-a613-6554f3646ba6", "child_id": "cd9b9b7e-c0a4-4850-8267-8680c864b0bf", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.37600068272995874, "mape": 0.3312332371433151, "rmse": 1221.1983806148971}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.07710105699399639, "mape": 0.22569175560247035, "rmse": 623.1633387860032}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 43.19271922111511, "changes": "Change 1: Replace 70 lines with 116 lines"}}
{"iteration": 3, "timestamp": 1765795460.5998943, "parent_id": "6d9d581c-1298-49cd-a613-6554f3646ba6", "child_id": "44e986eb-25d8-4269-938f-b6c89bcf2c9a", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.43271866072765647, "mape": 0.15755110816146986, "rmse": 646.4251160594703}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4049 \u2192 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Change 1: Replace 70 lines with 145 lines\n- Metrics: combined_score: 0.4049, mape: 0.2360, rmse: 1411.2519\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4049)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4049), Performs well on mape (0.2360), Performs well on rmse (1411.2519)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4049, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 145 lines, Alternative mape approach, Excellent rmse (1411.252)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.020383078996298654, "mape": 0.052009626620625096, "rmse": 48.390074230576374}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 52.77628231048584, "changes": "Change 1: Replace 70 lines with 138 lines"}}
{"iteration": 4, "timestamp": 1765795508.0561638, "parent_id": "37f7ea35-46f9-47c2-a1a4-ab87cac1034b", "child_id": "d02fdc7c-ae0d-4b83-b5b3-87603abac232", "parent_metrics": {"combined_score": 0.4048816689326994, "mape": 0.23600953258134025, "rmse": 1411.2518777714163}, "child_metrics": {"combined_score": 0.45310862647230254, "mape": 0.10552505009932613, "rmse": 597.9168667979126}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols and train[c].dtype.kind in numeric_kinds]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[\"yield\"].astype(float))\n\n    test_pred = model.predict(test[features])\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4049\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.3760 \u2192 0.4049\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 116 lines\n- Metrics: combined_score: 0.3760, mape: 0.3312, rmse: 1221.1984\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 70 lines with 145 lines\n- Metrics: combined_score: 0.4049, mape: 0.2360, rmse: 1411.2519\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4049)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4049), Performs well on mape (0.2360), Performs well on rmse (1411.2519)\n\n### Program 3 (Score: 0.3760)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.3760), Performs well on mape (0.3312), Performs well on rmse (1221.1984)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols and train[c].dtype.kind in numeric_kinds]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[\"yield\"].astype(float))\n\n    test_pred = model.predict(test[features])\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.048226957539603155, "mape": -0.1304844824820141, "rmse": -813.3350109735037}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 47.45469093322754, "changes": "Change 1: Replace 145 lines with 56 lines"}}
{"iteration": 5, "timestamp": 1765795552.9236648, "parent_id": "37f7ea35-46f9-47c2-a1a4-ab87cac1034b", "child_id": "f2fc3a66-dbe9-4c06-88f0-9911cf545f45", "parent_metrics": {"combined_score": 0.4048816689326994, "mape": 0.23600953258134025, "rmse": 1411.2518777714163}, "child_metrics": {"combined_score": 0.4550332384259089, "mape": 0.10090963012575396, "rmse": 578.1370339613667}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4049\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4049\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 145 lines\n- Metrics: combined_score: 0.4049, mape: 0.2360, rmse: 1411.2519\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 70 lines with 138 lines\n- Metrics: combined_score: 0.4327, mape: 0.1576, rmse: 646.4251\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 2 (Score: 0.4327)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4327), Performs well on mape (0.1576), Performs well on rmse (646.4251)\n\n### Program 3 (Score: 0.4049)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4049), Performs well on mape (0.2360), Performs well on rmse (1411.2519)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3760)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 2 (Score: 0.4327, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 138 lines, Alternative mape approach, Excellent rmse (646.425)\n\n### Inspiration 3 (Score: 0.3760, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 116 lines, Excellent rmse (1221.198), NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.050151569493209536, "mape": -0.13509990245558628, "rmse": -833.1148438100496}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 44.86111760139465, "changes": "Change 1: Replace 145 lines with 67 lines"}}
{"iteration": 6, "timestamp": 1765795591.156543, "parent_id": "44e986eb-25d8-4269-938f-b6c89bcf2c9a", "child_id": "b5fbaf42-11b1-4a20-817b-b510cb3eef90", "parent_metrics": {"combined_score": 0.43271866072765647, "mape": 0.15755110816146986, "rmse": 646.4251160594703}, "child_metrics": {"combined_score": 0.0, "error": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: state_cat: object"}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int).astype(\"category\")\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1800,\n        learning_rate=0.04,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[\"yield\"].astype(float), categorical_feature=[\"state_enc\"])\n\n    test_pred = model.predict(test[features])\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4327\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4327\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 70 lines with 138 lines\n- Metrics: combined_score: 0.4327, mape: 0.1576, rmse: 646.4251\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n### Attempt 1\n- Changes: Change 1: Replace 145 lines with 56 lines\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 597.9169\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols and train[c].dtype.kind in numeric_kinds]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[\"yield\"].astype(float))\n\n    test_pred = model.predict(test[features])\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (597.9169)\n\n### Program 2 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n### Program 3 (Score: 0.4327)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4327), Performs well on mape (0.1576), Performs well on rmse (646.4251)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.3760)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4049)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols and train[c].dtype.kind in numeric_kinds]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[\"yield\"].astype(float))\n\n    test_pred = model.predict(test[features])\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 145 lines with 56 lines, Alternative mape approach, Excellent rmse (597.917)\n\n### Inspiration 2 (Score: 0.3760, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = np.where(m >= 10, m - 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(float)\n    ang = 2.0 * np.pi * (m - 1.0) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - y.mean()\n    return df\n\ndef add_target_encodings(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    gmean = float(tr[\"yield\"].mean())\n\n    def merge_mean(keys, name):\n        m = tr.groupby(keys, dropna=False)[\"yield\"].mean().rename(name).reset_index()\n        return m\n\n    for keys, name in [\n        ([\"state_enc\"], \"te_state\"),\n        ([\"state_enc\", \"month\"], \"te_state_month\"),\n        ([\"state_enc\", \"year\"], \"te_state_year\"),\n    ]:\n        mdf = merge_mean(keys, name)\n        tr = tr.merge(mdf, on=keys, how=\"left\")\n        te = te.merge(mdf, on=keys, how=\"left\")\n        tr[name] = tr[name].fillna(gmean)\n        te[name] = te[name].fillna(gmean)\n\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    train, test = add_target_encodings(train, test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # time-aware split by year (fallback to random split if only one year)\n    years = sorted(train[\"year\"].dropna().unique().tolist())\n    if len(years) >= 2:\n        n_val = min(2, max(1, len(years) // 4))\n        val_years = set(years[-n_val:])\n        tr_idx = ~train[\"year\"].isin(val_years)\n        va_idx = train[\"year\"].isin(val_years)\n    else:\n        rs = np.random.RandomState(42)\n        mask = rs.rand(len(train)) < 0.8\n        tr_idx, va_idx = mask, ~mask\n\n    y_tr = np.log1p(np.clip(train.loc[tr_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n    y_va = np.log1p(np.clip(train.loc[va_idx, \"yield\"].to_numpy(dtype=float), 0, None))\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_data_in_leaf=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=42,\n        n_jobs=-1,\n    )\n\n    model.fit(\n        train.loc[tr_idx, features],\n        y_tr,\n        eval_set=[(train.loc[va_idx, features], y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    pred_log = model.predict(test[features])\n    test_pred = np.expm1(pred_log)\n    test_pred = np.clip(test_pred, 0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 116 lines, Excellent rmse (1221.198), NumPy-based implementation\n\n### Inspiration 3 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int).astype(\"category\")\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1800,\n        learning_rate=0.04,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[\"yield\"].astype(float), categorical_feature=[\"state_enc\"])\n\n    test_pred = model.predict(test[features])\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.43271866072765647}, "island_id": 0, "generation": 2, "metadata": {"iteration_time": 38.245731830596924, "changes": "Change 1: Replace 138 lines with 55 lines"}}
{"iteration": 7, "timestamp": 1765795633.7595997, "parent_id": "6d9d581c-1298-49cd-a613-6554f3646ba6", "child_id": "0e189663-0cd0-4951-99c5-cec731121de3", "parent_metrics": {"combined_score": 0.4531017397239551, "mape": 0.10554148154084476, "rmse": 598.0350418288939}, "child_metrics": {"combined_score": 0.3904898971613729, "mape": 0.28199886147509096, "rmse": 1053.9632252327401}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef _add_time_feats(df: pd.DataFrame, year_med: float) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_feats(train, year_med)\n    test = _add_time_feats(test, year_med)\n\n    # numeric-only features; exclude raw string 'state'\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # validation: hold out most recent year if feasible else deterministic random split\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n        deterministic=True,\n        force_col_wise=True,\n    )\n\n    model = lgb.LGBMRegressor(**params)\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n    best_iter = int(getattr(model, \"best_iteration_\", params[\"n_estimators\"])) or params[\"n_estimators\"]\n\n    params_full = dict(params)\n    params_full[\"n_estimators\"] = best_iter\n    model_full = lgb.LGBMRegressor(**params_full)\n    model_full.fit(X, y)\n\n    test_pred = np.expm1(model_full.predict(X_test))\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4531\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4531\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n### Attempt 2\n- Changes: Change 1: Replace 145 lines with 56 lines\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 597.9169\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 145 lines with 67 lines\n- Metrics: combined_score: 0.4550, mape: 0.1009, rmse: 578.1370\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4550)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4550), Performs well on mape (0.1009), Performs well on rmse (578.1370)\n\n### Program 2 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols and train[c].dtype.kind in numeric_kinds]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[\"yield\"].astype(float))\n\n    test_pred = model.predict(test[features])\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (597.9169)\n\n### Program 3 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4327)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4049)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols and train[c].dtype.kind in numeric_kinds]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[\"yield\"].astype(float))\n\n    test_pred = model.predict(test[features])\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 145 lines with 56 lines, Alternative mape approach, Excellent rmse (597.917)\n\n### Inspiration 2 (Score: 0.4550, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 145 lines with 67 lines, Alternative mape approach, Excellent rmse (578.137)\n\n### Inspiration 3 (Score: 0.4049, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 145 lines, Alternative mape approach, Excellent rmse (1411.252)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef _add_time_feats(df: pd.DataFrame, year_med: float) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_feats(train, year_med)\n    test = _add_time_feats(test, year_med)\n\n    # numeric-only features; exclude raw string 'state'\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # validation: hold out most recent year if feasible else deterministic random split\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n        deterministic=True,\n        force_col_wise=True,\n    )\n\n    model = lgb.LGBMRegressor(**params)\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n    best_iter = int(getattr(model, \"best_iteration_\", params[\"n_estimators\"])) or params[\"n_estimators\"]\n\n    params_full = dict(params)\n    params_full[\"n_estimators\"] = best_iter\n    model_full = lgb.LGBMRegressor(**params_full)\n    model_full.fit(X, y)\n\n    test_pred = np.expm1(model_full.predict(X_test))\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.06261184256258223, "mape": 0.1764573799342462, "rmse": 455.9281834038462}, "island_id": 0, "generation": 1, "metadata": {"iteration_time": 42.59871482849121, "changes": "Change 1: Replace 70 lines with 114 lines"}}
{"iteration": 8, "timestamp": 1765795814.0145478, "parent_id": "3d8d3931-4570-4450-9cdc-d750cbf73a28", "child_id": "fe9b4b64-4097-48f6-b41b-a6a4fe128ca5", "parent_metrics": {"combined_score": 0.4550332384259089, "mape": 0.10090963012575396, "rmse": 578.1370339613667}, "child_metrics": {"combined_score": 0.4009676171805823, "mape": 0.24800693617116382, "rmse": 1519.5969956653557}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(int)\n    df[\"year_centered\"] = (y - y.min()).astype(int)\n    return df\n\ndef _smoothed_mean(stats: pd.DataFrame, global_mean: float, smooth: float) -> pd.Series:\n    # stats has columns: [\"mean\",\"count\"]\n    return (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n\ndef add_target_encoding_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    y: pd.Series,\n    cols: List[str],\n    n_splits: int = 5,\n    smooth: float = 20.0,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n\n    key_name = \"_\".join(cols)\n    te_col = f\"te_{key_name}\"\n    global_mean = float(y.mean())\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = pd.Series(index=tr.index, dtype=float)\n\n    # precompute key for speed/determinism\n    tr_key = tr[cols].astype(str).agg(\"|\".join, axis=1)\n    te_key = te[cols].astype(str).agg(\"|\".join, axis=1)\n\n    for tr_idx, va_idx in kf.split(tr):\n        stats = (\n            pd.DataFrame({\"key\": tr_key.iloc[tr_idx].values, \"y\": y.iloc[tr_idx].values})\n            .groupby(\"key\")[\"y\"]\n            .agg([\"mean\", \"count\"])\n        )\n        mapping = _smoothed_mean(stats, global_mean, smooth).to_dict()\n        oof.iloc[va_idx] = tr_key.iloc[va_idx].map(mapping).fillna(global_mean).astype(float)\n\n    # test mapping from full train\n    stats_full = (\n        pd.DataFrame({\"key\": tr_key.values, \"y\": y.values})\n        .groupby(\"key\")[\"y\"]\n        .agg([\"mean\", \"count\"])\n    )\n    mapping_full = _smoothed_mean(stats_full, global_mean, smooth).to_dict()\n    te_vals = te_key.map(mapping_full).fillna(global_mean).astype(float)\n\n    tr[te_col] = oof.values\n    te[te_col] = te_vals.values\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    y = train[\"yield\"].astype(float)\n\n    # Out-of-fold target encoding to reduce leakage/overfit\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\"], n_splits=5, smooth=30.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"month\"], n_splits=5, smooth=50.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"months_since_crop_start\"], n_splits=5, smooth=50.0)\n\n    # numeric-only features (state kept as state_enc + TE)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X_all = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log-target to better match MAPE-like behavior; ensure positivity on output\n    y_log = np.log1p(y.clip(lower=0.0).values)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(X_test), dtype=float)\n\n    for tr_idx, va_idx in kf.split(X_all):\n        X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n        y_tr, y_va = y_log[tr_idx], y_log[va_idx]\n\n        model = lgb.LGBMRegressor(\n            n_estimators=6000,\n            learning_rate=0.03,\n            num_leaves=127,\n            min_child_samples=25,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_alpha=0.0,\n            reg_lambda=1.0,\n            random_state=42,\n            objective=\"regression\",\n            n_jobs=-1,\n        )\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4550\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4550\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4550, mape: 0.1009, rmse: 578.1370\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4550)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4550), Performs well on mape (0.1009), Performs well on rmse (578.1370)\n\n\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(int)\n    df[\"year_centered\"] = (y - y.min()).astype(int)\n    return df\n\ndef _smoothed_mean(stats: pd.DataFrame, global_mean: float, smooth: float) -> pd.Series:\n    # stats has columns: [\"mean\",\"count\"]\n    return (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n\ndef add_target_encoding_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    y: pd.Series,\n    cols: List[str],\n    n_splits: int = 5,\n    smooth: float = 20.0,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n\n    key_name = \"_\".join(cols)\n    te_col = f\"te_{key_name}\"\n    global_mean = float(y.mean())\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = pd.Series(index=tr.index, dtype=float)\n\n    # precompute key for speed/determinism\n    tr_key = tr[cols].astype(str).agg(\"|\".join, axis=1)\n    te_key = te[cols].astype(str).agg(\"|\".join, axis=1)\n\n    for tr_idx, va_idx in kf.split(tr):\n        stats = (\n            pd.DataFrame({\"key\": tr_key.iloc[tr_idx].values, \"y\": y.iloc[tr_idx].values})\n            .groupby(\"key\")[\"y\"]\n            .agg([\"mean\", \"count\"])\n        )\n        mapping = _smoothed_mean(stats, global_mean, smooth).to_dict()\n        oof.iloc[va_idx] = tr_key.iloc[va_idx].map(mapping).fillna(global_mean).astype(float)\n\n    # test mapping from full train\n    stats_full = (\n        pd.DataFrame({\"key\": tr_key.values, \"y\": y.values})\n        .groupby(\"key\")[\"y\"]\n        .agg([\"mean\", \"count\"])\n    )\n    mapping_full = _smoothed_mean(stats_full, global_mean, smooth).to_dict()\n    te_vals = te_key.map(mapping_full).fillna(global_mean).astype(float)\n\n    tr[te_col] = oof.values\n    te[te_col] = te_vals.values\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    y = train[\"yield\"].astype(float)\n\n    # Out-of-fold target encoding to reduce leakage/overfit\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\"], n_splits=5, smooth=30.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"month\"], n_splits=5, smooth=50.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"months_since_crop_start\"], n_splits=5, smooth=50.0)\n\n    # numeric-only features (state kept as state_enc + TE)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X_all = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log-target to better match MAPE-like behavior; ensure positivity on output\n    y_log = np.log1p(y.clip(lower=0.0).values)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(X_test), dtype=float)\n\n    for tr_idx, va_idx in kf.split(X_all):\n        X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n        y_tr, y_va = y_log[tr_idx], y_log[va_idx]\n\n        model = lgb.LGBMRegressor(\n            n_estimators=6000,\n            learning_rate=0.03,\n            num_leaves=127,\n            min_child_samples=25,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_alpha=0.0,\n            reg_lambda=1.0,\n            random_state=42,\n            objective=\"regression\",\n            n_jobs=-1,\n        )\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.0540656212453266, "mape": 0.14709730604540985, "rmse": 941.459961703989}, "island_id": 1, "generation": 3, "metadata": {"iteration_time": 180.24435997009277, "changes": "Change 1: Replace 67 lines with 152 lines"}}
{"iteration": 9, "timestamp": 1765795851.3753686, "parent_id": "f2fc3a66-dbe9-4c06-88f0-9911cf545f45", "child_id": "6aa20788-6209-42a2-9ed6-1cad6f19f39b", "parent_metrics": {"combined_score": 0.4550332384259089, "mape": 0.10090963012575396, "rmse": 578.1370339613667}, "child_metrics": {"combined_score": 0.0, "error": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: state_cat: object"}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef _add_time_feats(df: pd.DataFrame, year_med: float) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_feats(train, year_med=year_med)\n    test = _add_time_feats(test, year_med=year_med)\n\n    # treat state as categorical (better than ordinal integers)\n    train[\"state_enc\"] = train[\"state_enc\"].astype(\"category\")\n    test[\"state_enc\"] = test[\"state_enc\"].astype(\"category\")\n\n    # use all non-target features (keep categorical + numeric)\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # median impute numeric columns only\n    num_cols = [c for c in features if X[c].dtype.kind in (\"b\", \"i\", \"u\", \"f\", \"c\")]\n    if num_cols:\n        med = X[num_cols].median(numeric_only=True)\n        X[num_cols] = X[num_cols].fillna(med)\n        X_test[num_cols] = X_test[num_cols].fillna(med)\n\n    # log1p target tends to improve relative-error metrics (MAPE)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # deterministic validation: latest year if sufficient, else fixed random split\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).to_numpy()\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = rng.rand(len(train)) < 0.2\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y[~val_mask], y[val_mask]\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n        deterministic=True,\n        force_col_wise=True,\n    )\n\n    model = lgb.LGBMRegressor(**params)\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"l1\",\n        categorical_feature=[\"state_enc\"],\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n    best_iter = int(getattr(model, \"best_iteration_\", params[\"n_estimators\"])) or params[\"n_estimators\"]\n\n    # refit on full data with best iteration\n    params_full = dict(params)\n    params_full[\"n_estimators\"] = best_iter\n    model_full = lgb.LGBMRegressor(**params_full)\n    model_full.fit(X, y, categorical_feature=[\"state_enc\"])\n\n    test_pred = np.expm1(model_full.predict(X_test))\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4550\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.4531 \u2192 0.4550\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 598.0350\n- Outcome: Improvement in all metrics\n\n### Attempt 2\n- Changes: Change 1: Replace 145 lines with 56 lines\n- Metrics: combined_score: 0.4531, mape: 0.1055, rmse: 597.9169\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 145 lines with 67 lines\n- Metrics: combined_score: 0.4550, mape: 0.1009, rmse: 578.1370\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4550)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4550), Performs well on mape (0.1009), Performs well on rmse (578.1370)\n\n### Program 2 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols and train[c].dtype.kind in numeric_kinds]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[\"yield\"].astype(float))\n\n    test_pred = model.predict(test[features])\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (597.9169)\n\n### Program 3 (Score: 0.4531)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4531), Performs well on mape (0.1055), Performs well on rmse (598.0350)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4049)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].astype(str).unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].astype(str).map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].astype(int).apply(transform)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - df[\"year\"].astype(float).median()\n    return df\n\n\ndef _target_encode_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    cols: List[str],\n    target: str,\n    n_splits: int = 5,\n    seed: int = 42,\n    smooth: float = 20.0,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Out-of-fold smoothed target encoding for train; full-train encoding for test (no label leakage).\"\"\"\n    global_mean = float(train[target].mean())\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n\n    oof = pd.Series(index=train.index, dtype=float)\n    for tr_idx, val_idx in kf.split(train):\n        tr = train.iloc[tr_idx]\n        stats = tr.groupby(cols)[target].agg([\"mean\", \"count\"])\n        enc = (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n        enc = enc.rename(\"te\").reset_index()\n        oof.iloc[val_idx] = (\n            train.iloc[val_idx][cols].merge(enc, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n        )\n\n    stats_full = train.groupby(cols)[target].agg([\"mean\", \"count\"])\n    enc_full = (stats_full[\"mean\"] * stats_full[\"count\"] + global_mean * smooth) / (stats_full[\"count\"] + smooth)\n    enc_full = enc_full.rename(\"te\").reset_index()\n    te_test = test[cols].merge(enc_full, on=cols, how=\"left\")[\"te\"].fillna(global_mean).values\n    return oof.values, te_test\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # basic transforms\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = _add_time_features(train)\n    test = _add_time_features(test)\n\n    # OOF target encoding to improve generalization\n    te_state_tr, te_state_te = _target_encode_oof(train, test, [\"state\"], \"yield\", n_splits=5, seed=42, smooth=30.0)\n    train[\"te_state\"] = te_state_tr\n    test[\"te_state\"] = te_state_te\n\n    te_sm_tr, te_sm_te = _target_encode_oof(\n        train, test, [\"state\", \"month\"], \"yield\", n_splits=5, seed=42, smooth=50.0\n    )\n    train[\"te_state_month\"] = te_sm_tr\n    test[\"te_state_month\"] = te_sm_te\n\n    # feature selection (numeric only; exclude raw string)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate_features = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate_features if train[c].dtype.kind in numeric_kinds]\n\n    # log1p target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # CV bagging + early stopping for stability\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(test), dtype=float)\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n\n    X = train[features]\n    X_test = test[features]\n\n    for tr_idx, va_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"l1\",\n            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test, num_iteration=model.best_iteration_) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n### Program D2 (Score: 0.4327)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n\n    def transform(x: int) -> int:\n        return x - 10 if x >= 10 else x + 2\n\n    df[\"months_since_crop_start\"] = m.apply(transform).astype(int)\n    return df\n\n\ndef _add_time_features(df: pd.DataFrame, year_med: float | None = None) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    if year_med is None:\n        year_med = float(np.nanmedian(y))\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_features(train, year_med=year_med)\n    test = _add_time_features(test, year_med=year_med)\n\n    # numeric-only features, drop raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    candidate = [c for c in train.columns if c not in drop_cols]\n    features = [c for c in candidate if train[c].dtype.kind in numeric_kinds]\n\n    # simple median imputation for stability across splits\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    y = train[\"yield\"].astype(float)\n\n    # validation: hold out most recent year if large enough, else random\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params_a = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_a = lgb.LGBMRegressor(**params_a)\n    model_a.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    # second model on log target for better relative error; blend back to original scale\n    y_log = np.log1p(y.clip(lower=0.0))\n    y_tr_l, y_va_l = y_log.loc[~val_mask], y_log.loc[val_mask]\n\n    params_b = dict(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.0,\n        reg_lambda=1.5,\n        random_state=42,\n        objective=\"regression\",\n    )\n    model_b = lgb.LGBMRegressor(**params_b)\n    model_b.fit(\n        X_tr,\n        y_tr_l,\n        eval_set=[(X_va, y_va_l)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=250, verbose=False)],\n    )\n\n    pred_a = model_a.predict(X_test, num_iteration=getattr(model_a, \"best_iteration_\", None))\n    pred_b = np.expm1(model_b.predict(X_test, num_iteration=getattr(model_b, \"best_iteration_\", None)))\n    test_pred = 0.75 * pred_a + 0.25 * pred_b\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to mape\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int).astype(\"category\")\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1800,\n        learning_rate=0.04,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[\"yield\"].astype(float), categorical_feature=[\"state_enc\"])\n\n    test_pred = model.predict(test[features])\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 138 lines with 55 lines, Alternative combined_score approach, Comprehensive implementation\n\n### Inspiration 2 (Score: 0.4531, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (598.035), Comprehensive implementation\n\n### Inspiration 3 (Score: 0.3905, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef _add_time_feats(df: pd.DataFrame, year_med: float) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(float)\n    df[\"year_centered\"] = y - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_feats(train, year_med)\n    test = _add_time_feats(test, year_med)\n\n    # numeric-only features; exclude raw string 'state'\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log target helps relative-error metrics (MAPE-like)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # validation: hold out most recent year if feasible else deterministic random split\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).values\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = (rng.rand(len(train)) < 0.2)\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y.loc[~val_mask], y.loc[val_mask]\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n        deterministic=True,\n        force_col_wise=True,\n    )\n\n    model = lgb.LGBMRegressor(**params)\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"l1\",\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n    best_iter = int(getattr(model, \"best_iteration_\", params[\"n_estimators\"])) or params[\"n_estimators\"]\n\n    params_full = dict(params)\n    params_full[\"n_estimators\"] = best_iter\n    model_full = lgb.LGBMRegressor(**params_full)\n    model_full.fit(X, y)\n\n    test_pred = np.expm1(model_full.predict(X_test))\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 70 lines with 114 lines, Alternative mape approach, Excellent rmse (1053.963)\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n=======\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef _add_time_feats(df: pd.DataFrame, year_med: float) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int).clip(1, 12)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    df[\"year_centered\"] = df[\"year\"].astype(float) - year_med\n    df[\"state_month\"] = df[\"state_enc\"].astype(float) * m.astype(float)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    year_med = float(np.nanmedian(train[\"year\"].astype(float)))\n    train = _add_time_feats(train, year_med=year_med)\n    test = _add_time_feats(test, year_med=year_med)\n\n    # treat state as categorical (better than ordinal integers)\n    train[\"state_enc\"] = train[\"state_enc\"].astype(\"category\")\n    test[\"state_enc\"] = test[\"state_enc\"].astype(\"category\")\n\n    # use all non-target features (keep categorical + numeric)\n    drop_cols = {\"yield\", \"state\"}\n    features = [c for c in train.columns if c not in drop_cols]\n\n    X = train[features].copy()\n    X_test = test[features].copy()\n\n    # median impute numeric columns only\n    num_cols = [c for c in features if X[c].dtype.kind in (\"b\", \"i\", \"u\", \"f\", \"c\")]\n    if num_cols:\n        med = X[num_cols].median(numeric_only=True)\n        X[num_cols] = X[num_cols].fillna(med)\n        X_test[num_cols] = X_test[num_cols].fillna(med)\n\n    # log1p target tends to improve relative-error metrics (MAPE)\n    y = np.log1p(train[\"yield\"].astype(float).clip(lower=0.0))\n\n    # deterministic validation: latest year if sufficient, else fixed random split\n    max_year = train[\"year\"].max()\n    val_mask = (train[\"year\"] == max_year).to_numpy()\n    if int(val_mask.sum()) < max(30, int(0.1 * len(train))):\n        rng = np.random.RandomState(42)\n        val_mask = rng.rand(len(train)) < 0.2\n\n    X_tr, X_va = X.loc[~val_mask], X.loc[val_mask]\n    y_tr, y_va = y[~val_mask], y[val_mask]\n\n    params = dict(\n        n_estimators=8000,\n        learning_rate=0.03,\n        num_leaves=127,\n        min_child_samples=20,\n        subsample=0.85,\n        subsample_freq=1,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n        deterministic=True,\n        force_col_wise=True,\n    )\n\n    model = lgb.LGBMRegressor(**params)\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"l1\",\n        categorical_feature=[\"state_enc\"],\n        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n    )\n    best_iter = int(getattr(model, \"best_iteration_\", params[\"n_estimators\"])) or params[\"n_estimators\"]\n\n    # refit on full data with best iteration\n    params_full = dict(params)\n    params_full[\"n_estimators\"] = best_iter\n    model_full = lgb.LGBMRegressor(**params_full)\n    model_full.fit(X, y, categorical_feature=[\"state_enc\"])\n\n    test_pred = np.expm1(model_full.predict(X_test))\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": -0.4550332384259089}, "island_id": 0, "generation": 3, "metadata": {"iteration_time": 37.369749784469604, "changes": "Change 1: Replace 67 lines with 124 lines"}}
{"iteration": 10, "timestamp": 1765795919.2531073, "parent_id": "fe9b4b64-4097-48f6-b41b-a6a4fe128ca5", "child_id": "7ed6449c-4633-4b1d-b4b8-0eea9477177a", "parent_metrics": {"combined_score": 0.4009676171805823, "mape": 0.24800693617116382, "rmse": 1519.5969956653557}, "child_metrics": {"combined_score": 0.45493578162362386, "mape": 0.10142436620197497, "rmse": 510.1706123796568}, "parent_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(int)\n    df[\"year_centered\"] = (y - y.min()).astype(int)\n    return df\n\ndef _smoothed_mean(stats: pd.DataFrame, global_mean: float, smooth: float) -> pd.Series:\n    # stats has columns: [\"mean\",\"count\"]\n    return (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n\ndef add_target_encoding_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    y: pd.Series,\n    cols: List[str],\n    n_splits: int = 5,\n    smooth: float = 20.0,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n\n    key_name = \"_\".join(cols)\n    te_col = f\"te_{key_name}\"\n    global_mean = float(y.mean())\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = pd.Series(index=tr.index, dtype=float)\n\n    # precompute key for speed/determinism\n    tr_key = tr[cols].astype(str).agg(\"|\".join, axis=1)\n    te_key = te[cols].astype(str).agg(\"|\".join, axis=1)\n\n    for tr_idx, va_idx in kf.split(tr):\n        stats = (\n            pd.DataFrame({\"key\": tr_key.iloc[tr_idx].values, \"y\": y.iloc[tr_idx].values})\n            .groupby(\"key\")[\"y\"]\n            .agg([\"mean\", \"count\"])\n        )\n        mapping = _smoothed_mean(stats, global_mean, smooth).to_dict()\n        oof.iloc[va_idx] = tr_key.iloc[va_idx].map(mapping).fillna(global_mean).astype(float)\n\n    # test mapping from full train\n    stats_full = (\n        pd.DataFrame({\"key\": tr_key.values, \"y\": y.values})\n        .groupby(\"key\")[\"y\"]\n        .agg([\"mean\", \"count\"])\n    )\n    mapping_full = _smoothed_mean(stats_full, global_mean, smooth).to_dict()\n    te_vals = te_key.map(mapping_full).fillna(global_mean).astype(float)\n\n    tr[te_col] = oof.values\n    te[te_col] = te_vals.values\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    y = train[\"yield\"].astype(float)\n\n    # Out-of-fold target encoding to reduce leakage/overfit\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\"], n_splits=5, smooth=30.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"month\"], n_splits=5, smooth=50.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"months_since_crop_start\"], n_splits=5, smooth=50.0)\n\n    # numeric-only features (state kept as state_enc + TE)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X_all = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log-target to better match MAPE-like behavior; ensure positivity on output\n    y_log = np.log1p(y.clip(lower=0.0).values)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(X_test), dtype=float)\n\n    for tr_idx, va_idx in kf.split(X_all):\n        X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n        y_tr, y_va = y_log[tr_idx], y_log[va_idx]\n\n        model = lgb.LGBMRegressor(\n            n_estimators=6000,\n            learning_rate=0.03,\n            num_leaves=127,\n            min_child_samples=25,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_alpha=0.0,\n            reg_lambda=1.0,\n            random_state=42,\n            objective=\"regression\",\n            n_jobs=-1,\n        )\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "child_code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        keys = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(keys)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(int)\n    df[\"year_centered\"] = (y - y.min()).astype(int)\n    return df\n\ndef add_loo_target_enc(\n    train: pd.DataFrame, test: pd.DataFrame, y: pd.Series, cols: List[str], smooth: float\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    key = tr[cols].astype(str).agg(\"|\".join, axis=1)\n    key_te = te[cols].astype(str).agg(\"|\".join, axis=1)\n    global_mean = float(y.mean())\n\n    grp = pd.DataFrame({\"k\": key.values, \"y\": y.values}).groupby(\"k\")[\"y\"].agg([\"sum\", \"count\"])\n    ssum, cnt = grp[\"sum\"].to_dict(), grp[\"count\"].to_dict()\n\n    def tr_enc(k, yi):\n        c = cnt.get(k, 0)\n        if c <= 1:\n            return global_mean\n        return (ssum[k] - yi + global_mean * smooth) / (c - 1 + smooth)\n\n    enc_tr = [tr_enc(k, yi) for k, yi in zip(key.values, y.values)]\n    enc_te = [(ssum[k] + global_mean * smooth) / (cnt[k] + smooth) if k in cnt else global_mean for k in key_te.values]\n\n    name = \"te_\" + \"_\".join(cols)\n    tr[name] = np.asarray(enc_tr, float)\n    te[name] = np.asarray(enc_te, float)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = add_time_features(months_since_crop_start(train))\n    test = add_time_features(months_since_crop_start(test))\n\n    y = train[\"yield\"].astype(float)\n\n    # leakage-safe (LOO) target encodings\n    train, test = add_loo_target_enc(train, test, y, cols=[\"state\"], smooth=30.0)\n    train, test = add_loo_target_enc(train, test, y, cols=[\"state\", \"month\"], smooth=50.0)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.15, random_state=42)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.2,\n        reg_lambda=0.8,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        categorical_feature=[\"state_enc\"],\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "prompt": {"system": "\"\u4f60\u662f\u4e00\u540d\u9876\u7ea7\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8981\u6539\u5199 EVOLVE-BLOCK \u5185\u7684\u4ee3\u7801\u6765\u63d0\u5347\u5df4\u897f\u5927\u8c46\u4ea7\u91cf\u9884\u6d4b\u6548\u679c\u3002\\n\"\n\"\\n\"\n\"\u76ee\u6807\uff1a\\n\"\n\"- \u4e3b\u6307\u6807\uff1a\u964d\u4f4e MAPE\uff1b\u6b21\u6307\u6807\uff1a\u964d\u4f4e RMSE\u3002\u7efc\u5408\u5f97\u5206\u6309\u8bc4\u6d4b\u811a\u672c\u8ba1\u7b97\u3002\\n\"\n\"- \u8bad\u7ec3\u6570\u636e\uff1atrain.csv\uff1b\u9884\u6d4b\u6570\u636e\uff1atest.csv\uff1b\u9700\u751f\u6210 submission.csv\uff0c\u5217\u540d\u4e0e test_answer.csv \u5b8c\u5168\u4e00\u81f4\u3002\\n\"\n\"\\n\"\n\"\u786c\u7ea6\u675f\uff08\u4e0d\u8981\u8fdd\u53cd\uff09\uff1a\\n\"\n\"- \u53ea\u4fee\u6539 EVOLVE-BLOCK \u5185\u4ee3\u7801\uff0c\u4e0d\u6539\u51fd\u6570/\u6587\u4ef6\u8def\u5f84/\u5165\u53c2/\u5199\u76d8\u4f4d\u7f6e\u3002\\n\"\n\"- \u4fdd\u6301\u7a0b\u5e8f\u53ef\u72ec\u7acb\u8fd0\u884c\uff1a\u8bfb\u53d6 train.csv/test.csv\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u8f93\u51fa submission.csv\u3002\\n\"\n\"- \u4e0d\u6cc4\u6f0f\u6d4b\u8bd5\u6807\u7b7e\uff0c\u4e0d\u4e71\u6539\u968f\u673a\u79cd\u5b50/\u8def\u5f84\uff0c\u4fdd\u6301 determinism\uff08seed=42\uff09\u3002\\n\"\n\"\\n\"\n\"\u53ef\u4f18\u5316\u65b9\u5411\uff1a\\n\"\n\"- \u66f4\u597d\u7684\u6a21\u578b\uff1aLightGBM / CatBoost / XGBoost / \u968f\u673a\u68ee\u6797 / \u7ebf\u6027\u6216\u96c6\u6210\uff1b\u5408\u7406\u8d85\u53c2\uff08\u5b66\u4e60\u7387\u3001\u6df1\u5ea6\u3001L1/L2\u3001\u65e9\u505c\uff09\u3002\\n\"\n\"- \u7279\u5f81\u5de5\u7a0b\uff1a\u6570\u503c\u6807\u51c6\u5316/\u5206\u7bb1\u3001\u4ea4\u53c9\u7279\u5f81\u3001\u6bd4\u503c/\u5dee\u503c\u3001\u5bf9\u6570/\u5e42\u53d8\u6362\u3001\u65f6\u95f4/\u7a7a\u95f4\u7279\u5f81\u3001\u7f3a\u5931\u503c\u7a33\u5065\u586b\u5145\u3002\\n\"\n\"- \u8bad\u7ec3\u7b56\u7565\uff1a\u4ea4\u53c9\u9a8c\u8bc1\u6216\u9a8c\u8bc1\u96c6\u9632\u8fc7\u62df\u5408\uff1b\u5904\u7406\u5f02\u5e38\u503c\uff1b\u7279\u5f81\u9009\u62e9\u6216\u6b63\u5219\u5316\u63a7\u5236\u590d\u6742\u5ea6\u3002\\n\"\n\"- \u8f93\u51fa\u7a33\u5b9a\uff1a\u907f\u514d\u8fc7\u5ea6\u6253\u5370\uff1b\u786e\u4fdd submission.csv \u6392\u5e8f\u4e0e\u683c\u5f0f\u6b63\u786e\uff08\u4e0e test_answer.csv \u540c\u5217\u987a\u5e8f\uff09\u3002\\n\"\n\"\\n\"\n\"\u56de\u590d\u683c\u5f0f\uff1a\u53ea\u8fd4\u56de EVOLVE-BLOCK \u7684 SEARCH/REPLACE diff\uff1b\u82e5\u65e0\u6539\u8fdb\uff0c\u8fd4\u56de\u7a7a diff\u3002\\n\"", "user": "# Current Program Information\n- Fitness: 0.4010\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at 0.4010\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Change 1: Replace 67 lines with 152 lines\n- Metrics: combined_score: 0.4010, mape: 0.2480, rmse: 1519.5970\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.4550, mape: 0.1009, rmse: 578.1370\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4550)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4550), Performs well on mape (0.1009), Performs well on rmse (578.1370)\n\n### Program 2 (Score: 0.4010)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(int)\n    df[\"year_centered\"] = (y - y.min()).astype(int)\n    return df\n\ndef _smoothed_mean(stats: pd.DataFrame, global_mean: float, smooth: float) -> pd.Series:\n    # stats has columns: [\"mean\",\"count\"]\n    return (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n\ndef add_target_encoding_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    y: pd.Series,\n    cols: List[str],\n    n_splits: int = 5,\n    smooth: float = 20.0,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n\n    key_name = \"_\".join(cols)\n    te_col = f\"te_{key_name}\"\n    global_mean = float(y.mean())\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = pd.Series(index=tr.index, dtype=float)\n\n    # precompute key for speed/determinism\n    tr_key = tr[cols].astype(str).agg(\"|\".join, axis=1)\n    te_key = te[cols].astype(str).agg(\"|\".join, axis=1)\n\n    for tr_idx, va_idx in kf.split(tr):\n        stats = (\n            pd.DataFrame({\"key\": tr_key.iloc[tr_idx].values, \"y\": y.iloc[tr_idx].values})\n            .groupby(\"key\")[\"y\"]\n            .agg([\"mean\", \"count\"])\n        )\n        mapping = _smoothed_mean(stats, global_mean, smooth).to_dict()\n        oof.iloc[va_idx] = tr_key.iloc[va_idx].map(mapping).fillna(global_mean).astype(float)\n\n    # test mapping from full train\n    stats_full = (\n        pd.DataFrame({\"key\": tr_key.values, \"y\": y.values})\n        .groupby(\"key\")[\"y\"]\n        .agg([\"mean\", \"count\"])\n    )\n    mapping_full = _smoothed_mean(stats_full, global_mean, smooth).to_dict()\n    te_vals = te_key.map(mapping_full).fillna(global_mean).astype(float)\n\n    tr[te_col] = oof.values\n    te[te_col] = te_vals.values\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    y = train[\"yield\"].astype(float)\n\n    # Out-of-fold target encoding to reduce leakage/overfit\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\"], n_splits=5, smooth=30.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"month\"], n_splits=5, smooth=50.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"months_since_crop_start\"], n_splits=5, smooth=50.0)\n\n    # numeric-only features (state kept as state_enc + TE)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X_all = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log-target to better match MAPE-like behavior; ensure positivity on output\n    y_log = np.log1p(y.clip(lower=0.0).values)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(X_test), dtype=float)\n\n    for tr_idx, va_idx in kf.split(X_all):\n        X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n        y_tr, y_va = y_log[tr_idx], y_log[va_idx]\n\n        model = lgb.LGBMRegressor(\n            n_estimators=6000,\n            learning_rate=0.03,\n            num_leaves=127,\n            min_child_samples=25,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_alpha=0.0,\n            reg_lambda=1.0,\n            random_state=42,\n            objective=\"regression\",\n            n_jobs=-1,\n        )\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.4010), Performs well on mape (0.2480), Performs well on rmse (1519.5970)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4550, Type: Experimental)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # numeric-only features; exclude raw string\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n    y = train[\"yield\"].astype(float)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.05,\n        num_leaves=63,\n        min_child_samples=20,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(X, y)\n\n    test_pred = model.predict(X_test)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Alternative mape approach, Excellent rmse (578.137), Comprehensive implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from judge.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT\n    OUTPUT_DIR = PROJECT_ROOT.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(int)\n    df[\"year_centered\"] = (y - y.min()).astype(int)\n    return df\n\ndef _smoothed_mean(stats: pd.DataFrame, global_mean: float, smooth: float) -> pd.Series:\n    # stats has columns: [\"mean\",\"count\"]\n    return (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n\ndef add_target_encoding_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    y: pd.Series,\n    cols: List[str],\n    n_splits: int = 5,\n    smooth: float = 20.0,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n\n    key_name = \"_\".join(cols)\n    te_col = f\"te_{key_name}\"\n    global_mean = float(y.mean())\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = pd.Series(index=tr.index, dtype=float)\n\n    # precompute key for speed/determinism\n    tr_key = tr[cols].astype(str).agg(\"|\".join, axis=1)\n    te_key = te[cols].astype(str).agg(\"|\".join, axis=1)\n\n    for tr_idx, va_idx in kf.split(tr):\n        stats = (\n            pd.DataFrame({\"key\": tr_key.iloc[tr_idx].values, \"y\": y.iloc[tr_idx].values})\n            .groupby(\"key\")[\"y\"]\n            .agg([\"mean\", \"count\"])\n        )\n        mapping = _smoothed_mean(stats, global_mean, smooth).to_dict()\n        oof.iloc[va_idx] = tr_key.iloc[va_idx].map(mapping).fillna(global_mean).astype(float)\n\n    # test mapping from full train\n    stats_full = (\n        pd.DataFrame({\"key\": tr_key.values, \"y\": y.values})\n        .groupby(\"key\")[\"y\"]\n        .agg([\"mean\", \"count\"])\n    )\n    mapping_full = _smoothed_mean(stats_full, global_mean, smooth).to_dict()\n    te_vals = te_key.map(mapping_full).fillna(global_mean).astype(float)\n\n    tr[te_col] = oof.values\n    te[te_col] = te_vals.values\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    y = train[\"yield\"].astype(float)\n\n    # Out-of-fold target encoding to reduce leakage/overfit\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\"], n_splits=5, smooth=30.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"month\"], n_splits=5, smooth=50.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"months_since_crop_start\"], n_splits=5, smooth=50.0)\n\n    # numeric-only features (state kept as state_enc + TE)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X_all = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log-target to better match MAPE-like behavior; ensure positivity on output\n    y_log = np.log1p(y.clip(lower=0.0).values)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(X_test), dtype=float)\n\n    for tr_idx, va_idx in kf.split(X_all):\n        X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n        y_tr, y_va = y_log[tr_idx], y_log[va_idx]\n\n        model = lgb.LGBMRegressor(\n            n_estimators=6000,\n            learning_rate=0.03,\n            num_leaves=127,\n            min_child_samples=25,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_alpha=0.0,\n            reg_lambda=1.0,\n            random_state=42,\n            objective=\"regression\",\n            n_jobs=-1,\n        )\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean03\\inputs\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements."}, "llm_response": "<<<<<<< SEARCH\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        states = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(states)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(int)\n    df[\"year_centered\"] = (y - y.min()).astype(int)\n    return df\n\ndef _smoothed_mean(stats: pd.DataFrame, global_mean: float, smooth: float) -> pd.Series:\n    # stats has columns: [\"mean\",\"count\"]\n    return (stats[\"mean\"] * stats[\"count\"] + global_mean * smooth) / (stats[\"count\"] + smooth)\n\ndef add_target_encoding_oof(\n    train: pd.DataFrame,\n    test: pd.DataFrame,\n    y: pd.Series,\n    cols: List[str],\n    n_splits: int = 5,\n    smooth: float = 20.0,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr = train.copy()\n    te = test.copy()\n\n    key_name = \"_\".join(cols)\n    te_col = f\"te_{key_name}\"\n    global_mean = float(y.mean())\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = pd.Series(index=tr.index, dtype=float)\n\n    # precompute key for speed/determinism\n    tr_key = tr[cols].astype(str).agg(\"|\".join, axis=1)\n    te_key = te[cols].astype(str).agg(\"|\".join, axis=1)\n\n    for tr_idx, va_idx in kf.split(tr):\n        stats = (\n            pd.DataFrame({\"key\": tr_key.iloc[tr_idx].values, \"y\": y.iloc[tr_idx].values})\n            .groupby(\"key\")[\"y\"]\n            .agg([\"mean\", \"count\"])\n        )\n        mapping = _smoothed_mean(stats, global_mean, smooth).to_dict()\n        oof.iloc[va_idx] = tr_key.iloc[va_idx].map(mapping).fillna(global_mean).astype(float)\n\n    # test mapping from full train\n    stats_full = (\n        pd.DataFrame({\"key\": tr_key.values, \"y\": y.values})\n        .groupby(\"key\")[\"y\"]\n        .agg([\"mean\", \"count\"])\n    )\n    mapping_full = _smoothed_mean(stats_full, global_mean, smooth).to_dict()\n    te_vals = te_key.map(mapping_full).fillna(global_mean).astype(float)\n\n    tr[te_col] = oof.values\n    te[te_col] = te_vals.values\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n    train = add_time_features(train)\n    test = add_time_features(test)\n\n    y = train[\"yield\"].astype(float)\n\n    # Out-of-fold target encoding to reduce leakage/overfit\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\"], n_splits=5, smooth=30.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"month\"], n_splits=5, smooth=50.0)\n    train, test = add_target_encoding_oof(train, test, y, cols=[\"state\", \"months_since_crop_start\"], n_splits=5, smooth=50.0)\n\n    # numeric-only features (state kept as state_enc + TE)\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    # robust missing fill\n    med = train[features].median(numeric_only=True)\n    X_all = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    # log-target to better match MAPE-like behavior; ensure positivity on output\n    y_log = np.log1p(y.clip(lower=0.0).values)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    test_pred_log = np.zeros(len(X_test), dtype=float)\n\n    for tr_idx, va_idx in kf.split(X_all):\n        X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n        y_tr, y_va = y_log[tr_idx], y_log[va_idx]\n\n        model = lgb.LGBMRegressor(\n            n_estimators=6000,\n            learning_rate=0.03,\n            num_leaves=127,\n            min_child_samples=25,\n            subsample=0.8,\n            subsample_freq=1,\n            colsample_bytree=0.8,\n            reg_alpha=0.0,\n            reg_lambda=1.0,\n            random_state=42,\n            objective=\"regression\",\n            n_jobs=-1,\n        )\n        model.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_va, y_va)],\n            eval_metric=\"rmse\",\n            callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n        )\n        test_pred_log += model.predict(X_test) / kf.get_n_splits()\n\n    test_pred = np.expm1(test_pred_log)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n=======\nfrom typing import Dict, Tuple, List\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    df = df.copy()\n    s = df[\"state\"].astype(str)\n    if mapping is None:\n        keys = sorted(s.unique())\n        mapping = {k: i for i, k in enumerate(keys)}\n    df[\"state_enc\"] = s.map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    df[\"months_since_crop_start\"] = (m - 10).where(m >= 10, m + 2).astype(int)\n    return df\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    m = df[\"month\"].astype(int)\n    ang = 2.0 * np.pi * (m - 1) / 12.0\n    df[\"month_sin\"] = np.sin(ang)\n    df[\"month_cos\"] = np.cos(ang)\n    y = df[\"year\"].astype(int)\n    df[\"year_centered\"] = (y - y.min()).astype(int)\n    return df\n\ndef add_loo_target_enc(\n    train: pd.DataFrame, test: pd.DataFrame, y: pd.Series, cols: List[str], smooth: float\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    tr, te = train.copy(), test.copy()\n    key = tr[cols].astype(str).agg(\"|\".join, axis=1)\n    key_te = te[cols].astype(str).agg(\"|\".join, axis=1)\n    global_mean = float(y.mean())\n\n    grp = pd.DataFrame({\"k\": key.values, \"y\": y.values}).groupby(\"k\")[\"y\"].agg([\"sum\", \"count\"])\n    ssum, cnt = grp[\"sum\"].to_dict(), grp[\"count\"].to_dict()\n\n    def tr_enc(k, yi):\n        c = cnt.get(k, 0)\n        if c <= 1:\n            return global_mean\n        return (ssum[k] - yi + global_mean * smooth) / (c - 1 + smooth)\n\n    enc_tr = [tr_enc(k, yi) for k, yi in zip(key.values, y.values)]\n    enc_te = [(ssum[k] + global_mean * smooth) / (cnt[k] + smooth) if k in cnt else global_mean for k in key_te.values]\n\n    name = \"te_\" + \"_\".join(cols)\n    tr[name] = np.asarray(enc_tr, float)\n    te[name] = np.asarray(enc_te, float)\n    return tr, te\n\ndef train_and_predict(root) -> Path:\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n\n    train = add_time_features(months_since_crop_start(train))\n    test = add_time_features(months_since_crop_start(test))\n\n    y = train[\"yield\"].astype(float)\n\n    # leakage-safe (LOO) target encodings\n    train, test = add_loo_target_enc(train, test, y, cols=[\"state\"], smooth=30.0)\n    train, test = add_loo_target_enc(train, test, y, cols=[\"state\", \"month\"], smooth=50.0)\n\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    features = [c for c in train.columns if c != \"yield\" and train[c].dtype.kind in numeric_kinds]\n\n    med = train[features].median(numeric_only=True)\n    X = train[features].fillna(med)\n    X_test = test[features].fillna(med)\n\n    X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.15, random_state=42)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=6000,\n        learning_rate=0.03,\n        num_leaves=63,\n        min_child_samples=25,\n        subsample=0.8,\n        subsample_freq=1,\n        colsample_bytree=0.8,\n        reg_alpha=0.2,\n        reg_lambda=0.8,\n        random_state=42,\n        objective=\"regression\",\n        n_jobs=-1,\n    )\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_va, y_va)],\n        eval_metric=\"rmse\",\n        categorical_feature=[\"state_enc\"],\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n    )\n\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    test_pred = np.clip(test_pred, 0.0, None)\n\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    return out_path\n>>>>>>> REPLACE", "improvement_delta": {"combined_score": 0.05396816444304153, "mape": -0.14658256996918884, "rmse": -1009.4263832856989}, "island_id": 1, "generation": 4, "metadata": {"iteration_time": 67.87476301193237, "changes": "Change 1: Replace 152 lines with 116 lines"}}

{"id": "e23f8b25-3e23-42fe-b28f-8b8ada82db7f", "code": "\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from evaluator.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT / \"data\"\n    OUTPUT_DIR = PROJECT_ROOT.parent.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- target-based aggregate features ----------\n    target = \"yield\"\n    global_mean = train[target].mean()\n\n    # per-state and per-month historical mean yields\n    state_mean = train.groupby(\"state_enc\")[target].mean()\n    month_mean = train.groupby(\"month\")[target].mean()\n\n    train[\"state_mean_yield\"] = train[\"state_enc\"].map(state_mean)\n    train[\"month_mean_yield\"] = train[\"month\"].map(month_mean)\n\n    test[\"state_mean_yield\"] = test[\"state_enc\"].map(state_mean)\n    test[\"month_mean_yield\"] = test[\"month\"].map(month_mean)\n\n    for df in (train, test):\n        df[\"state_mean_yield\"] = df[\"state_mean_yield\"].fillna(global_mean)\n        df[\"month_mean_yield\"] = df[\"month_mean_yield\"].fillna(global_mean)\n\n    # state + crop-phase level mean yield\n    sp_mean = (\n        train.groupby([\"state_enc\", \"months_since_crop_start\"])[target]\n        .mean()\n        .reset_index()\n        .rename(columns={target: \"state_phase_mean_yield\"})\n    )\n    train = train.merge(\n        sp_mean,\n        on=[\"state_enc\", \"months_since_crop_start\"],\n        how=\"left\",\n    )\n    test = test.merge(\n        sp_mean,\n        on=[\"state_enc\", \"months_since_crop_start\"],\n        how=\"left\",\n    )\n    for df in (train, test):\n        df[\"state_phase_mean_yield\"] = df[\"state_phase_mean_yield\"].fillna(global_mean)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != target]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n\n    # ---------- train ----------\n    model = lgb.LGBMRegressor(\n        n_estimators=800,\n        learning_rate=0.05,\n        num_leaves=31,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean01\\inputs\\source\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n", "language": "python", "parent_id": "486846ab-2d29-4482-9b45-05eef619320a", "generation": 1, "timestamp": 1765273950.2650452, "iteration_found": 3, "metrics": {"combined_score": 0.6248295932069328, "mape": 0.0006590358103089158, "rmse": 2.994921844199208}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 21 lines with 59 lines", "parent_metrics": {"combined_score": 0.7867218637397513, "mape": 8.218379499041217e-05, "rmse": 0.7436004060451427}, "island": 0}, "prompts": {"diff_user": {"system": "You are optimizing bean_test exp2 for Brazilian soybean yield. Goal: minimize MAPE/RMSE on the held-out test set used by evaluator.py.\\n\\nHard constraints:\\n- Only modify code inside the EVOLVE-BLOCK; do not touch any other code, function signatures, paths, or helpers.\\n- Keep the script runnable standalone: read ./input/train.csv & ./input/test.csv; write ./output/submission.csv with columns [year, month, state, yield].\\n- Preserve feature pipeline basics (state encoding, months_since_crop_start); do not drop all features.\\n- You can use any other standard Python libraries for better model.\\n\\nFreedom:\\n- Inside EVOLVE-BLOCK you may change model type (LightGBM/CatBoost/XGBoost/linear/NN/heuristics/FFN), features, and hyperparameters to improve accuracy.\\n\\nOutput format:\\n- Respond ONLY with valid SEARCH/REPLACE diffs for the EVOLVE-BLOCK. If you cannot propose a valid diff, return an empty diff.", "user": "# Current Program Information\n- Fitness: 0.7867\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness improved: 0.0000 \u2192 0.7867\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Change 1: Replace 3 lines with 4 lines\nChange 2: Replace 21 lines with 50 lines\n- Metrics: combined_score: 0.0000, error: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\n- Outcome: Regression in all metrics\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.7867, mape: 0.0001, rmse: 0.7436\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.7867)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from evaluator.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT / \"data\"\n    OUTPUT_DIR = PROJECT_ROOT.parent.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean01\\inputs\\source\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.7867), Performs well on mape (0.0001), Performs well on rmse (0.7436)\n\n### Program 2 (Score: 0.0000)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from evaluator.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT / \"data\"\n    OUTPUT_DIR = PROJECT_ROOT.parent.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # sort by time for a simple train/validation split\n    train_sorted = train.sort_values([\"year\", \"month\"])\n    X = train_sorted[features]\n    y = train_sorted[target].astype(float)\n\n    # optional log1p transform for stability\n    use_log = (y > 0).all()\n    if use_log:\n        y = np.log1p(y)\n\n    n = len(X)\n    split = int(n * 0.8)\n    X_tr, X_val = X.iloc[:split], X.iloc[split:]\n    y_tr, y_val = y.iloc[:split], y.iloc[split:]\n\n    # ---------- train ----------\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_val, y_val)],\n        eval_metric=\"rmse\",\n        early_stopping_rounds=100,\n        verbose=False,\n    )\n\n    # ---------- predict ----------\n    X_test = test[features]\n    if getattr(model, \"best_iteration_\", None) is not None:\n        raw_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        raw_pred = model.predict(X_test)\n    if use_log:\n        test_pred = np.expm1(raw_pred)\n    else:\n        test_pred = raw_pred\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean01\\inputs\\source\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nKey features: Performs well on combined_score (0.0000), Performs well on error (LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds')\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from evaluator.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT / \"data\"\n    OUTPUT_DIR = PROJECT_ROOT.parent.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # sort by time for a simple train/validation split\n    train_sorted = train.sort_values([\"year\", \"month\"])\n    X = train_sorted[features]\n    y = train_sorted[target].astype(float)\n\n    # optional log1p transform for stability\n    use_log = (y > 0).all()\n    if use_log:\n        y = np.log1p(y)\n\n    n = len(X)\n    split = int(n * 0.8)\n    X_tr, X_val = X.iloc[:split], X.iloc[split:]\n    y_tr, y_val = y.iloc[:split], y.iloc[split:]\n\n    # ---------- train ----------\n    model = lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        num_leaves=127,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_val, y_val)],\n        eval_metric=\"rmse\",\n        early_stopping_rounds=100,\n        verbose=False,\n    )\n\n    # ---------- predict ----------\n    X_test = test[features]\n    if getattr(model, \"best_iteration_\", None) is not None:\n        raw_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    else:\n        raw_pred = model.predict(X_test)\n    if use_log:\n        test_pred = np.expm1(raw_pred)\n    else:\n        test_pred = raw_pred\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean01\\inputs\\source\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nUnique approach: Modification: Change 1: Replace 3 lines with 4 lines\nChange 2: Replace 21 lines with 50 lines, Alternative combined_score approach, NumPy-based implementation\n\n# Current Program\n```python\n\"\"\"\nBaseline model for bean yield prediction\n\nReads ./data/train.csv and ./data/test.csv\nwrites predictions to ./result/submission.csv with columns [year, month, state, yield].\n\nThe EVOLVE-BLOCK marks the scope that OpenEvolve is allowed to tune.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\n# construct real path from evaluator.py's ROOT\ndef construct_real_path(root):\n    PROJECT_ROOT = Path(root)\n    INPUT_DIR = PROJECT_ROOT / \"data\"\n    OUTPUT_DIR = PROJECT_ROOT.parent.parent / \"outputs\" / \"submission\"\n    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    TRAIN_PATH = INPUT_DIR / \"train.csv\"\n    TEST_PATH = INPUT_DIR / \"test.csv\"\n\n    return TRAIN_PATH, TEST_PATH, OUTPUT_DIR\n\n# EVOLVE-BLOCK-START\nfrom typing import Dict, Tuple\nimport lightgbm as lgb\nimport pandas as pd\n\ndef encode_state(df: pd.DataFrame, mapping: Dict[str, int] | None = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"Encode state column to integer IDs.\"\"\"\n    df = df.copy()\n    if mapping is None:\n        states = sorted(df[\"state\"].unique())\n        mapping = {s: i for i, s in enumerate(states)}\n    df[\"state_enc\"] = df[\"state\"].map(mapping).fillna(-1).astype(int)\n    return df, mapping\n\n\ndef months_since_crop_start(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Map month to a simple crop-phase index.\"\"\"\n    df = df.copy()\n\n    def transform(m: int) -> int:\n        return m - 10 if m >= 10 else m + 2\n\n    df[\"months_since_crop_start\"] = df[\"month\"].apply(transform)\n    return df\n\n\ndef train_and_predict(root) -> Path:\n\n    # do not change this line\n    train_path, test_path, output_path = construct_real_path(root)\n\n    # ---------- read ----------\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n\n    # ---------- encode & transform ----------\n    train, state2idx = encode_state(train)\n    test, _ = encode_state(test, mapping=state2idx)\n    train = months_since_crop_start(train)\n    test = months_since_crop_start(test)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n\n    # ---------- output ----------\n    test_out = test.copy()\n    test_out[\"yield\"] = test_pred\n    test_out = test_out[[\"year\", \"month\", \"state\", \"yield\"]]\n    out_path = output_path / \"submission.csv\"\n    test_out.to_csv(out_path, index=False)\n    print(f\"\u2705 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u9884\u6d4b\u7ed3\u679c\u5df2\u4fdd\u5b58\u81f3: {out_path}\")\n    return out_path\n\n# EVOLVE-BLOCK-END\n\ndef main() -> Path:\n    root = r\"D:\\\u6e05\u534e\u5de5\u7a0b\u535a\u58eb\\C3I\\daguan\\agentic-rl\\mle-openevolve\\experiments\\bean01\\inputs\\source\"\n    out_path = train_and_predict(root)\n    return out_path\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != \"yield\"]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n    target = \"yield\"\n\n    # ---------- train ----------\n    \n    model = lgb.LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n=======\n    # ---------- target-based aggregate features ----------\n    target = \"yield\"\n    global_mean = train[target].mean()\n\n    # per-state and per-month historical mean yields\n    state_mean = train.groupby(\"state_enc\")[target].mean()\n    month_mean = train.groupby(\"month\")[target].mean()\n\n    train[\"state_mean_yield\"] = train[\"state_enc\"].map(state_mean)\n    train[\"month_mean_yield\"] = train[\"month\"].map(month_mean)\n\n    test[\"state_mean_yield\"] = test[\"state_enc\"].map(state_mean)\n    test[\"month_mean_yield\"] = test[\"month\"].map(month_mean)\n\n    for df in (train, test):\n        df[\"state_mean_yield\"] = df[\"state_mean_yield\"].fillna(global_mean)\n        df[\"month_mean_yield\"] = df[\"month_mean_yield\"].fillna(global_mean)\n\n    # state + crop-phase level mean yield\n    sp_mean = (\n        train.groupby([\"state_enc\", \"months_since_crop_start\"])[target]\n        .mean()\n        .reset_index()\n        .rename(columns={target: \"state_phase_mean_yield\"})\n    )\n    train = train.merge(\n        sp_mean,\n        on=[\"state_enc\", \"months_since_crop_start\"],\n        how=\"left\",\n    )\n    test = test.merge(\n        sp_mean,\n        on=[\"state_enc\", \"months_since_crop_start\"],\n        how=\"left\",\n    )\n    for df in (train, test):\n        df[\"state_phase_mean_yield\"] = df[\"state_phase_mean_yield\"].fillna(global_mean)\n\n    # ---------- feature selection ----------\n    numeric_kinds = (\"b\", \"i\", \"u\", \"f\", \"c\")\n    candidate_features = [col for col in train.columns if col != target]\n    features = [col for col in candidate_features if train[col].dtype.kind in numeric_kinds]\n\n    # ---------- train ----------\n    model = lgb.LGBMRegressor(\n        n_estimators=800,\n        learning_rate=0.05,\n        num_leaves=31,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=1.0,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(train[features], train[target])\n\n    # ---------- predict ----------\n    test_pred = model.predict(test[features])\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}